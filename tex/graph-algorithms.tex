%%-----------------------------------------------------------------------%%
%%--- Graph Algorithms --------------------------------------------------%%

\chapter{Graph Algorithms}
\label{chap:graph_algorithms}

Graph algorithms have many applications. Suppose you are a salesman
with a product you would like to sell in several cities. To determine
the cheapest travel route from city-to-city, you must effectively
search a graph having weighted edges for the ``cheapest'' route
visiting each city once. Each vertex denotes a city you must visit and
each edge has a weight indicating either the distance from one city to
another or the cost to travel from one city to another.

Shortest path algorithms are some of the most important algorithms in
algorithmic graph theory. We shall examine several in this chapter.


%%-----------------------------------------------------------------------%%
%%--- Representing graphs in a computer ---------------------------------%%

\section{Representing graphs in a computer}

In section~\ref{sec:introduction:matrix_representation}, we discussed
how to use matrices for representing graphs and digraphs. If
$A = [a_{ij}]$ is an $m \times n$ matrix, the adjacency matrix
representation of a graph would require representing all the $mn$
entries of $A$. Alternative graph representations exist that are much
more efficient than representing all entries of a matrix.


%%--- Adjacency lists ---------------------------------------------------%%

\subsection{Adjacency lists}

A \emph{list} is a sequence of objects. Unlike sets, a list may
contain multiple copies of the same object. Each object of a list is
referred to as an \emph{element} of the list. A list $L$ of
$n \geq 0$ elements is written as $L = [a_1, a_2, \dots, a_n]$, where
the $i$-th element $a_i$ can be indexed as $L[i]$. In case $n = 0$,
the list $L = [\,]$ is referred to as the \emph{empty list}. Two lists
are equivalent if they both contain the same elements at exactly the
same positions.

Define the adjacency lists of a graph as follows. Let $G$ be a graph
with vertex set $V = \{v_1, v_2, \dots, v_n\}$. Assign to each vertex
$v_i$ a list $L_i$ containing all the vertices that are adjacent to
$v_i$. The list $L_i$ associated with $v_i$ is referred to as the
\emph{adjacency list} of $v_i$. Then $L_i = [\,]$ if and only if $v_i$
is an isolated vertex. We say that $L_i$ is \emph{the} adjacency list
of $v_i$ because any permutation of the elements of $L_i$ results in a
list that contains the same vertices adjacent to $v_i$. If each
adjacency list $L_i$ contains $s_i$ elements where
$0 \leq s_i \leq n$, we say that $L_i$ has \emph{length} $s_i$. The
adjacency list representation of the graph $G$ requires that we
represent $\sum_i s_i = 2 \cdot |E(G)| \leq n^2$ elements in a
computer's memory, since each edge appears twice in the adjacency list
representation. An adjacency list is explicit about which vertices are
adjacent to a vertex, and implicit about which vertices are not
adjacent to that same vertex. Without knowing the graph $G$, given the
adjacency lists $L_1, L_2, \dots, L_n$, we can reconstruct $G$. For
example, Figure~\ref{fig:graph_algorithms:graph_adjacency_lists} shows
a graph and its adjacency list representation.

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  linedecorate/.style={-,thick}]
%% nodes or vertices
\foreach \nodename/\x/\y/\direction/\navigate in {
  1/0/0/left/west, 2/4/0/right/east, 3/4/3.5/right/east,
  4/0/3.5/left/west, 5/2/1/below/south, 6/3/2/above/north,
  7/2/3/below/south, 8/1/2/above/north} {
  \node (\nodename) at (\x,\y) [nodedecorate] {};
  \node [\direction] at (\nodename.\navigate) {$\nodename$};
}
%% adjacency lists
\node (L1) at (5,2.1) [] {};
\node [right] at (L1.east) {$L_1 = [2,8]$};
\node (L2) at (5,1.4) [] {};
\node [right] at (L2.east) {$L_2 = [1,6]$};
\node (L3) at (5,0.7) [] {};
\node [right] at (L3.east) {$L_3 = [4]$};
\node (L4) at (5,0) [] {};
\node [right] at (L4.east) {$L_4 = [3]$};
\node (L5) at (8,2.1) [] {};
\node [right] at (L5.east) {$L_5 = [6,8]$};
\node (L6) at (8,1.4) [] {};
\node [right] at (L6.east) {$L_6 = [2,5,8]$};
\node (L7) at (8,0.7) [] {};
\node [right] at (L7.east) {$L_7 = [\,]$};
\node (L8) at (8,0) [] {};
\node [right] at (L8.east) {$L_8 = [1,5,6]$};
%% edges or lines
\path
\foreach \startnode/\endnode in {1/2, 1/8, 2/6, 3/4, 5/6, 5/8, 6/8} {
  (\startnode) edge[linedecorate] node {} (\endnode)
};
\end{tikzpicture}
\caption{A graph and its adjacency lists.}
\label{fig:graph_algorithms:graph_adjacency_lists}
\end{figure}


%%--- The graph6 format -------------------------------------------------%%

\subsection{The graph6 format}

The graph formats graph6 and sparse6 were developed by Brendan
McKay~\cite{McKay2010} at The Australian National University as a
compact way to represent graphs. These two formats use bit vectors and
printable characters of the American Standard Code for Information
Interchange~(ASCII) encoding scheme. The 64 printable ASCII characters
used in graph6 and sparse6 are those ASCII characters with decimal
codes from 63 to 126, inclusive, as shown in
Table~\ref{tab:graph_algorithms:graph6_sparse6_ASCII_printable_characters}.
This section shall only cover the graph6 format. For full
specification on both of the graph6 and sparse6 formats, see
McKay~\cite{McKay2010}.

\begin{table}[!htbp]
\centering
\begin{tabular}{|ccc|ccc|} \hline
binary         & decimal   & glyph    & binary & decimal & glyph \\\hline\hline
\verb!0111111! & \verb!63! & \verb!?! & \verb!1011111! & \verb!95!  & \verb!_! \\
\verb!1000000! & \verb!64! & \verb!@! & \verb!1100000! & \verb!96!  & \verb!`! \\
\verb!1000001! & \verb!65! & \verb!A! & \verb!1100001! & \verb!97!  & \verb!a! \\
\verb!1000010! & \verb!66! & \verb!B! & \verb!1100010! & \verb!98!  & \verb!b! \\
\verb!1000011! & \verb!67! & \verb!C! & \verb!1100011! & \verb!99!  & \verb!c! \\
\verb!1000100! & \verb!68! & \verb!D! & \verb!1100100! & \verb!100! & \verb!d! \\
\verb!1000101! & \verb!69! & \verb!E! & \verb!1100101! & \verb!101! & \verb!e! \\
\verb!1000110! & \verb!70! & \verb!F! & \verb!1100110! & \verb!102! & \verb!f! \\
\verb!1000111! & \verb!71! & \verb!G! & \verb!1100111! & \verb!103! & \verb!g! \\
\verb!1001000! & \verb!72! & \verb!H! & \verb!1101000! & \verb!104! & \verb!h! \\
\verb!1001001! & \verb!73! & \verb!I! & \verb!1101001! & \verb!105! & \verb!i! \\
\verb!1001010! & \verb!74! & \verb!J! & \verb!1101010! & \verb!106! & \verb!j! \\
\verb!1001011! & \verb!75! & \verb!K! & \verb!1101011! & \verb!107! & \verb!k! \\
\verb!1001100! & \verb!76! & \verb!L! & \verb!1101100! & \verb!108! & \verb!l! \\
\verb!1001101! & \verb!77! & \verb!M! & \verb!1101101! & \verb!109! & \verb!m! \\
\verb!1001110! & \verb!78! & \verb!N! & \verb!1101110! & \verb!110! & \verb!n! \\
\verb!1001111! & \verb!79! & \verb!O! & \verb!1101111! & \verb!111! & \verb!o! \\
\verb!1010000! & \verb!80! & \verb!P! & \verb!1110000! & \verb!112! & \verb!p! \\
\verb!1010001! & \verb!81! & \verb!Q! & \verb!1110001! & \verb!113! & \verb!q! \\
\verb!1010010! & \verb!82! & \verb!R! & \verb!1110010! & \verb!114! & \verb!r! \\
\verb!1010011! & \verb!83! & \verb!S! & \verb!1110011! & \verb!115! & \verb!s! \\
\verb!1010100! & \verb!84! & \verb!T! & \verb!1110100! & \verb!116! & \verb!t! \\
\verb!1010101! & \verb!85! & \verb!U! & \verb!1110101! & \verb!117! & \verb!u! \\
\verb!1010110! & \verb!86! & \verb!V! & \verb!1110110! & \verb!118! & \verb!v! \\
\verb!1010111! & \verb!87! & \verb!W! & \verb!1110111! & \verb!119! & \verb!w! \\
\verb!1011000! & \verb!88! & \verb!X! & \verb!1111000! & \verb!120! & \verb!x! \\
\verb!1011001! & \verb!89! & \verb!Y! & \verb!1111001! & \verb!121! & \verb!y! \\
\verb!1011010! & \verb!90! & \verb!Z! & \verb!1111010! & \verb!122! & \verb!z! \\
\verb!1011011! & \verb!91! & \verb![! & \verb!1111011! & \verb!123! & \verb!{! \\
\verb!1011100! & \verb!92! & \verb!\! & \verb!1111100! & \verb!124! & \verb!|! \\
\verb!1011101! & \verb!93! & \verb!]! & \verb!1111101! & \verb!125! & \verb!}! \\
\verb!1011110! & \verb!94! & \verb!^! & \verb!1111110! & \verb!126! & \verb!~! \\\hline
\end{tabular}
\caption{ASCII printable characters used by graph6 and sparse6.}
\label{tab:graph_algorithms:graph6_sparse6_ASCII_printable_characters}
\end{table}


%%--- Bit vectors -------------------------------------------------------%%

\subsubsection{Bit vectors}

Before discussing how graph6 and sparse6 represent graphs using
printable ASCII characters, we first present encoding schemes used by
these two formats. A \emph{bit vector} is, as its name suggests, a
vector whose elements are 1's and 0's. It can be represented as a list
of bits, e.g. \verb!E! can be represented as the ASCII bit vector
$[\texttt{1}, \texttt{0}, \texttt{0}, \texttt{0}, \texttt{1},
  \texttt{0}, \texttt{1}]$. For brevity, we write a bit vector in a
compact form such as \texttt{1000101}. The \emph{length} of a bit
vector is its number of bits. The \emph{most significant bit}
of a bit vector $v$ is the bit position with the largest value among
all the bit positions in $v$. Similarly, the
\emph{least significant bit} is the bit position in $v$ having the
least value among all the bit positions in $v$. The least significant
bit of $v$ is usually called the parity bit because when $v$
interpreted as an integer the parity bit determines whether the
integer is even or odd. Reading \texttt{1000101} from left to right,
the first bit \texttt{1} is the most significant bit, followed by the
second bit \texttt{0} which is the second most significant bit, and so
on all the way down to the seventh bit \texttt{1} which is the least
significant bit.

The order in which we process the bits of a bit vector is referred to
as \emph{endianness}. Processing $v$ in \emph{big-endian} order means
that we first process the most significant bit of $v$, followed by the
second most significant bit, and so on all the way down to the least
significant bit of $v$. \emph{Little-endian} order means that we first
process the least significant bit, followed by the second least
significant bit, and so on all the way up to the most significant
bit. In big-endian order, the ASCII binary representation of
\texttt{E} is written \texttt{1000101}~(see
Table~\ref{tab:graph_algorithms:big_endian_ASCII_binary_E}), while
the little-ending order is written \texttt{1010001}~(see
Table~\ref{tab:graph_algorithms:little_endian_ASCII_binary_E}). To
determine the integer representation of a bit vector, multiply each
bit value by its corresponding position value, then add up all the
results. In general, if the bit vector $v = b_1 b_2 \cdots b_k$ is the
big-endian binary representation of a positive integer, then the
integer representation of $v$ is
%
\begin{equation}
\label{eq:graph_algorithms:big_endian_binary_to_integer}
\sum_{i=1}^k 2^{k-i} b_i
=
2^{k-1} b_1 + 2^{k-2} b_2 + 2^{k-3} b_3 + \cdots + 2^0 b_k.
\end{equation}

\begin{table}[!htbp]
\centering
\begin{tabular}{|l|ccccccc|} \hline
position       & 1          & 2          & 3          & 4          & 5          & 6          & 7 \\\hline
bit value      & \texttt{1} & \texttt{0} & \texttt{0} & \texttt{0} & \texttt{1} & \texttt{0} & \texttt{1} \\\hline
position value & $2^6$      & $2^5$      & $2^4$      & $2^3$      & $2^2$      & $2^1$      & $2^0$ \\\hline
\end{tabular}
\caption{Big-endian order of the ASCII binary code of \texttt{E}.}
\label{tab:graph_algorithms:big_endian_ASCII_binary_E}
\end{table}

\begin{table}[!htbp]
\centering
\begin{tabular}{|l|ccccccc|} \hline
position       & 1          & 2          & 3          & 4          & 5          & 6          & 7 \\\hline
bit value      & \texttt{1} & \texttt{0} & \texttt{1} & \texttt{0} & \texttt{0} & \texttt{0} & \texttt{1} \\\hline
position value & $2^0$      & $2^1$      & $2^2$      & $2^3$      & $2^4$      & $2^5$      & $2^6$ \\\hline
\end{tabular}
\caption{Little-endian order of the ASCII binary code of \texttt{E}.}
\label{tab:graph_algorithms:little_endian_ASCII_binary_E}
\end{table}

In graph6 and sparse6 formats, the length of a bit vector must be a
multiple of 6. Suppose $v$ is a bit vector of length $k$ such that
$6 \nmid k$. To transform $v$ into a bit vector having length a
multiple of 6, let $r = k \mod 6$ be the remainder upon dividing $k$
by 6, and pad $6 - r$ zeros to the right of $v$.

Suppose $v = b_1 b_2 \cdots b_k$ is a bit vector of length $k$, where
$6 \;|\; k$. We split $v$ into $k/6$ bit vectors $v_i$, each of length
6. For $0 \leq i \leq k/6$, the $i$-th bit vector is given by
\[
v_i
=
b_{6i-5} b_{6i-4} b_{6i-3} b_{6i-2} b_{6i-1} b_{6i}.
\]
Consider each $v_i$ as the big-endian binary representation of a
positive
integer. Use~(\ref{eq:graph_algorithms:big_endian_binary_to_integer})
to obtain the integer representation $N_i$ of each $v_i$. Then add 63
to each $N_i$ to obtain $N_i'$ and store $N_i'$ in one byte of
memory. That is, each $N_i'$ can be represented as a bit vector of
length $8$. Thus the required number of bytes to store $v$ is
$\lceil k/6 \rceil$. Let $B_i$ be the byte representation of $N_i'$ so
that
%
\begin{equation}
\label{eq:graph_algorithms:byte_representation_bit_vector}
R(v)
=
B_1 B_2 \cdots B_{\lceil k/6 \rceil}
\end{equation}
%
denotes the representation of $v$ as a sequence of $\lceil k/6 \rceil$
bytes.

We now discuss how to encode an integer $n$ in the range
$0 \leq n \leq 2^{36} - 1$
using~(\ref{eq:graph_algorithms:byte_representation_bit_vector}) and
denote such an encoding of $n$ as $N(n)$. Let $v$ be the big-endian
binary representation of $n$. Then $N(n)$ is given by
%
\begin{equation}
\label{eq:graph_algorithms:graph6_sparse6_graph_orders}
N(n)
=
\begin{cases}
n + 63, & \text{if $0 \leq n \leq 62$}, \\
126 \, R(v), & \text{if $63 \leq n \leq 258047$}, \\
126 \, 126 \, R(v), & \text{if $258048 \leq n \leq 2^{36}-1$}.
\end{cases}
\end{equation}
%% If $0 \leq n \leq 62$, then
%% we write $N(n) = n + 63$. If $63 \leq n \leq 258047$, let $v$ be the
%% big-endian binary representation of $n$ and write
%% $N(n) = 126 \, R(v)$. Finally, if $258048 \leq n \leq 2^{36} - 1$,
%% again we let $v$ be the big-endian binary representation of $n$ and
%% write $N(n) = 126 \, 126 \, R(v)$.
Note that $n + 63$ requires one byte of storage memory, while
$126 \, R(v)$ and $126 \, 126 \, R(v)$ require 4 and 8 bytes,
respectively.


%%--- The graph6 format -------------------------------------------------%%

\subsubsection{The graph6 format}

The graph6 format is used to represent simple, undirected graphs of
order from $0$ to $2^{36} - 1$, inclusive. Let $G$ be a simple,
undirected graph of order $0 \leq n \leq 2^{36} - 1$. If $n = 0$, then
$G$ is represented in graph6 format as ``\verb!?!''. Suppose $n >
0$. Let $M = [a_{ij}]$ be the adjacency matrix of $G$. Consider the
upper triangle of $M$, excluding the main diagonal, and write that
upper triangle as the bit vector
\[
v
=
\underbrace{a_{0,1}}_{c_1}
\underbrace{a_{0,2} a_{1,2}}_{c_2}
\underbrace{a_{0,3} a_{1,3} a_{2,3}}_{c_3} \cdots
\underbrace{a_{0,i} a_{1,i} \cdots a_{i-1,i}}_{c_i} \cdots
\underbrace{a_{0,n} a_{1,n} \cdots a_{n-1,n}}_{c_n}
\]
where $c_i$ denotes the entries $a_{0,i} a_{1,i} \cdots a_{i-1,i}$ in
column $i$ of $M$. Then the graph6 representation of $G$ is
$N(n) R(v)$, where $R(v)$ and $N(n)$ are as
in~(\ref{eq:graph_algorithms:byte_representation_bit_vector})
and~(\ref{eq:graph_algorithms:graph6_sparse6_graph_orders}),
respectively. That is, $N(n)$ encodes the order of $G$ and $R(v)$
encodes the edges of $G$.


%%-----------------------------------------------------------------------%%
%%--- Graph searching ---------------------------------------------------%%

\section{Graph searching}
\label{sec:graph_algorithms:graph_searching}

This section discusses two fundamental algorithms for graph traversal:
breadth-first search and depth-first search. The word ``search'' used
in describing these two algorithms is rather misleading. It would be
more accurate to describe them as algorithms for constructing trees
using the adjacency information of a given graph. However, the names
``breadth-first search'' and ``depth-first search'' are entrenched in
literature on graph theory and computer science. From hereon, we use
these two names as given above, bearing in mind their intended
purposes.


%%--- Breadth-first search ----------------------------------------------%%

\subsection{Breadth-first search}

Breadth-first search (BFS) is a strategy for running through the
vertices of a graph. It was presented by Moore~\cite{Moore1959} in
1959 within the context of traversing mazes. Lee~\cite{Lee1961}
independently discovered the same algorithm in 1961 in his work on
routing wires on circuit boards.

The basic BFS algorithm can be described as follows. Starting from a
given vertex $v$ of a graph $G$, we first explore the neighborhood of
$v$ by visiting all vertices that are adjacent to $v$. We then apply
the same strategy to each of the neighbors of $v$. The strategy of
exploring the neighborhood of a vertex is applied to all vertices of
$G$. The result is a tree rooted at $v$ and this tree is a subgraph of
$G$. Algorithm~\ref{alg:graph_algorithms:breadth_first_search_template}
presents a general template for the BFS strategy. The tree resulting
from the BFS algorithm is called a \emph{breadth-first search tree}.
\index{BFS}
\index{breadth-first search}

\begin{algorithm}[!htpb]
\dontprintsemicolon  % no semicolon at end of pseudocode statements
%% data section
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
%% input/output
\Input{A directed or undirected graph $G = (V, E)$ of order $n > 0$. A
  vertex $s$ from which to start the search. The vertices are numbered
  from $1$ to  $n = |V|$, i.e. $V = \{1, 2, \dots, n\}$.}
\Output{A list $D$ of distances of all vertices from $s$. A tree $T$
  rooted at $s$.}
\BlankLine
$Q \leftarrow [s]$~\nllabel{alg:BFS:initialize_queue_visit_nodes} \tcc*[f]{queue of nodes to visit}\;
$D \leftarrow [\infty, \infty, \dots, \infty]$ \tcc*[f]{$n$ copies of $\infty$}\;
$D[s] \leftarrow 0$\;
$T \leftarrow [\,]$~\nllabel{alg:BFS:initialize_empty_tree}\;
\While{$\length(Q) > 0$~\nllabel{alg:BFS:while_loop:non_empty_queue}}{
  $v \leftarrow \dequeue(Q)$\;
  \For{\emph{each} $w \in \adj(v)$~\nllabel{alg:BFS:explore_neighborhood}}{
    \If{$D[w] = \infty$~\nllabel{alg:BFS:marking_vertex_as_visited}}{
      $D[w] \leftarrow D[v] + 1$\;
      $\enqueue(Q, w)$\;
      $\append(T, vw)$~\nllabel{alg:BFS:while_loop:append_to_tree}\;
    }
  }
}
\Return $(D, T)$\;
\caption{A general breadth-first search template.}
\label{alg:graph_algorithms:breadth_first_search_template}
\end{algorithm}

The breadth-first search algorithm makes use of a special type of list
called a \emph{queue}. This is analogous to a queue of people waiting
in line to be served. A person may enter the queue by joining the rear
of the queue. The person who is in the queue the longest amount of
time is served first, followed by the person who has waited the second
longest time, and so on. Formally, a queue $Q$ is a list of
elements. At any time, we only have access to the first element of
$Q$, known as the \emph{front} or \emph{start} of the queue. We insert
a new element into $Q$ by appending the new element to the \emph{rear}
or \emph{end} of the queue. The operation of removing the front of $Q$
is referred to as \emph{dequeue}, while the operation of appending to
the rear of $Q$ is called \emph{enqueue}. That is, a queue implements
a first-in first-out~(FIFO) protocol for adding and moving
elements. As with lists, the \emph{length} of a queue is its total
number of elements.

\begin{figure}[!htbp]
\centering
\subfigure[]{
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  linedecorate/.style={-,thick}]
%% nodes or vertices
\foreach \nodename/\x/\y/\direction/\navigate in {2/4/4/above/north,
  1/0/4/above/north, 3/6/3/above/north, 4/6/1/below/south,
  5/4/0/below/south, 7/-2/2/above/north, 6/0/0/below/south} {
  \node (\nodename) at (\x,\y) [nodedecorate] {};
  \node [\direction] at (\nodename.\navigate) {$\nodename$};
}
%% edges or lines
\path
\foreach \startnode/\endnode in {1/2, 1/5, 2/3, 2/5, 2/7, 3/4, 3/5,
  4/6, 5/6, 6/7} {
  (\startnode) edge[linedecorate] node {} (\endnode)
};
\end{tikzpicture}
}
%%
\qquad
\subfigure[]{
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  linedecorate/.style={-,thick},%
  scale=1.6]
%% nodes or vertices
\foreach \nodename/\x/\y/\direction/\navigate in {4/0/0/below/south,
  3/0/1/left/west, 7/1/1/below/south, 6/2/1/below/south,
  2/0.5/2/left/west, 5/2/2/left/west, 1/1.25/3/above/north} {
  \node (\nodename) at (\x,\y) [nodedecorate] {};
  \node [\direction] at (\nodename.\navigate) {$\nodename$};
}
%% edges or lines
\path
\foreach \startnode/\endnode in {1/2, 1/5, 2/3, 2/7, 3/4, 5/6} {
  (\startnode) edge[linedecorate] node {} (\endnode)
};
\end{tikzpicture}
}
%%
%%
\subfigure[]{
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  arrowdecorate/.style={->,>=stealth,thick}]
%% nodes or vertices
\foreach \nodename/\x/\y/\direction/\navigate in {2/4/4/above/north,
  1/0/4/above/north, 3/6/3/above/north, 4/6/1/below/south,
  5/4/0/below/south, 7/-2/2/above/north, 6/0/0/below/south} {
  \node (\nodename) at (\x,\y) [nodedecorate] {};
  \node [\direction] at (\nodename.\navigate) {$\nodename$};
}
%% edges or lines
\path
\foreach \startnode/\endnode in {2/5, 3/2, 3/5, 4/3, 5/1, 6/7, 7/2} {
  (\startnode) edge[arrowdecorate] node {} (\endnode)
}
\foreach \startnode/\endnode/\benddirection/\angle in {
  1/2/bend left/12, 4/6/bend right/12, 6/5/bend right/12} {
  (\startnode) edge[arrowdecorate,\benddirection=\angle] node {} (\endnode)
};
\end{tikzpicture}
}
%%
\qquad
\subfigure[]{
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  arrowdecorate/.style={->,>=stealth,thick},
  scale=1.6]
%% nodes or vertices
\foreach \nodename/\x/\y/\direction/\navigate in {2/0/1/below/south,
  1/1/0/below/south, 5/1/1/left/west, 7/2/1/below/south,
  6/2/2/left/west, 3/0.5/2/left/west, 4/1.25/3/above/north} {
  \node (\nodename) at (\x,\y) [nodedecorate] {};
  \node [\direction] at (\nodename.\navigate) {$\nodename$};
}
%% edges or lines
\path
\foreach \startnode/\endnode in {4/3, 4/6, 3/2, 3/5, 6/7, 5/1} {
  (\startnode) edge[arrowdecorate] node {} (\endnode)
};
\end{tikzpicture}
}
\caption{Breadth-first search trees for undirected and directed graphs.}
\label{fig:graph_algorithms:breadth_first_search_undirected}
\end{figure}

Note that the BFS
Algorithm~\ref{alg:graph_algorithms:breadth_first_search_template}
works on both undirected and directed graphs. For an undirected graph,
line~\ref{alg:BFS:explore_neighborhood} means that we explore all
the neighbors of vertex $v$, i.e. the set $\adj(v)$ of vertices
adjacent to $v$. In the case of a digraph, we replace
``$w \in \adj(v)$'' on line~\ref{alg:BFS:explore_neighborhood} with
``$w \in \oadj(v)$'' because we only want to explore all vertices that
are out-neighbors of $v$. The algorithm returns two lists $D$ and
$T$. The list $T$ contains a subset of edges in $E(G)$ that make up a
tree rooted at the given start vertex $s$. As trees are connected
graphs without cycles, we may take the vertices comprising the edges
of $T$ to be the vertex set of the tree. It is clear that $T$
represents a tree by means of a list of edges, which allows us to
identify the tree under consideration as the edge list $T$. The list
$D$ has the same number of elements as the order of $G = (V, E)$,
i.e. $\length(D) = |V|$. The $i$-th element $D[i]$ counts the number
of edges in $T$ between the vertices $s$ and $v_i$. In other words,
$D[i]$ is the length of the $s$-$v_i$ path in $T$. It can be shown
that $D[i] = \infty$ if and only if $G$ is disconnected. After one
application of
Algorithm~\ref{alg:graph_algorithms:breadth_first_search_template}, it
may happen that $D[i] = \infty$ for at least one vertex
$v_i \in V$. To traverse those vertices that are unreachable from $s$,
again we apply
Algorithm~\ref{alg:graph_algorithms:breadth_first_search_template} on
$G$ with starting vertex $v_i$. Repeat this algorithm as often as
necessary until all vertices of $G$ are visited. The result may be a
tree that contains all the vertices of $G$ or a collection of trees,
each of which contains a subset of $V(G)$.
Figure~\ref{fig:graph_algorithms:breadth_first_search_undirected}
presents BFS trees resulting from applying
Algorithm~\ref{alg:graph_algorithms:breadth_first_search_template} on
an undirected graph and a digraph.

\begin{theorem}
\label{thm:graph_algorithms:BFS:worst_case_time_complexity}
The worst-case time complexity of
Algorithm~\ref{alg:graph_algorithms:breadth_first_search_template} is
$O(|V| + |E|)$.
\end{theorem}

\begin{proof}
Without loss of generality, we can assume that $G = (V, E)$ is
connected. The initialization steps in
lines~\ref{alg:BFS:initialize_queue_visit_nodes}
to~\ref{alg:BFS:initialize_empty_tree} take $O(|V|)$ time. After
initialization, all but one vertex are labelled
$\infty$. Line~\ref{alg:BFS:marking_vertex_as_visited} ensures that
each vertex is enqueued at most once and hence dequeued at most
once. Each of enqueuing and dequeuing takes constant time. The total
time devoted to queue operations is $O(|V|)$. The adjacency list of a
vertex is scanned after dequeuing that vertex, so each adjacency list
is scanned at most once. Summing the lengths of the adjacency lists,
we have $\Theta(|E|)$ and therefore we require $O(|E|)$ time to scan
the adjacency lists. After the adjacency list of a vertex is scanned,
at most $k$ edges are added to the list $T$, where $k$ is the length
of the adjacency list under consideration. Like queue operations,
appending to a list takes constant time, hence we require $O(|E|)$
time to build the list $T$. Therefore, BFS runs in $O(|V| + |E|)$
time.
\end{proof}

\begin{theorem}
\label{thm:graph_algorithms:BFS:list_D_length_shortest_paths}
For the list $D$ resulting from
Algorithm~\ref{alg:graph_algorithms:breadth_first_search_template},
let $s$ be a starting vertex and let $v$ be a vertex such that
$D[v] \neq \infty$. Then $D[v]$ is the length of any shortest path
from $s$ to $v$.
\end{theorem}

\begin{proof}
It is clear that $D[v] = \infty$ if and only if there are no paths
from $s$ to $v$. Let $v$ be a vertex such that $D[v] \neq \infty$. As
$v$ can be reached from $s$ by a path of length $D[v]$, the length
$d(s,v)$ of any shortest $s$-$v$ path satisfies $d(s,v) \leq
D[v]$. Use induction on $d(s,v)$ to show that equality holds. For the
base case $s = v$, we have $d(s,v) = D[v] = 0$ since the trivial path
has length zero. Assume for induction that if $d(s,v) = k$, then
$d(s,v) = D[v]$.
%% We need to show that if $d(s,u)$ is the length of any
%% shortest $s$-$u$ path, then $d(s,u) = D[u]$.
Let $d(s,u) = k + 1$ with the corresponding shortest $s$-$u$ path
being $(s, v_1, v_2, \dots, v_k, u)$. Then by our induction
hypothesis, $(s, v_1, v_2, \dots, v_k)$ is a shortest path from $s$ to
$v_k$ of length $d(s, v_k) = D[v_k] = k$. In other words, $D[v_k] <
D[u]$ and the while loop spanning
lines~\ref{alg:BFS:while_loop:non_empty_queue}
to~\ref{alg:BFS:while_loop:append_to_tree} processes $v_k$ before
processing $u$. The graph under consideration has the edge $v_k
u$. When examining the adjacency list of $v_k$, BFS reaches $u$~(if
$u$ is not reached earlier) and so $D[u] \leq k + 1$. Hence,
$D[u] = k + 1$ and therefore $d(s,u) = D[u] = k + 1$.
\end{proof}

%% Another version of
%% Algorithm~\ref{alg:graph_algorithms:breadth_first_search} is where you
%% are searching the graph for a vertex (or edge) satisfying a certain
%% property $P$. In that situation, you simply quit at the step where you
%% increment the counter, i.e. line~7 in
%% Algorithm~\ref{alg:graph_algorithms:breadth_first_search}. Other
%% variations are also possible as well.

%% For the example of the graph in
%% Figure~\ref{fig:introduction:types_of_walks}, the list of distances
%% from vertex \verb!a! to any other vertex is
%% %
%% \begin{center}
%% \fontsize{9pt}{9pt}
%% \selectfont
%% \tt
%% \begin{lstlisting}
%% [['a', 0], ['b', 1], ['c', 2], ['d', 3], ['e', 1], ['f', 2], ['g', 2]]
%% \end{lstlisting}
%% \end{center}
%% %
%% To create this list,
%% %
%% \begin{itemize}
%% \item
%% Start at \verb!a! and compute the distance from \verb!a! to itself.

%% \item
%% Move to each neighbor of \verb!a!, namely \verb!b! and \verb!e!, and
%% compute the distance from \verb!a! to each of them.

%% \item
%% Move to each ``unseen'' neighbor of \verb!b!, namely just \verb!c!,
%% and compute the distance from \verb!a! to it.

%% \item
%% Move to each ``unseen'' neighbor of \verb!e!, namely just \verb!f!,
%% and compute the distance from \verb!a! to it.

%% \item
%% Move to each ``unseen'' neighbor of \verb!c!, namely just \verb!d!,
%% and compute the distance from \verb!a! to it.

%% \item
%% Move to each ``unseen'' neighbor of \verb!f!, namely just \verb!g!,
%% and compute the distance from \verb!a! to it.
%% \end{itemize}

%% As an example, here is some Sage code which implements BFS to compute
%% the list distances from a given vertex.
%% %
%% \begin{center}
%% \fontsize{9pt}{9pt}
%% \selectfont
%% \tt
%% \begin{lstlisting}
%% def graph_distance(G, v0):
%%     """
%%     Breadth first search algorithm to find the
%%     distance from a fixed vertex $v_0$ to any
%%     other vertex.

%%     INPUT:
%%         G - a connected graph
%%         v0 - a vertex

%%     OUTPUT:
%%         D - a list of distances to
%%             every other vertex

%%     EXAMPLES:
%%         sage: G = Graph({1: [2, 4], 2: [1, 4], 3: [2, 6],
%%                          4: [1, 3], 5: [4, 2], 6: [3, 1]})
%%         sage: v0 = 1
%%         sage: graph_distance(G,v0)
%%         [[1, 0], [2, 1], [3, 2], [4, 1], [5, 2], [6, 1]]
%%         sage: G = Graph({"a": ["b", "e"], "b": ["c", "e"], \
%%          "c": ["d", "e"], "d": ["f"], "e": ["f"], "f": ["g"], "g":["b"]})
%%         sage: v0 = "a"
%%         sage: graph_distance(G, v0)
%%         [['a', 0], ['b', 1], ['c', 2], ['d', 3], ['e', 1],
%%          ['f', 2], ['g', 2]]
%%         sage: G = Graph({1: [2,3], 2: [1, 3], 3: [2], 4: [5], 5: [6], 6: [5]})
%%         sage: v0 = 1
%%         sage: graph_distance(G, v0) # note G is disconnected
%%         [[1, 0], [2, 1], [3, 1]]
%%     """
%%     V = G.vertices()
%%     Q = [v0]
%%     T = []
%%     D = []
%%     while Q<>[] and T<>V:
%%         for v in Q:
%%             if not(v in T):
%%                 D.append([v,G.distance(v0,v)])
%%             if v in Q:
%%                 Q.remove(v)
%%             T.append(v)
%%             T = list(Set(T))
%%             Q = Q+[x for x in G.neighbors(v) if not(x in T+Q)]
%%             if T == V:
%%                 break
%%     D.sort()
%%     print Q, T
%%     return D
%% \end{lstlisting}
%% \end{center}
%% %
%% \begin{exercise}
%% Using Sage's \verb!shortest_path! method, can you modify the above
%% function to return a list of shortest paths from $v_0$ to any other
%% vertex?
%% \end{exercise}


%%--- Depth-first search ------------------------------------------------%%

\subsection{Depth-first search}

A depth-first search~(DFS) is a graph traversal strategy similar to
breadth-first search. Both BFS and DFS differ in how they explore each
vertex. Whereas BFS explores the neighborhood of a vertex $v$ before
moving on to explore the neighborhoods of the neighbors, DFS explores
as deep as possible a path starting at $v$. One can think of BFS as
exploring the immediate surrounding, while DFS prefers to see what is
on the other side of the hill. In the 19th century,
Lucas~\cite{Lucas1882.1894} and Tarry~\cite{Tarry1895} investigated
DFS as a strategy for traversing mazes. Fundamental properties of DFS
were discovered in the early 1970s by Hopcroft and
Tarjan~\cite{HopcroftTarjan1973,Tarjan1972}.

To get an intuitive appreciation for DFS, suppose we have an
$8 \times 8$ chessboard in front of us. We place a single knight
piece on a fixed square of the board. Our objective is to find a
sequence of knight moves that visits each and every square exactly
once, while obeying the rules of chess that govern the movement of the
knight piece. Such a sequence of moves, if one exists, is called a
\emph{knight's tour}. How do we find such a tour? We could make one
knight move after another, recording each move to ensure that we do
not step on a square that is already visited, until we could not make
any more moves. Acknowledging defeat when encountering a dead end, it
might make sense to \emph{backtrack} a few moves and try again, hoping
we would not get stuck. If we fail again, we try backtracking a few
more moves and traverse yet another path, hoping to make further
progress. Repeat this strategy until a tour is found or until we have
exhausted all possible moves. The above strategy for finding a
knight's tour is an example of depth-first search, sometimes called
\emph{backtracking}.
\index{backtracking}
\index{knight's tour}

\begin{algorithm}[!htpb]
\dontprintsemicolon  % no semicolon at end of pseudocode statements
%% data section
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
%% input/output
\Input{A directed or undirected graph $G = (V, E)$ of order $n > 0$. A
  vertex $s$ from which to start the search. The vertices are numbered
  from $1$ to  $n = |V|$, i.e. $V = \{1, 2, \dots, n\}$.}
\Output{A list $D$ of distances of all vertices from $s$. A tree $T$
  rooted at $s$.}
\BlankLine
$S \leftarrow [s]$ \tcc*[f]{stack of nodes to visit}\;
$D \leftarrow [\infty, \infty, \dots, \infty]$ \tcc*[f]{$n$ copies of $\infty$}\;
$D[s] \leftarrow 0$\;
$T \leftarrow [\,]$\;
\While{$\length(S) > 0$~\nllabel{alg:DFS:while_loop_tests_non_empty_stack}}{
  $v \leftarrow \pop(S)$\;
  \For{\emph{each} $w \in \adj(v)$~\nllabel{alg:DFS:for_loop_visit_neighbors}}{
    \If{$D[w] = \infty$~\nllabel{alg:DFS:if_test_unvisited_neighbors}}{
      $D[w] \leftarrow D[v] + 1$\;
      $\push(S, w)$\;
      $\append(T, vw)$\;
    }
  }
}
\Return $(D, T)$\;
\caption{A general depth-first search template.}
\label{alg:graph_algorithms:depth_first_search_template}
\end{algorithm}

Algorithm~\ref{alg:graph_algorithms:depth_first_search_template}
formalizes the above description of depth-first search. The tree
resulting from applying DFS on a graph is called a
\emph{depth-first search tree}. The general structure of this
algorithm bears close resemblance to
Algorithm~\ref{alg:graph_algorithms:breadth_first_search_template}. A
significant difference is that instead of using a queue to structure
and organize vertices to be visited, DFS uses another special type of
list called a \emph{stack}. To understand how elements of a stack are
organized, we use the analogy of a stack of cards. A new card is added
to the stack by placing it on top of the stack. Any time we want to
remove a card, we are only allowed to remove the top-most card that is
on the top of the stack. A list $L = [a_1, a_2, \dots, a_k]$ of $k$
elements is a stack when we impose the same rules for element
insertion and removal. The top and bottom of the stack are $L[k]$ and
$L[1]$, respectively. The operation of removing the top element of the
stack is referred to as \emph{popping} the element off the
stack. Inserting an element into the stack is called \emph{pushing}
the element onto the stack. In other words, a stack implements a
last-in first-out~(LIFO) protocol for element insertion and removal,
in contrast to the FIFO policy of a queue. We also use the term
\emph{length} to refer to the number of elements in the stack.

The depth-first search
Algorithm~\ref{alg:graph_algorithms:depth_first_search_template} can
be analyzed similar to how we analyzed
Algorithm~\ref{fig:graph_algorithms:breadth_first_search_undirected}. Just
as BFS is applicable to both directed and undirected graphs, we can
also have undirected graphs and digraphs as input to DFS. For the case
of an undirected graph, line~\ref{alg:DFS:for_loop_visit_neighbors} of
Algorithm~\ref{alg:graph_algorithms:depth_first_search_template}
considers all vertices adjacent to the current vertex $v$. In case the
input graph is directed, we replace ``$w \in \adj(v)$'' on
line~\ref{alg:DFS:for_loop_visit_neighbors} with ``$w \in \oadj(v)$''
to signify that we only want to consider the out-neighbors of $v$. If
any neighbors (respectively, out-neighbors) of $v$ are labelled as
$\infty$, we know that we have not explored any paths starting from
any of those vertices. So we label each of those unexplored vertices
with a positive integer and push them onto the stack $S$, where they
will wait for later processing. We also record the paths leading from
$v$ to each of those unvisited neighbors, i.e. the edges $vw$ for each
vertex $w \in \adj(v)$ (respectively, $w \in \oadj(v)$) are appended
to the list $T$. The test on
line~\ref{alg:DFS:if_test_unvisited_neighbors} ensures that we do not
push onto $S$ any vertices on the path that lead to $v$. When we
resume another round of the while loop that starts on
line~\ref{alg:DFS:while_loop_tests_non_empty_stack}, the previous
vertex $v$ have been popped off $S$ and the neighbors (respectively,
out-neighbors) of $v$ have been pushed onto $S$. To explore a path
starting at $v$, we choose any unexplored neighbors of $v$ by popping
an element off $S$ and repeat the for loop starting on
line~\ref{alg:DFS:for_loop_visit_neighbors}. Repeat the DFS algorithm
as often as required in order to traverse all vertices of the input
graph. The output of DFS consists of two lists $D$ and $T$: $T$ is a
tree rooted at the starting vertex $s$; and each $D[i]$ counts the
length of the $s$-$v_i$ path in $T$.
Figure~\ref{fig:graph_algorithms:depth_first_search_undirected}
shows the DFS trees resulting from running
Algorithm~\ref{alg:graph_algorithms:depth_first_search_template} on an
undirected graph and a digraph. The worst-case time complexity of DFS
can be analyzed using an argument similar to that in
Theorem~\ref{thm:graph_algorithms:BFS:worst_case_time_complexity}. Arguing
along the same lines as in the proof of
Theorem~\ref{thm:graph_algorithms:BFS:list_D_length_shortest_paths},
we can also show that the list $D$ returned by DFS contains lengths of
any shortest paths from the starting vertex $s$ to any other vertex in
the tree $T$.

\begin{figure}[!htbp]
\centering
\subfigure[]{
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  linedecorate/.style={-,thick}]
%% nodes or vertices
\foreach \nodename/\x/\y/\direction/\navigate in {2/4/4/above/north,
  1/0/4/above/north, 3/6/3/above/north, 4/6/1/below/south,
  5/4/0/below/south, 7/-2/2/above/north, 6/0/0/below/south} {
  \node (\nodename) at (\x,\y) [nodedecorate] {};
  \node [\direction] at (\nodename.\navigate) {$\nodename$};
}
%% edges or lines
\path
\foreach \startnode/\endnode in {1/2, 1/5, 2/3, 2/5, 2/7, 3/4, 3/5,
  4/6, 5/6, 6/7} {
  (\startnode) edge[linedecorate] node {} (\endnode)
};
\end{tikzpicture}
}
%%
\qquad
\subfigure[]{
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  linedecorate/.style={-,thick},%
  scale=1.6]
%% nodes or vertices
\foreach \nodename/\x/\y/\direction/\navigate in {
  1/1.25/3/above/north, 2/0.5/2/below/south, 5/2/2/right/east,
  3/1.25/1/below/south, 6/2.75/1/right/east, 7/2/0/below/south,
  4/3.5/0/below/south} {
  \node (\nodename) at (\x,\y) [nodedecorate] {};
  \node [\direction] at (\nodename.\navigate) {$\nodename$};
}
%% edges or lines
\path
\foreach \startnode/\endnode in {1/2, 1/5, 3/5, 5/6, 6/7, 6/4} {
  (\startnode) edge[linedecorate] node {} (\endnode)
};
\end{tikzpicture}
}
%%
%%
\subfigure[]{
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  arrowdecorate/.style={->,>=stealth,thick}]
%% nodes or vertices
\foreach \nodename/\x/\y/\direction/\navigate in {2/4/4/above/north,
  1/0/4/above/north, 3/6/3/above/north, 4/6/1/below/south,
  5/4/0/below/south, 7/-2/2/above/north, 6/0/0/below/south} {
  \node (\nodename) at (\x,\y) [nodedecorate] {};
  \node [\direction] at (\nodename.\navigate) {$\nodename$};
}
%% edges or lines
\path
\foreach \startnode/\endnode in {2/5, 3/2, 3/5, 4/3, 5/1, 6/7, 7/2} {
  (\startnode) edge[arrowdecorate] node {} (\endnode)
}
\foreach \startnode/\endnode/\benddirection/\angle in {
  1/2/bend left/12, 4/6/bend right/12, 6/5/bend right/12} {
  (\startnode) edge[arrowdecorate,\benddirection=\angle] node {} (\endnode)
};
\end{tikzpicture}
}
%%
\qquad
\subfigure[]{
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  arrowdecorate/.style={->,>=stealth,thick},
  scale=1.6]
%% nodes or vertices
\foreach \nodename/\x/\y/\direction/\navigate in {
  4/1.25/3/above/north, 3/0.5/2/below/south, 6/2/2/right/east,
  5/1.25/1/left/west, 7/2.75/1/left/west, 1/1.25/0/below/south,
  2/2.75/0/below/south} {
  \node (\nodename) at (\x,\y) [nodedecorate] {};
  \node [\direction] at (\nodename.\navigate) {$\nodename$};
}
%% edges or lines
\path
\foreach \startnode/\endnode in {4/3, 4/6, 6/5, 6/7, 5/1, 7/2} {
  (\startnode) edge[arrowdecorate] node {} (\endnode)
};
\end{tikzpicture}
}
\caption{Depth-first search trees for undirected and directed graphs.}
\label{fig:graph_algorithms:depth_first_search_undirected}
\end{figure}


%%--- Connectivity of a graph -------------------------------------------%%

\subsection{Connectivity of a graph}

Both BFS and DFS can be used to determine if an undirected graph is
connected. Let $G = (V, E)$ be an undirected graph of order $n > 0$
and let $s$ be an arbitrary vertex of $G$. We initialize a counter
$c \leftarrow 1$ to mean that we are starting our exploration at $s$,
hence we have already visited $s$. We apply either BFS or DFS,
treating $G$ and $s$ as input to any of these algorithms. Each time we
visit a vertex that was previously unvisited, we increment the counter
$c$. At the end of the algorithm, we compare $c$ with $n$. If $c = n$,
we know that we have visited all vertices of $G$ and conclude that $G$
is connected. Otherwise, we conclude that $G$ is disconnected. This
procedure is summarized in
Algorithm~\ref{alg:graph_algorithms:graph_connectivity}.

\begin{algorithm}[!htpb]
\dontprintsemicolon  % no semicolon at end of pseudocode statements
%% data section
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKwData{False}{False}
\SetKwData{True}{True}
%% input/output
\Input{An undirected graph $G = (V, E)$ of order $n > 0$. A vertex $s$
  from which to start the search. The vertices are numbered from $1$
  to  $n = |V|$, i.e. $V = \{1, 2, \dots, n\}$.}
\Output{$\True$ if $G$ is connected; $\False$ otherwise.}
\BlankLine
$Q \leftarrow [s]$~\tcc*[f]{queue of nodes to visit}\;
$D \leftarrow [0, 0, \dots, 0]$~\tcc*[f]{$n$ copies of $0$}\;
$D[s] \leftarrow 1$\;
$c \leftarrow 1$\;
\While{$\length(Q) > 0$}{
  $v \leftarrow \dequeue(Q)$\;
  \For{\emph{each} $w \in \adj(v)$}{
    \If{$D[w] = 0$}{
      $D[w] \leftarrow 1$\;
      $c \leftarrow c + 1$\;
      $\enqueue(Q, w)$\;
    }
  }
}
\If{$c = |V|$~\nllabel{alg:BFS:connectivity_test}}{
  \Return \True\;
} \Else{
  \Return \False\;
}
\caption{Determining whether an undirected graph is connected.}
\label{alg:graph_algorithms:graph_connectivity}
\end{algorithm}

Note that Algorithm~\ref{alg:graph_algorithms:graph_connectivity} uses
the BFS template of
Algorithm~\ref{alg:graph_algorithms:breadth_first_search_template},
with some minor changes. Instead of initializing the list $D$ with
$n = |V|$ copies of $\infty$, we use $n$ copies of $0$. Each time we
have visited a vertex $w$, we make the assignment $D[w] \leftarrow 1$,
instead of incrementing the value $D[v]$ of $w$'s parent vertex and
assign that value to $D[w]$. At the end of the while loop, we have the
equality $c = \sum_{d \in D} d$. The value of this sum could be used
in the test starting from line~\ref{alg:BFS:connectivity_test}.
However, the value of the counter $c$ is incremented immediately after
we have visited an unvisited vertex. An advantage is that we do not
need to perform a separate summation outside of the while loop. To use
the DFS template for determining graph connectivity, we simply replace
the queue implementation in
Algorithm~\ref{alg:graph_algorithms:graph_connectivity} with a stack
implementation.


%%--- Problems ----------------------------------------------------------%%

\subsection*{Problems~\ref{sec:graph_algorithms:graph_searching}}

\begin{enumerate}
\item Let $G = (V, E)$ be an undirected graph, let $s \in V$, and $D$
  is the list of distances resulting from running
  Algorithm~\ref{alg:graph_algorithms:breadth_first_search_template}
  with $G$ and $s$ as input. Show that $G$ is connected if and only if
  $D[v]$ is defined for each $v \in V$.

\item Show that the worst-case time complexity of depth-first search
  Algorithm~\ref{alg:graph_algorithms:depth_first_search_template} is
  $O(|V| + |E|)$.

\item Let $D$ be the list of distances returned by
  Algorithm~\ref{alg:graph_algorithms:depth_first_search_template},
  let $s$ be a starting vertex, and let $v$ be a vertex such that
  $D[v] \neq \infty$. Show that $D[v]$ is the length of any shortest
  path from $s$ to $v$.
\end{enumerate}


%%-----------------------------------------------------------------------%%
%%--- Weights and distances ---------------------------------------------%%

\section{Weights and distances}

In Chapter~\ref{chap:introduction}, we briefly mentioned some
applications of weighted graphs, but we did not define the concept of
weighted graphs. A graph is said to be
\emph{weighted}\index{weighted graph} when we assign a numeric label
or weight to each of its edges. Depending on the application, we can
let the vertices represent physical locations and interpret the weight
of an edge as the distance separating two adjacent vertices. There
might be a cost involved in travelling from a vertex to one of its
neighbors, in which case the weight assigned to the corresponding edge
can represent such a cost. The concept of
\emph{weighted digraphs}\index{weighted digraph} can be similarly
defined. When no explicit weights are assigned to the edges of an
undirected graph or digraph, it is usually convenient to consider each
edge as having a weight of one.

Based on the concept of weighted graphs, we now define what it means
for a path to be a shortest path. Let $G = (V,E)$ be a (di)graph with
non-negative edge weights $w(e)$ for each edge $e \in E$. The
\emph{length}\index{path!length} or
\emph{distance}\index{path!distance} $d(P)$ of a path $P$ from
$v \in V$ to $w \in V$ is the sum of the edge weights for edges in
$P$. Denote by $d(v,w)$ the smallest value of $d(P)$ for all paths $P$
from $v$ to $w$. When we regard edge weights as physical distances, a
$v$-$w$ path that realizes $d(v,w)$ is sometimes called a
\emph{shortest path}\index{path!shortest} from $v$ to $w$.

The distance function $d$ on a graph with nonnegative edge weights is
known as a \emph{metric function}. Intuitively, the distance between
two physical locations is greater than zero. When these two locations
coincide, i.e. they are one and the same location, the distance
separating them is zero. Regardless of whether we are measuring the
distance from location $a$ to $b$ or from $b$ to $a$, we would obtain
the same distance. Imagine now a third location $c$. The distance from
$a$ to $b$ plus the distance from $b$ to $c$ is greater than or equal
to the distance from $a$ to $c$. The latter principle is known as the
\emph{triangle inequality}. In summary, given three vertices $u,v,w$
in a connected graph $G$, the distance function $d$ on $G$ satisfies
the following property.

\begin{lemma}
Let $G = (V,E)$ be a connected graph with a positive weight function
$w: E \longrightarrow \RR^{+}$. Define a distance function
$d: V \times V \longrightarrow \RR$ given by
\[
d(u,v)
=
\begin{cases}
\infty, & \text{if there are no paths from $u$ to $v$}, \\
\min\{w(W) \;|\; \text{$W$ is a $u$-$v$ walk}\}, & \text{otherwise}.
\end{cases}
\]
Then $d$ satisfies the following properties:
%
\begin{enumerate}
\item Nonnegativity: $d(u,v) \geq 0$ with $d(u,v) = 0$ if and only if
  $u = v$.

\item Symmetry: $d(u,v) = d(v,u)$.

\item Triangle inequality: $d(u,v) + d(v,w) \geq d(u,w)$.
\end{enumerate}
\end{lemma}

The pair $(V, d)$ is called a \emph{metric space}, where the word
``metric'' refers to the distance function $d$. Any graphs we consider
are assumed to have finite sets of vertices. For this reason, $(V,d)$
is also known as a \emph{finite metric space}. The distance matrix
$D = [d(v_i, v_j)]$ of a connected graph is the distance matrix of its
finite metric space. The topic of metric space is covered in further
details in topology texts such as Runde~\cite{Runde2005} and Shirali
and Vasudeva~\cite{ShiraliVasudeva2006}.

Many different algorithms exist for computing a shortest path in a
weighted graph. Some only work if the graph has no negative weight
cycles. Some assume that there is a single start or source
vertex. Some compute the shortest paths from any vertex to any other,
and also detect if the graph has a negative weight cycle. No matter
what algorithm is used for the special case of non-negative weights,
the length of the shortest path can neither equal nor exceed the order
of the graph.

\begin{lemma}
\label{lem:graph_algorithms:shortest_path_length}
Fix a vertex $v$ in a connected graph $G = (V,E)$ of order
$n = |V|$. If there are no negative weight cycles in $G$, then there
exists a shortest path from $v$ to any other vertex $w \in V$ that
uses at most $n - 1$ edges.
\end{lemma}

\begin{proof}
Suppose that $G$ contains no negative weight cycles. Observe that at
most $n - 1$ edges are required to construct a path from $v$ to any
vertex $w$
(Corollary~\ref{cor:introduction:any_path_has_length_at_most_n_minus_1}).
Let $P$ denote such a path:
\[
P: v_0 = v,\, v_1,\, v_2, \dots, v_k = w.
\]
Since $G$ has no negative weight cycles, the weight of $P$ is no less
than the weight of $P'$, where $P'$ is the same as $P$ except that all
cycles have been removed. Thus, we can remove all cycles from $P$ and
obtain a $v$-$w$ path $P'$ of lower weight. Since the final path is
acyclic, it must have no more than $n - 1$ edges.
\end{proof}

\begin{algorithm}[!htpb]
\dontprintsemicolon  % no semicolon at end of pseudocode statements
%% data section
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
%% input/output
\Input{A weighted graph or digraph $G = (V, E)$, where the vertices
  are numbered as $V = \{1, 2, \dots, n\}$. A starting vertex $s$.}
\Output{A list $D$ of distances from $s$ to all other vertices. A list
  $P$ of parent vertices such that $P[v]$ is the parent of $v$.}
\BlankLine
$D \leftarrow [\infty, \infty, \dots, \infty]$~\tcc*[f]{$n$ copies of $\infty$}\;
let $C$ be a list of candidate vertices to visit\;
\While{$\length(C) > 0$}{
  select $v \in C$\;
  $C \leftarrow \remove(C, v)$\;
  \For{\emph{each} $u \in \adj(v)$~\nllabel{alg:generic_shortest_path:neighbors}}{
    \If{$D[u] > D[v] + w(vu)$}{
      $D[u] \leftarrow D[v] + w(vu)$\;
      $P[u] \leftarrow v$\;
      if $u$ is not in $C$, add $u$ to $C$\;
    }
  }
}
\Return $(D,P)$\;
\caption{A template for shortest path algorithms.}
\label{alg:graph_algorithms:generic_shortest_path_algorithm}
\end{algorithm}

Having defined weights and distances, we are now ready to discuss
shortest path algorithms for weighted graphs. The breadth-first search
Algorithm~\ref{alg:graph_algorithms:breadth_first_search_template} can
be applied where each edge has unit weight. Moving on to the general
case of graphs with positive edge weights, algorithms for determining
shortest paths in such graphs can be classified as
\emph{weight-setting} or
\emph{weight-corrrecting}~\cite{GalloPallottino1986}. A weight-setting
method traverses a graph and assigns weights that, once assigned,
remain unchanged for the duration of the algorithm. Weight-setting
algorithms cannot deal with negative weights. On the other hand, a
weight-correcting method is able to change the value of a weight many
times while traversing a graph. In contrast to a weight-setting
algorithm, a weight-correcting algorithm is able to deal with negative
weights, provided that the weight sum of any cycle is
nonnegative. The term \emph{negative cycle} refers to the weight sum $s$
of a cycle such that $s < 0$.

Algorithm~\ref{alg:graph_algorithms:generic_shortest_path_algorithm}
is a general template for many shortest path algorithms. With a tweak
here and there, one could modify it to suit the problem at hand. Note
that $w(vu)$ is the weight of the edge $vu$. If the input graph is
undirected, line~\ref{alg:generic_shortest_path:neighbors} considers
all the neighbors of $v$. For digraphs, we are interested in
out-neighbors of $v$ and accordingly we replace ``$u \in \adj(v)$'' in
line~\ref{alg:generic_shortest_path:neighbors} with
``$u \in \oadj(v)$''. The general flow of
Algorithm~\ref{alg:graph_algorithms:generic_shortest_path_algorithm}
follows the same pattern as depth-first and breadth-first searches.


%%-----------------------------------------------------------------------%%
%%--- Dijkstra's algorithm ----------------------------------------------%%

\section{Dijkstra's algorithm}
\label{sec:graph_algorithms:Dijkstra_algorithm}

Dijkstra's algorithm~\cite{Dijkstra1959}, discovered by E. W.~Dijkstra
in 1959, is a graph search algorithm that solves the single-source
shortest path problem for a graph with nonnegative edge
weights. Imagine that the vertices of a weighted graph represent
cities and edge weights represent distances between pairs of cities
connected by a direct road. Dijkstra's algorithm can be used to find a
shortest route from a fixed city to any other city.

Let $G = (V,E)$ be a (di)graph with nonnegative edge weights. Fix a
start or source vertex $s \in V$. Dijkstra's
Algorithm~\ref{alg:graph_algorithms:dijkstra_general} performs a
number of steps, basically one step for each vertex in $V$. First, we
initialize a list $D$ with $n$ copies of $\infty$ and then assign $0$
to $D[s]$. The purpose of the symbol $\infty$ is to denote the largest
possible value. The list $D$ is to store the distances of all shortest
paths from $s$ to any other vertices in $G$, where we take the
distance of $s$ to itself to be zero. The list $P$ of parent vertices
is initially empty and the queue $Q$ is initialized to all vertices in
$G$. We now consider each vertex in $Q$, removing any vertex after we
have visited it. The while loop starting on
line~\ref{alg:dijkstra_general:while_loop} runs until we have visited
all
vertices. Line~\ref{alg:dijkstra_general:find_vertex_minimal_distance}
chooses which vertex to visit, preferring a vertex $v$ whose distance
value $D[v]$ from $s$ is minimal. After we have determined such a
vertex $v$, we remove it from the queue $Q$ to signify that we have
visited $v$. The for loop starting on
line~\ref{alg:dijkstra_general:for_loop} adjusts the distance values
of each neighbor $u$ of $v$ such that $u$ is also in $Q$. If $G$ is
directed, we only consider out-neighbors of $v$ that are also in
$Q$. The conditional starting on
line~\ref{alg:dijkstra_general:if_relaxation} is where the adjustment
takes place. The expression $D[v] + w(vu)$ sums the distance from $s$
to $v$ and the distance from $v$ to $u$. If this total sum is less
than the distance $D[u]$ from $s$ to $u$, we assign this lesser
distance to $D[u]$ and let $v$ be the parent vertex of $u$. In this
way, we are choosing a neighbor vertex that results in minimal
distance from $s$. Each pass through the while loop decreases the
number of elements in $Q$ by one without adding any elements to
$Q$. Eventually, we would exit the while loop and the algorithm
returns the lists $D$ and $P$.

\begin{algorithm}[!htpb]
\dontprintsemicolon  % no semicolon at end of pseudocode statements
%% data section
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
%% input/output
\Input{An undirected or directed graph $G = (V, E)$ that is weighted
  and has no self-loops. The order of $G$ is $n > 0$. A vertex $s \in V$
  from which to start the search. Vertices are numbered from 1 to $n$,
  i.e. $V = \{1, 2, \dots, n\}$.}
\Output{A list $D$ of distances such that $D[v]$ is the distance of a
  shortest path from $s$ to $v$. A list $P$ of vertex parents such
  that $P[v]$ is the parent of $v$, i.e. $v$ is adjacent from $P[v]$.}
\BlankLine
%% algorithm body
$D \leftarrow [\infty, \infty, \dots, \infty]$~\tcc*[f]{$n$ copies of $\infty$}\;
$D[s] \leftarrow 0$\;
$P \leftarrow [\,]$\;
$Q \leftarrow V$~\tcc*[f]{list of nodes to visit}\;
\While{$\length(Q) > 0$~\nllabel{alg:dijkstra_general:while_loop}}{
  find $v \in Q$ such that $D[v]$ is minimal~\nllabel{alg:dijkstra_general:find_vertex_minimal_distance}\;
  $Q \leftarrow \remove(Q, v)$\;
  \For{\emph{each} $u \in \adj(v) \cap Q$~\nllabel{alg:dijkstra_general:for_loop}}{
    \If{$D[u] > D[v] + w(vu)$~\nllabel{alg:dijkstra_general:if_relaxation}}{
      $D[u] \leftarrow D[v] + w(vu)$\;
      $P[u] \leftarrow v$\;
    }
  }
}
\Return $(D, P)$\;
\caption{A general template for Dijkstra's algorithm.}
\label{alg:graph_algorithms:dijkstra_general}
\end{algorithm}

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  arrowdecorate/.style={->,>=stealth,thick}]
%% nodes or vertices
\foreach \nodename/\x/\y/\direction/\navigate in {
  v_1/0/0/below/south, v_2/2/2.5/above/north, v_3/4/0/below/south,
  v_4/6/2.5/above/north, v_5/8/0/below/south} {
  \node (\nodename) at (\x,\y) [nodedecorate] {};
  \node [\direction] at (\nodename.\navigate) {$\nodename$};
}
%% edges or lines
\path
\foreach \startnode/\endnode/\direction/\weight in {
  v_1/v_2/left/10, v_1/v_3/below/3, v_2/v_4/above/2, v_3/v_4/left/8,
  v_3/v_5/below/2} {
  (\startnode) edge[arrowdecorate] node[\direction]{$\weight$} (\endnode)
}
\foreach \startnode/\endnode/\direction/\weight in {
  v_3/v_2/left/4, v_2/v_3/right/1, v_4/v_5/right/7, v_5/v_4/left/9} {
  (\startnode) edge[arrowdecorate,bend left] node[\direction]{$\weight$} (\endnode)
};
\end{tikzpicture}
\caption{Searching a weighted digraph using Dijkstra's algorithm.}
\label{fig:graph_algorithms:Dijkstra_algorithm_digraph}
\end{figure}
%% sage: M = matrix([[0,10,3,0,0],[0,0,1,2,0],[0,4,0,8,2],[0,0,0,0,7],[0,0,0,9,0]])
%% sage: D = DiGraph(M, format="weighted_adjacency_matrix")
%% sage: D.plot(edge_labels=True, graph_border=True).show()
%%
%% sage: G = DiGraph({1: {2:10, 3:3},
%% ....: 2: {3:1, 4:2},
%% ....: 3: {2:4, 4:8, 5:2},
%% ....: 4: {5:7},
%% ....: 5: {4:9}}, implementation=''c_graph'')
%% sage: G.shortest_paths(1, by_weight=True)
%% {1: [1], 2: [1, 3, 2], 3: [1, 3], 4: [1, 3, 2, 4], 5: [1, 3, 5]}

\begin{table}[!htbp]
\centering
\begin{tabular}{|ccccc|} \hline
$v_1$               & $v_2$                 & $v_3$                 & $v_4$                 & $v_5$ \\\hline\hline
\underline{$(0,-)$} & $(\infty,-)$          & $(\infty,-)$          & $(\infty,-)$          & $(\infty,-)$ \\
                    & $(10,v_1)$            & \underline{$(3,v_1)$} & $(11,v_3)$            & \underline{$(5,v_3)$} \\
                    & \underline{$(7,v_3)$} &                       & \underline{$(9,v_2)$} & \\\hline
\end{tabular}
\caption{Stepping through Dijkstra's algorithm.}
\label{tab:graph_algorithms:working_through_Dijkstra_algorithm}
\end{table}

\begin{example}
Apply Dijkstra's algorithm to the graph in
Figure~\ref{fig:graph_algorithms:Dijkstra_algorithm_digraph}, with
starting vertex $v_1$.
\end{example}

\begin{proof}[Solution]
Dijkstra's Algorithm~\ref{alg:graph_algorithms:dijkstra_general}
applied to the graph in
Figure~\ref{fig:graph_algorithms:Dijkstra_algorithm_digraph} yields
Table~\ref{tab:graph_algorithms:working_through_Dijkstra_algorithm}. For
any column $v_i$ in the table, each 2-tuple represents the distance
and parent vertex of $v_i$. As we move along the graph, processing
vertices according to Dijkstra's algorithm, the distance and parent
vertex of a column are updated. The underlined 2-tuple represents the
final distance and parent vertex produced by Dijkstra's
algorithm. From
Table~\ref{tab:graph_algorithms:working_through_Dijkstra_algorithm},
we have the following shortest paths and distances:
\[
\begin{array}{ll}
v_1\text{-}v_2: v_1, v_3, v_2      &\quad d(v_1, v_2) = 7 \\[4pt]
v_1\text{-}v_3: v_1, v_3           &\quad d(v_1, v_3) = 3 \\[4pt]
v_1\text{-}v_4: v_1, v_3, v_2, v_4 &\quad d(v_1, v_4) = 9 \\[4pt]
v_1\text{-}v_5: v_1, v_3, v_5      &\quad d(v_1, v_5) = 5
\end{array}
\]
Intermediary vertices for a $u$-$v$ path are obtained by starting from
$v$ and work backward using the parent of $v$, then the parent of the
parent, and so on.
\end{proof}

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  arrowdecorate/.style={->,>=stealth,thick}]
%% nodes or vertices
\foreach \nodename/\x/\y/\direction/\navigate in {v_1/4/0/below/south,
  v_2/0/0/below/south, v_3/0/3/above/north, v_4/4/3/above/north,
  v_5/7/1.5/right/east} {
  \node (\nodename) at (\x,\y) [nodedecorate] {};
  \node [\direction] at (\nodename.\navigate) {$\nodename$};
}
%% edges or lines
\path
\foreach \startnode/\endnode/\direction/\weight in {v_1/v_2/below/1,
  v_1/v_5/below/6, v_2/v_4/right/3, v_3/v_4/above/1, v_4/v_1/right/3} {
  (\startnode) edge[arrowdecorate] node[\direction]{$\weight$} (\endnode)
}
\foreach \startnode/\endnode/\direction/\angle/\weight in {
  v_3/v_2/right/20/2, v_3/v_1/above left/15/1, v_2/v_3/left/20/1,
  v_1/v_3/below right/15/3, v_4/v_5/above/15/2, v_5/v_4/below/15/1} {
  (\startnode) edge[arrowdecorate,bend left=\angle] node[\direction]{$\weight$} (\endnode)
};
\end{tikzpicture}
\caption{Searching a directed house graph using Dijkstra's algorithm.}
\label{fig:graph_algorithms:Dijkstra_directed_house_graph}
\end{figure}
%sage: M = matrix([[0,1,3,0,6],[0,0,1,3,0],[1,2,0,1,0],[3,0,0,0,2],[0,0,0,1,0]])
%sage: D = DiGraph(M, format="weighted_adjacency_matrix")
%sage: D.plot(edge_labels=True, graph_border=True).show()

\begin{theorem}
\textbf{Correctness of
  Algorithm~\ref{alg:graph_algorithms:dijkstra_general}.}
Let $G = (V, E)$ be a weighted (di)graph with a nonnegative weight
function $w$. When Dijkstra's algorithm is applied to $G$ with source
vertex $s \in V$, the algorithm terminates with $D[u] = d(s,u)$ for
all $u \in V$. Furthermore, if $D[v] \neq \infty$ and $v \neq s$, then
$s=u_1, u_2, \dots, u_k = v$ is a shortest $s$-$v$ path such that
$u_{i-1} = P[u_i]$ for $i = 2,3,\dots,k$.
\end{theorem}

\begin{proof}
If $G$ is disconnected, then any $v \in V$ that cannot be reached from
$s$ has distance $D[v] = \infty$ upon algorithm termination. Hence, it
suffices to consider the case where $G$ is connected. Let
$V = \{s=v_1, v_2, \dots, v_n\}$ and use induction on $i$ to show that
after visiting $v_i$ we have
%
\begin{equation}
\label{eq:graph_algorithms:Dijkstra:shortest_distance}
D[v]
=
d(s,v)
\qquad
\text{for all $v \in V_i = \{v_1, v_2, \dots, v_i\}$}.
\end{equation}
%
For $i = 1$, equality holds. Assume for induction
that~(\ref{eq:graph_algorithms:Dijkstra:shortest_distance}) holds for
some $1 \leq i \leq n - 1$, so that now our task is to show
that~(\ref{eq:graph_algorithms:Dijkstra:shortest_distance}) holds for
$i + 1$. To verify $D[v_{i+1}] = d(s, v_{i+1})$, note that by our
inductive hypothesis,
\[
D[v_{i+1}]
=
\min \left\{
\left. d(s,v) + w(vu) \;\right|\;
v \in V_i \text{ and } u \in \adj(v) \cap (Q \backslash V_i)
\right\}
\]
and respectively
\[
D[v_{i+1}]
=
\min \left\{
\left. d(s,v) + w(vu) \;\right|\;
v \in V_i \text{ and } u \in \oadj(v) \cap (Q \backslash V_i)
\right\}
\]
if $G$ is directed. Therefore, $D[v_{i+1}] = d(s, v_{i+1})$.

Let $v \in V$ such that $D[v] \neq \infty$ and $v \neq s$. We now
construct an $s$-$v$ path. When
Algorithm~\ref{alg:graph_algorithms:dijkstra_general} terminates, we
have $D[v] = D[v_1] + w(v_1 v)$, where $P[v] = v_1$ and
$d(s,v) = d(s, v_1) + w(v_1 v)$. This means that $v_1$ is the
second-to-last vertex in a shortest $s$-$v$ path. Repeated application
of this process using the parent list $P$, we eventually produce a
shortest $s$-$v$ path $s=v_m, v_{m-1}, \dots, v_1, v$, where
$P[v_i] = v_{i+1}$ for $i = 1, 2, \dots, m - 1$.
\end{proof}

To analyze the worst case time complexity of
Algorithm~\ref{alg:graph_algorithms:dijkstra_general}, note that
initializing $D$ takes $O(n + 1)$ and initializing $Q$ takes $O(n)$,
for a total of $O(n)$ devoted to initialization. Each extraction of a
vertex $v$ with minimal $D[v]$ requires $O(n)$ since we search through
the entire list $Q$ to determine the minimum value, for a total of
$O(n^2)$. Each insertion into $D$ requires constant time and the same
holds for insertion into $P$. Thus, insertion into $D$ and $P$ takes
$O(|E| + |E|) = O(|E|)$, which require at most $O(n)$ time. In the
worst case, Dijkstra's
Algorithm~\ref{alg:graph_algorithms:dijkstra_general} has running time
$O(n^2 + n) = O(n^2)$.


%%--- Problems ----------------------------------------------------------%%

\subsection*{Problems~\ref{sec:graph_algorithms:Dijkstra_algorithm}}

\begin{enumerate}
\item Consider the graph in
  Figure~\ref{fig:graph_algorithms:Dijkstra_algorithm_digraph} as
  undirected. Run this undirected version through Dijkstra's algorithm
  with starting vertex $v_1$.

\item Consider the graph in
  Figure~\ref{fig:graph_algorithms:Dijkstra_directed_house_graph}. Choose
  any vertex as a starting vertex and run Dijkstra's algorithm over
  it. Now consider the undirected version of that digraph and repeat
  the exercise.
\end{enumerate}


%%-----------------------------------------------------------------------%%
%%--- Bellman-Ford algorithm --------------------------------------------%%

\subsection{Bellman-Ford algorithm}

See section~24.1 of Cormen~et~al.~\cite{CormenEtAl2001}, and
section~8.5 of Berman and Paul~\cite{BermanPaul1997}.

The Bellman-Ford algorithm computes single-source shortest paths in a
weighted graph or digraph, where some of the edge weights may be
negative. Instead of the ``greedy'' approach that Dijkstra's algorithm
took, i.e. searching for the ``cheapest'' path, the Bellman-Ford
algorithm searches over all edges and keeps track of the shortest one
found as it searches.

The implementation below takes in a graph or digraph, and creates two
Python dictionaries \verb!dist! and \verb!predecessor!, keyed on the
list of vertices, which store the distance and shortest
paths. However, if a negative weight cycle exists~(in the case of a
digraph), then an error is raised.

\begin{center}
\fontsize{9pt}{9pt}
\selectfont
\tt
\begin{lstlisting}
def bellman_ford(Gamma, s):
    """
    Computes the shortest distance from s to all other vertices in Gamma.
    If Gamma has a negative weight cycle, then return an error.

    INPUT:

    - Gamma -- a graph.
    - s -- the source vertex.

    OUTPUT:

    - (d,p) -- pair of dictionaries keyed on the list of vertices,
      which store the distance and shortest paths.

    REFERENCE:

    http://en.wikipedia.org/wiki/Bellman-Ford_algorithm
    """
    P = []
    dist = {}
    predecessor = {}
    V = Gamma.vertices()
    E = Gamma.edges()
    for v in V:
        if v == s:
            dist[v] = 0
        else:
            dist[v] = infinity
        predecessor[v] = 0
    for i in range(1, len(V)):
        for e in E:
            u = e[0]
            v = e[1]
            wt = e[2]
            if dist[u] + wt < dist[v]:
                dist[v] = dist[u] + wt
                predecessor[v] = u
    # check for negative-weight cycles
    for e in E:
        u = e[0]
        v = e[1]
        wt = e[2]
        if dist[u] + wt < dist[v]:
            raise ValueError("Graph contains a negative-weight cycle")
    return dist, predecessor
\end{lstlisting}
\end{center}

Bellman-Ford runs in $O(|V|\cdot |E|)$-time, which is $O(n^3)$ for
``dense'' connected graphs (where $n=|V|$).

Here are some examples.

\begin{center}
\fontsize{9pt}{9pt}
\selectfont
\tt
\begin{lstlisting}
sage: M = matrix([[0,1,4,0], [0,0,1,5], [0,0,0,3], [0,0,0,0]])
sage: G = Graph(M, format="weighted_adjacency_matrix")
sage: bellman_ford(G, G.vertices()[0])
  {0: 0, 1: 1, 2: 2, 3: 5}
\end{lstlisting}
\end{center}
%
The plot of this graph is given in
Figure~\ref{fig:graph_algorithms:Bellman_Ford_example}.

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  linedecorate/.style={-,thick}]
% nodes or vertices
\node (0) at (0,0) [nodedecorate] {};
\node [below] at (0.south) {$0$};
\node (2) at (4,0) [nodedecorate] {};
\node [below] at (2.south) {$2$};
\node (1) at (1,2.5) [nodedecorate] {};
\node [above] at (1.north) {$1$};
\node (3) at (5,2.5) [nodedecorate] {};
\node [above] at (3.north) {$3$};
% edges or lines
\path
(0) edge[linedecorate] node[left]{$1$} (1)
(0) edge[linedecorate] node[below]{$4$} (2)
(1) edge[linedecorate] node[right]{$1$} (2)
(1) edge[linedecorate] node[above]{$5$} (3)
(2) edge[linedecorate] node[right]{$3$} (3);
\end{tikzpicture}
\caption{Shortest paths in a weighted graph using the Bellman-Ford
  algorithm.}
\label{fig:graph_algorithms:Bellman_Ford_example}
\end{figure}
%sage: M = matrix([[0,1,4,0],[0,0,1,5],[0,0,0,3],[0,0,0,0]])
%sage: G = Graph(M, format = "weighted_adjacency_matrix")
%sage: G.plot(graph_border=True, edge_labels=True).show()

The following example illustrates the case of a negative-weight cycle.

\begin{center}
\fontsize{9pt}{9pt}
\selectfont
\tt
\begin{lstlisting}
sage: M = matrix([[0,1,0,0],[1,0,-4,1],[1,1,0,0],[0,0,1,0]])
sage: G = DiGraph(M, format = "weighted_adjacency_matrix")
sage: bellman_ford(G, G.vertices()[0])
---------------------------------------------------------------------------
...
ValueError: Graph contains a negative-weight cycle
\end{lstlisting}
\end{center}
%
The plot of this graph is given in
Figure~\ref{fig:graph_algorithms:Bellman_Ford_negative_weights}.

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  arrowdecorate/.style={->,>=stealth,thick}]
% nodes or vertices
\node (0) at (5,0) [nodedecorate] {};
\node [below] at (0.south) {$0$};
\node (1) at (1,0) [nodedecorate] {};
\node [below] at (1.south) {$1$};
\node (2) at (4,3) [nodedecorate] {};
\node [above] at (2.north) {$2$};
\node (3) at (0,3) [nodedecorate] {};
\node [above] at (3.north) {$3$};
% edges or lines
\path
(0) edge[arrowdecorate,bend left=15] node[below]{$1$} (1)
(1) edge[arrowdecorate,bend left=10] node[above]{$1$} (0)
(1) edge[arrowdecorate,bend left=15] node[left]{$-4$} (2)
(1) edge[arrowdecorate] node[left]{$1$} (3)
(2) edge[arrowdecorate] node[right]{$1$} (0)
(2) edge[arrowdecorate,bend left=15] node[right]{$1$} (1)
(3) edge[arrowdecorate] node[above]{$1$} (2);
\end{tikzpicture}
\caption{Searching a digraph with negative weight using the
  Bellman-Ford algorithm.}
\label{fig:graph_algorithms:Bellman_Ford_negative_weights}
\end{figure}
%sage: M = matrix([[0,1,0,0],[1,0,-4,1],[1,1,0,0],[0,0,1,0]])
%sage: G = Graph(M, format = "weighted_adjacency_matrix")
%sage: G.plot(graph_border=True, edge_labels=True).show()


%%-----------------------------------------------------------------------%%
%%--- Floyd-Warshall algorithm ------------------------------------------%%

\subsection{Floyd-Roy-Warshall algorithm}

See section~25.2 of Cormen~et~al.~\cite{CormenEtAl2001}, and section
14.4 of Berman and Paul~\cite{BermanPaul1997}.

The \emph{Floyd-Roy-Warshall algorithm}~(FRW), or the Floyd-Warshall
algorithm, is an algorithm for finding shortest paths in a weighted,
directed graph. Like the Bellman-Ford algorithm, it allows for
negative edge weights and detects a negative weight cycle if one
exists. Assuming that there are no negative weight cycles, a single
execution of the FRW algorithm will find the shortest paths between
all pairs of vertices. It was discovered independently by Bernard Roy
in 1959, Robert Floyd in 1962, and by Stephen Warshall in 1962.

In some sense, the FRW algorithm is an example of
``dynamic programming,'' which allows one to break the computation
into simpler steps using some sort of recursive procedure. The rough
idea is as follows. Temporarily label the vertices of $G$ as
$V = \{1,2,\dots,n\}$. Call $SD(i,j,k)$ a shortest distance from
vertex $i$ to vertex $j$ that only uses vertices $1$ through $k$. This
can be computed using the recursive expression
\[
SD(i,j,k)
=
\min\{ SD(i,j,k-1),\, SD(i,k,k-1) + SD(k,j,k-1)\}.
\]
The key to the Floyd-Roy-Warshall algorithm lies in exploiting this
formula. If $n = |V|$, then this is a $O(n^3)$ time algorithm. For
comparison, the Bellman-Ford algorithm has complexity
$O(|V| \cdot |E|)$, which is $O(n^3)$ time for ``dense''
graphs. However, Bellman-Ford only yields the shortest paths emanating
from a \emph{single} vertex. To achieve comparable output, we would
need to iterate Bellman-Ford over \emph{all} vertices, which would be
an  $O(n^4)$ time algorithm for ``dense'' graphs. Except possibly for
``sparse'' graphs, Floyd-Roy-Warshall is better than an iterated
implementation of Bellman-Ford.

Here is an implementation in Sage.
%
\begin{center}
\fontsize{9pt}{9pt}
\selectfont
\tt
\begin{lstlisting}
def floyd_roy_warshall(A):
    """
    Shortest paths

    INPUT:

    - A -- weighted adjacency matrix

    OUTPUT:

    - dist -- a matrix of distances of shortest paths.
    - paths -- a matrix of shortest paths.
    """
    G = Graph(A, format="weighted_adjacency_matrix")
    V = G.vertices()
    E = [(e[0],e[1]) for e in G.edges()]
    n = len(V)
    dist = [[0]*n for i in range(n)]
    paths = [[-1]*n for i in range(n)]
    # initialization step
    for i in range(n):
        for j in range(n):
            if (i,j) in E:
                paths[i][j] = j
            if i == j:
                dist[i][j] = 0
            elif A[i][j]<>0:
                dist[i][j] = A[i][j]
            else:
                dist[i][j] = infinity
    # iteratively finding the shortest path
    for j in range(n):
        for i in range(n):
            if i <> j:
                for k in range(n):
                    if k <> j:
                        if dist[i][k]>dist[i][j]+dist[j][k]:
                            paths[i][k] = V[j]
                        dist[i][k] = min(dist[i][k], dist[i][j] +dist[j][k])
    for i in range(n):
        if dist[i][i] < 0:
            raise ValueError, "A negative edge weight cycle exists."
    return dist, matrix(paths)
\end{lstlisting}
\end{center}

Here are some examples.

%
%\begin{center}
%\fontsize{9pt}{9pt}
%\selectfont
%\tt
%\begin{lstlisting}
%
%        sage: A = matrix([[0,1,2,3],[0,0,2,1],[20,10,0,3],[11,12,13,0]]); A
%        sage: floyd_roy_warshall(A)
%        ([[0, 1, 2, 2], [12, 0, 2, 1], [14, 10, 0, 3], [11, 12, 13, 0]],
%          [-1  1  2  1]
%          [ 3 -1  2  3]
%          [ 3 -1 -1  3]
%          [-1 -1 -1 -1])
%
%\end{lstlisting}
%\end{center}
%

%
%\begin{center}
%\fontsize{9pt}{9pt}
%\selectfont
%\tt
%\begin{lstlisting}
%
%        sage: A = matrix([[0,1,2,4],[0,0,2,1],[0,0,0,5],[0,0,0,0]])
%        sage: floyd_roy_warshall(A)
%        ([[0, 1, 2, 2], [+Infinity, 0, 2, 1], [+Infinity, +Infinity, 0, 5],
%          [+Infinity, +Infinity, +Infinity, 0]],
%          [-1  1  2  1]
%          [-1 -1  2  3]
%          [-1 -1 -1  3]
%          [-1 -1 -1 -1])
%
%\end{lstlisting}
%\end{center}
%

\begin{center}
\fontsize{9pt}{9pt}
\selectfont
\tt
\begin{lstlisting}
sage: A = matrix([[0,1,2,3],[0,0,2,1],[-5,0,0,3],[1,0,1,0]]); A
sage: floyd_roy_warshall(A)
Traceback (click to the left of this block for traceback)
...
ValueError: A negative edge weight cycle exists.
\end{lstlisting}
\end{center}

The plot of this weighted digraph with four vertices appears in
Figure~\ref{fig:graph_algorithms:Floyd_Roy_Warshall_demo}.

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  arrowdecorate/.style={->,>=stealth,thick}]
% nodes or vertices
\node (0) at (0,0) [nodedecorate] {};
\node [below] at (0.south) {$0$};
\node (1) at (6,0) [nodedecorate] {};
\node [below] at (1.south) {$1$};
\node (2) at (6,5) [nodedecorate] {};
\node [above] at (2.north) {$2$};
\node (3) at (0,5) [nodedecorate] {};
\node [above] at (3.north) {$3$};
% edges or lines
\path
(0) edge[arrowdecorate] node[below]{$1$} (1)
(0) edge[arrowdecorate,bend left=15] node[below left]{$2$} (2)
(0) edge[arrowdecorate,bend left=15] node[left]{$3$} (3)
(1) edge[arrowdecorate] node[right]{$2$} (2)
(1) edge[arrowdecorate] node[right]{$1$} (3)
(2) edge[arrowdecorate,bend left=15] node[above right]{$5$} (0)
(2) edge[arrowdecorate,bend left=10] node[below]{$3$} (3)
(3) edge[arrowdecorate,bend left=15] node[right]{$1$} (0)
(3) edge[arrowdecorate,bend left=10] node[above]{$1$} (2);
\end{tikzpicture}
\caption{Demonstrating the Floyd-Roy-Warshall algorithm.}
\label{fig:graph_algorithms:Floyd_Roy_Warshall_demo}
\end{figure}
%sage: A = matrix([[0,1,2,3],[0,0,2,1],[-5,0,0,3],[1,0,1,0]])
%sage: D = DiGraph(A, format="weighted_adjacency_matrix")
%sage: D.plot(edge_labels=True, graph_border=True).show()

\begin{center}
\fontsize{9pt}{9pt}
\selectfont
\tt
\begin{lstlisting}
sage: A = matrix([[0,1,2,3],[0,0,2,1],[-1/2,0,0,3],[1,0,1,0]]); A
sage: floyd_roy_warshall(A)
([[0, 1, 2, 2], [3/2, 0, 2, 1], [-1/2, 1/2, 0, 3/2], [1/2, 3/2, 1, 0]],
  [-1  1  2  1]
  [ 2 -1  2  3]
  [-1  0 -1  1]
  [ 2  2 -1 -1])
\end{lstlisting}
\end{center}

The plot of this weighted digraph with four vertices appears in
Figure~\ref{fig:graph_algorithms:another_Floyd_Roy_Warshall_demo}.

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  arrowdecorate/.style={->,>=stealth,thick}]
% nodes or vertices
\node (0) at (0,0) [nodedecorate] {};
\node [below] at (0.south) {$0$};
\node (1) at (0,7) [nodedecorate] {};
\node [above] at (1.north) {$1$};
\node (2) at (4,3.5) [nodedecorate] {};
\node [right] at (2.east) {$2$};
\node (3) at (-4,3.5) [nodedecorate] {};
\node [left] at (3.west) {$3$};
% edges or lines
\path
(0) edge[arrowdecorate] node[left]{$1$} (1)
(0) edge[arrowdecorate,bend left=10] node[above]{$2$} (2)
(0) edge[arrowdecorate,bend left=15] node[below]{$3$} (3)
(1) edge[arrowdecorate] node[above]{$2$} (2)
(1) edge[arrowdecorate] node[above]{$1$} (3)
(2) edge[arrowdecorate,bend left=15] node[below]{$-1/2$} (0)
(2) edge[arrowdecorate,bend left=10] node[below right]{$3$} (3)
(3) edge[arrowdecorate,bend left=10] node[above]{$1$} (0)
(3) edge[arrowdecorate,bend left=10] node[above right]{$1$} (2);
\end{tikzpicture}
\caption{Another demonstration of the Floyd-Roy-Warshall algorithm.}
\label{fig:graph_algorithms:another_Floyd_Roy_Warshall_demo}
\end{figure}
%sage: A = matrix([[0,1,2,3],[0,0,2,1],[-1/2,0,0,3],[1,0,1,0]])
%sage: D = DiGraph(A, format="weighted_adjacency_matrix")
%sage: D.plot(edge_labels=True, graph_border=True).show()



%%-----------------------------------------------------------------------%%
%%--- Johnson's algorithm -----------------------------------------------%%

\subsection{Johnson's algorithm}

See section~25.3 of Cormen~et~al.~\cite{CormenEtAl2001} and
Johnson~\cite{Johnson1977}.

Let $G = (V,E)$ be a graph with edge weights but no negative cycles.
\emph{Johnson's algorithm} finds a shortest path between all pairs of
vertices in a ``sparse'' directed graph.
\index{Johnson's algorithm}

%\begin{itemize}
%\item
%Add a new vertex $v_0$ with zero weight edges from it to all $v\in V$.
%
%\item
%Run the Bellman-Ford algorithm to check for negative weight cycles
%and find $h(v)$,
%the least weight of a path from the new node $v_0$ to $v\in V$.
%If this step detects a negative cycle, the algorithm is terminated.
%
%\item
%Reweight the edges using the vertices' $h(v)$ values: an edge from
%$v\in V$ to $w\in V$, having length $wt(v,w)$, is given the new length
%$wt(v,w) + h(v) - h(w)$.
%
%\item
%For each $v\in V$, run Dijkstra's algorithm and store the computed
%least weight to other vertices.
%\end{itemize}

\begin{algorithm}[!htpb]
\dontprintsemicolon  % no semicolon at end of pseudocode statements
%% data section
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKwData{Count}{count}
\SetKwData{False}{False}
\SetKwData{True}{True}
%% input/output
\Input{A connected graph $G = (V, E)$ having (possibly negative) edge
  weights.}
\Output{A shortest path between all pairs of vertices in $V$
  (or terminate if a negative edge cycle is detected).}
\BlankLine
%% algorithm body
Add a new vertex $v_0$ with zero weight edges from it to all $v \in V$.\;

Run the Bellman-Ford algorithm to check for negative weight cycles
and find $h(v)$, the least weight of a path from the new node $v_0$ to
$v \in V$.

If the last step detects a negative cycle, the algorithm is terminated.\;

Re-weight the edges using the vertices' $h(v)$ values: an edge from
$v \in V$ to $w \in V$, having length $wt(v,w)$, is given the new length
$wt(v,w) + h(v) - h(w)$.\;

For each $v \in V$, run Dijkstra's algorithm and store the computed
least weight to other vertices.
\caption{Johnson's algorithm.}
\label{alg:graph_algorithms:johnson}
\end{algorithm}

The time complexity, for sparse graphs, is
$O(|V|^2\log |V| + |V| \cdot |E)|=O(n^2\log n)$, where $n = |V|$ is
the number of vertices of the original graph $G$.
