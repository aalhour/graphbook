%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Graph Algorithms}
\label{chap:graph_algorithms}

Graph algorithms have many applications. Suppose you are a salesman
with a product you would like to sell in several cities. To determine
the cheapest travel route from city-to-city, you must effectively
search a graph having weighted edges for the ``cheapest'' route
visiting each city once. Each vertex denotes a city you must visit and
each edge has a weight indicating either the distance from one city to
another or the cost to travel from one city to another.

Shortest path algorithms are some of the most important algorithms in
algorithmic graph theory. We shall examine several in this chapter.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Representing graphs in a computer}

In section~\ref{sec:introduction:matrix_representation}, we discussed
how to use matrices for representing graphs and digraphs. If
$A = [a_{ij}]$ is an $m \times n$ matrix, the adjacency matrix
representation of a graph would require representing all the $mn$
entries of $A$. Alternative graph representations exist that are much
more efficient than representing all entries of a matrix. The graph
represenation used can be influenced by the size of a graph or the
purpose of the represenation.
Section~\ref{sec:graph_algorithms:adjacency_lists} discusses the
adjacency list representation that can result in less storage space
requirement than the adjacency matrix representation. The
\texttt{graph6} format discussed in
section~\ref{sec:graph_algorithms:graph6_format} provides a compact
means of storing graphs for archival purposes.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Adjacency lists}
\label{sec:graph_algorithms:adjacency_lists}

A \emph{list} is a sequence of objects. Unlike sets, a list may
contain multiple copies of the same object. Each object of a list is
referred to as an \emph{element} of the list. A list $L$ of
$n \geq 0$ elements is written as $L = [a_1, a_2, \dots, a_n]$, where
the $i$-th element $a_i$ can be indexed as $L[i]$. In case $n = 0$,
the list $L = [\,]$ is referred to as the \emph{empty list}. Two lists
are equivalent if they both contain the same elements at exactly the
same positions.

Define the adjacency lists of a graph as follows. Let $G$ be a graph
with vertex set $V = \{v_1, v_2, \dots, v_n\}$. Assign to each vertex
$v_i$ a list $L_i$ containing all the vertices that are adjacent to
$v_i$. The list $L_i$ associated with $v_i$ is referred to as the
\emph{adjacency list} of $v_i$. Then $L_i = [\,]$ if and only if $v_i$
is an isolated vertex. We say that $L_i$ is \emph{the} adjacency list
of $v_i$ because any permutation of the elements of $L_i$ results in a
list that contains the same vertices adjacent to $v_i$. If each
adjacency list $L_i$ contains $s_i$ elements where
$0 \leq s_i \leq n$, we say that $L_i$ has \emph{length} $s_i$. The
adjacency list representation of the graph $G$ requires that we
represent $\sum_i s_i = 2 \cdot |E(G)| \leq n^2$ elements in a
computer's memory, since each edge appears twice in the adjacency list
representation. An adjacency list is explicit about which vertices are
adjacent to a vertex, and implicit about which vertices are not
adjacent to that same vertex. Without knowing the graph $G$, given the
adjacency lists $L_1, L_2, \dots, L_n$, we can reconstruct $G$. For
example, Figure~\ref{fig:graph_algorithms:graph_adjacency_lists} shows
a graph and its adjacency list representation.

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  linedecorate/.style={-,thick}]
%% nodes or vertices
\foreach \nodename/\x/\y/\direction/\navigate in {
  1/0/0/left/west, 2/4/0/right/east, 3/4/3.5/right/east,
  4/0/3.5/left/west, 5/2/1/below/south, 6/3/2/above/north,
  7/2/3/below/south, 8/1/2/above/north} {
  \node (\nodename) at (\x,\y) [nodedecorate] {};
  \node [\direction] at (\nodename.\navigate) {$\nodename$};
}
%% adjacency lists
\node (L1) at (5,2.1) [] {};
\node [right] at (L1.east) {$L_1 = [2,8]$};
\node (L2) at (5,1.4) [] {};
\node [right] at (L2.east) {$L_2 = [1,6]$};
\node (L3) at (5,0.7) [] {};
\node [right] at (L3.east) {$L_3 = [4]$};
\node (L4) at (5,0) [] {};
\node [right] at (L4.east) {$L_4 = [3]$};
\node (L5) at (8,2.1) [] {};
\node [right] at (L5.east) {$L_5 = [6,8]$};
\node (L6) at (8,1.4) [] {};
\node [right] at (L6.east) {$L_6 = [2,5,8]$};
\node (L7) at (8,0.7) [] {};
\node [right] at (L7.east) {$L_7 = [\,]$};
\node (L8) at (8,0) [] {};
\node [right] at (L8.east) {$L_8 = [1,5,6]$};
%% edges or lines
\path
\foreach \startnode/\endnode in {1/2, 1/8, 2/6, 3/4, 5/6, 5/8, 6/8} {
  (\startnode) edge[linedecorate] node {} (\endnode)
};
\end{tikzpicture}
\caption{A graph and its adjacency lists.}
\label{fig:graph_algorithms:graph_adjacency_lists}
\end{figure}

We can categorize a graph $G = (V, E)$ as \emph{dense} or
\emph{sparse} based upon its size. A dense graph\index{dense graph}
has size $|E|$ that is close to $|V|^2$, i.e.
$|E| = \Omega\big(|V|^2\big)$, in which case it is feasible to
represent $G$ as an adjacency matrix. The size of a sparse
graph\index{sparse graph} is much less than $|V|^2$, i.e.
$|E| = \Omega\big(|V|\big)$, which renders the adjacency matrix
representation as unsuitable. For a sparse graph, an adjacency list
representation can require less storage space than an adjacency matrix
representation of the same graph.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The {\tt graph6} format}
\label{sec:graph_algorithms:graph6_format}

The graph formats {\tt graph6} and {\tt sparse6} were developed by Brendan
McKay~\cite{McKay2010} at The Australian National University as a
compact way to represent graphs. These two formats use bit vectors and
printable characters of the American Standard Code for Information
Interchange~(ASCII) encoding scheme. The 64 printable ASCII characters
used in {\tt graph6} and {\tt sparse6} are those ASCII characters with decimal
codes from 63 to 126, inclusive, as shown in
Table~\ref{tab:graph_algorithms:graph6_sparse6_ASCII_printable_characters}.
This section shall only cover the {\tt graph6} format. For full
specification on both of the {\tt graph6} and {\tt sparse6} formats, see
McKay~\cite{McKay2010}.

\begin{table}[!htbp]
\centering
\begin{tabular}{ccc|ccc} \hline
binary         & decimal   & glyph    & binary & decimal & glyph \\\hline
\verb!0111111! & \verb!63! & \verb!?! & \verb!1011111! & \verb!95!  & \verb!_! \\
\verb!1000000! & \verb!64! & \verb!@! & \verb!1100000! & \verb!96!  & \verb!`! \\
\verb!1000001! & \verb!65! & \verb!A! & \verb!1100001! & \verb!97!  & \verb!a! \\
\verb!1000010! & \verb!66! & \verb!B! & \verb!1100010! & \verb!98!  & \verb!b! \\
\verb!1000011! & \verb!67! & \verb!C! & \verb!1100011! & \verb!99!  & \verb!c! \\
\verb!1000100! & \verb!68! & \verb!D! & \verb!1100100! & \verb!100! & \verb!d! \\
\verb!1000101! & \verb!69! & \verb!E! & \verb!1100101! & \verb!101! & \verb!e! \\
\verb!1000110! & \verb!70! & \verb!F! & \verb!1100110! & \verb!102! & \verb!f! \\
\verb!1000111! & \verb!71! & \verb!G! & \verb!1100111! & \verb!103! & \verb!g! \\
\verb!1001000! & \verb!72! & \verb!H! & \verb!1101000! & \verb!104! & \verb!h! \\
\verb!1001001! & \verb!73! & \verb!I! & \verb!1101001! & \verb!105! & \verb!i! \\
\verb!1001010! & \verb!74! & \verb!J! & \verb!1101010! & \verb!106! & \verb!j! \\
\verb!1001011! & \verb!75! & \verb!K! & \verb!1101011! & \verb!107! & \verb!k! \\
\verb!1001100! & \verb!76! & \verb!L! & \verb!1101100! & \verb!108! & \verb!l! \\
\verb!1001101! & \verb!77! & \verb!M! & \verb!1101101! & \verb!109! & \verb!m! \\
\verb!1001110! & \verb!78! & \verb!N! & \verb!1101110! & \verb!110! & \verb!n! \\
\verb!1001111! & \verb!79! & \verb!O! & \verb!1101111! & \verb!111! & \verb!o! \\
\verb!1010000! & \verb!80! & \verb!P! & \verb!1110000! & \verb!112! & \verb!p! \\
\verb!1010001! & \verb!81! & \verb!Q! & \verb!1110001! & \verb!113! & \verb!q! \\
\verb!1010010! & \verb!82! & \verb!R! & \verb!1110010! & \verb!114! & \verb!r! \\
\verb!1010011! & \verb!83! & \verb!S! & \verb!1110011! & \verb!115! & \verb!s! \\
\verb!1010100! & \verb!84! & \verb!T! & \verb!1110100! & \verb!116! & \verb!t! \\
\verb!1010101! & \verb!85! & \verb!U! & \verb!1110101! & \verb!117! & \verb!u! \\
\verb!1010110! & \verb!86! & \verb!V! & \verb!1110110! & \verb!118! & \verb!v! \\
\verb!1010111! & \verb!87! & \verb!W! & \verb!1110111! & \verb!119! & \verb!w! \\
\verb!1011000! & \verb!88! & \verb!X! & \verb!1111000! & \verb!120! & \verb!x! \\
\verb!1011001! & \verb!89! & \verb!Y! & \verb!1111001! & \verb!121! & \verb!y! \\
\verb!1011010! & \verb!90! & \verb!Z! & \verb!1111010! & \verb!122! & \verb!z! \\
\verb!1011011! & \verb!91! & \verb![! & \verb!1111011! & \verb!123! & \verb!{! \\
\verb!1011100! & \verb!92! & \verb!\! & \verb!1111100! & \verb!124! & \verb!|! \\
\verb!1011101! & \verb!93! & \verb!]! & \verb!1111101! & \verb!125! & \verb!}! \\
\verb!1011110! & \verb!94! & \verb!^! & \verb!1111110! & \verb!126! & \verb!~! \\\hline
\end{tabular}
\caption{ASCII printable characters used by graph6 and sparse6.}
\label{tab:graph_algorithms:graph6_sparse6_ASCII_printable_characters}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Bit vectors}

Before discussing how {\tt graph6} and {\tt sparse6} represent graphs using
printable ASCII characters, we first present encoding schemes used by
these two formats. A \emph{bit vector} is, as its name suggests, a
vector whose elements are 1's and 0's. It can be represented as a list
of bits, e.g. \verb!E! can be represented as the ASCII bit vector
$[\texttt{1}, \texttt{0}, \texttt{0}, \texttt{0}, \texttt{1},
  \texttt{0}, \texttt{1}]$. For brevity, we write a bit vector in a
compact form such as \texttt{1000101}. The \emph{length} of a bit
vector is its number of bits. The \emph{most significant bit}
of a bit vector $v$ is the bit position with the largest value among
all the bit positions in $v$. Similarly, the
\emph{least significant bit} is the bit position in $v$ having the
least value among all the bit positions in $v$. The least significant
bit of $v$ is usually called the parity bit because when $v$
interpreted as an integer the parity bit determines whether the
integer is even or odd. Reading \texttt{1000101} from left to right,
the first bit \texttt{1} is the most significant bit, followed by the
second bit \texttt{0} which is the second most significant bit, and so
on all the way down to the seventh bit \texttt{1} which is the least
significant bit.

The order in which we process the bits of a bit vector is referred to
as \emph{endianness}. Processing $v$ in \emph{big-endian} order means
\index{big-endian order}
\index{endianness}
that we first process the most significant bit of $v$, followed by the
second most significant bit, and so on all the way down to the least
significant bit of $v$. \emph{Little-endian} order means that we first
process the least significant bit, followed by the second least
significant bit, and so on all the way up to the most significant
bit. In big-endian order, the ASCII binary representation of
\texttt{E} is written \texttt{1000101}~(see
Table~\ref{tab:graph_algorithms:big_endian_ASCII_binary_E}), while
the little-ending order is written \texttt{1010001}~(see
Table~\ref{tab:graph_algorithms:little_endian_ASCII_binary_E}). To
determine the integer representation of a bit vector, multiply each
bit value by its corresponding position value, then add up all the
results. In general, if the bit vector $v = b_1 b_2 \cdots b_k$ is the
big-endian binary representation of a positive integer, then the
integer representation of $v$ is
%
\begin{equation}
\label{eq:graph_algorithms:big_endian_binary_to_integer}
\sum_{i=1}^k 2^{k-i} b_i
=
2^{k-1} b_1 + 2^{k-2} b_2 + 2^{k-3} b_3 + \cdots + 2^0 b_k.
\end{equation}

\begin{table}[!htbp]
\centering
\begin{tabular}{l|ccccccc} \hline
position       & 1          & 2          & 3          & 4          & 5          & 6          & 7 \\
bit value      & \texttt{1} & \texttt{0} & \texttt{0} & \texttt{0} & \texttt{1} & \texttt{0} & \texttt{1} \\
position value & $2^6$      & $2^5$      & $2^4$      & $2^3$      & $2^2$      & $2^1$      & $2^0$ \\\hline
\end{tabular}
\caption{Big-endian order of the ASCII binary code of \texttt{E}.}
\label{tab:graph_algorithms:big_endian_ASCII_binary_E}
\end{table}

\begin{table}[!htbp]
\centering
\begin{tabular}{l|ccccccc} \hline
position       & 1          & 2          & 3          & 4          & 5          & 6          & 7 \\
bit value      & \texttt{1} & \texttt{0} & \texttt{1} & \texttt{0} & \texttt{0} & \texttt{0} & \texttt{1} \\
position value & $2^0$      & $2^1$      & $2^2$      & $2^3$      & $2^4$      & $2^5$      & $2^6$ \\\hline
\end{tabular}
\caption{Little-endian order of the ASCII binary code of \texttt{E}.}
\label{tab:graph_algorithms:little_endian_ASCII_binary_E}
\end{table}

In {\tt graph6} and {\tt sparse6} formats, the length of a bit vector must be a
multiple of 6. Suppose $v$ is a bit vector of length $k$ such that
$6 \nmid k$. To transform $v$ into a bit vector having length a
multiple of 6, let $r = k \mod 6$ be the remainder upon dividing $k$
by 6, and pad $6 - r$ zeros to the right of $v$.

Suppose $v = b_1 b_2 \cdots b_k$ is a bit vector of length $k$, where
$6 \;|\; k$. We split $v$ into $k/6$ bit vectors $v_i$, each of length
6. For $0 \leq i \leq k/6$, the $i$-th bit vector is given by
\[
v_i
=
b_{6i-5} b_{6i-4} b_{6i-3} b_{6i-2} b_{6i-1} b_{6i}.
\]
Consider each $v_i$ as the big-endian binary representation of a
positive
integer. Use~(\ref{eq:graph_algorithms:big_endian_binary_to_integer})
to obtain the integer representation $N_i$ of each $v_i$. Then add 63
to each $N_i$ to obtain $N_i'$ and store $N_i'$ in one byte of
memory. That is, each $N_i'$ can be represented as a bit vector of
length $8$. Thus the required number of bytes to store $v$ is
$\lceil k/6 \rceil$. Let $B_i$ be the byte representation of $N_i'$ so
that
%
\begin{equation}
\label{eq:graph_algorithms:byte_representation_bit_vector}
R(v)
=
B_1 B_2 \cdots B_{\lceil k/6 \rceil}
\end{equation}
%
denotes the representation of $v$ as a sequence of $\lceil k/6 \rceil$
bytes.

We now discuss how to encode an integer $n$ in the range
$0 \leq n \leq 2^{36} - 1$
using~(\ref{eq:graph_algorithms:byte_representation_bit_vector}) and
denote such an encoding of $n$ as $N(n)$. Let $v$ be the big-endian
binary representation of $n$. Then $N(n)$ is given by
%
\begin{equation}
\label{eq:graph_algorithms:graph6_sparse6_graph_orders}
N(n)
=
\begin{cases}
n + 63, & \text{if $0 \leq n \leq 62$}, \\
126 \, R(v), & \text{if $63 \leq n \leq 258047$}, \\
126 \, 126 \, R(v), & \text{if $258048 \leq n \leq 2^{36}-1$}.
\end{cases}
\end{equation}
%% If $0 \leq n \leq 62$, then
%% we write $N(n) = n + 63$. If $63 \leq n \leq 258047$, let $v$ be the
%% big-endian binary representation of $n$ and write
%% $N(n) = 126 \, R(v)$. Finally, if $258048 \leq n \leq 2^{36} - 1$,
%% again we let $v$ be the big-endian binary representation of $n$ and
%% write $N(n) = 126 \, 126 \, R(v)$.
Note that $n + 63$ requires one byte of storage memory, while
$126 \, R(v)$ and $126 \, 126 \, R(v)$ require 4 and 8 bytes,
respectively.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{The {\tt graph6} format}
\index{graph6}

The {\tt graph6} format is used to represent simple, undirected graphs of
order from $0$ to $2^{36} - 1$, inclusive. Let $G$ be a simple,
undirected graph of order $0 \leq n \leq 2^{36} - 1$. If $n = 0$, then
$G$ is represented in {\tt graph6} format as ``\verb!?!''. Suppose $n >
0$. Let $M = [a_{ij}]$ be the adjacency matrix of $G$. Consider the
upper triangle of $M$, excluding the main diagonal, and write that
upper triangle as the bit vector
\[
v
=
\underbrace{a_{0,1}}_{c_1}
\underbrace{a_{0,2} a_{1,2}}_{c_2}
\underbrace{a_{0,3} a_{1,3} a_{2,3}}_{c_3} \cdots
\underbrace{a_{0,i} a_{1,i} \cdots a_{i-1,i}}_{c_i} \cdots
\underbrace{a_{0,n} a_{1,n} \cdots a_{n-1,n}}_{c_n}
\]
where $c_i$ denotes the entries $a_{0,i} a_{1,i} \cdots a_{i-1,i}$ in
column $i$ of $M$. Then the {\tt graph6} representation of $G$ is
$N(n) R(v)$, where $R(v)$ and $N(n)$ are as
in~(\ref{eq:graph_algorithms:byte_representation_bit_vector})
and~(\ref{eq:graph_algorithms:graph6_sparse6_graph_orders}),
respectively. That is, $N(n)$ encodes the order of $G$ and $R(v)$
encodes the edges of $G$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Graph searching}
\label{sec:graph_algorithms:graph_searching}

This section discusses two fundamental algorithms for graph traversal:
breadth-first search and depth-first search. The word ``search'' used
in describing these two algorithms is rather misleading. It would be
more accurate to describe them as algorithms for constructing trees
using the adjacency information of a given graph. However, the names
``breadth-first search'' and ``depth-first search'' are entrenched in
literature on graph theory and computer science. From hereon, we use
these two names as given above, bearing in mind their intended
purposes.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Breadth-first search}

Breadth-first search (BFS) is a strategy for running through the
vertices of a graph. It was presented by Moore~\cite{Moore1959} in
1959 within the context of traversing mazes. Lee~\cite{Lee1961}
independently discovered the same algorithm in 1961 in his work on
routing wires on circuit boards.

The basic BFS algorithm can be described as follows. Starting from a
given vertex $v$ of a graph $G$, we first explore the neighborhood of
$v$ by visiting all vertices that are adjacent to $v$. We then apply
the same strategy to each of the neighbors of $v$. The strategy of
exploring the neighborhood of a vertex is applied to all vertices of
$G$. The result is a tree rooted at $v$ and this tree is a subgraph of
$G$. Algorithm~\ref{alg:graph_algorithms:breadth_first_search_template}
presents a general template for the BFS strategy. The tree resulting
from the BFS algorithm is called a \emph{breadth-first search tree}.
\index{BFS}
\index{breadth-first search}

\begin{algorithm}[!htpb]
\dontprintsemicolon  % no semicolon at end of pseudocode statements
%% data section
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
%% input/output
\Input{A directed or undirected graph $G = (V, E)$ of order $n > 0$. A
  vertex $s$ from which to start the search. The vertices are numbered
  from $1$ to  $n = |V|$, i.e. $V = \{1, 2, \dots, n\}$.}
\Output{A list $D$ of distances of all vertices from $s$. A tree $T$
  rooted at $s$.}
\BlankLine
$Q \leftarrow [s]$~\nllabel{alg:BFS:initialize_queue_visit_nodes} \tcc*[f]{queue of nodes to visit}\;
$D \leftarrow [\infty, \infty, \dots, \infty]$ \tcc*[f]{$n$ copies of $\infty$}\;
$D[s] \leftarrow 0$\;
$T \leftarrow [\,]$~\nllabel{alg:BFS:initialize_empty_tree}\;
\While{$\length(Q) > 0$~\nllabel{alg:BFS:while_loop:non_empty_queue}}{
  $v \leftarrow \dequeue(Q)$\;
  \For{\emph{each} $w \in \adj(v)$~\nllabel{alg:BFS:explore_neighborhood}}{
    \If{$D[w] = \infty$~\nllabel{alg:BFS:marking_vertex_as_visited}}{
      $D[w] \leftarrow D[v] + 1$\;
      $\enqueue(Q, w)$\;
      $\append(T, vw)$~\nllabel{alg:BFS:while_loop:append_to_tree}\;
    }
  }
}
\Return $(D, T)$\;
\caption{A general breadth-first search template.}
\label{alg:graph_algorithms:breadth_first_search_template}
\end{algorithm}

The breadth-first search algorithm makes use of a special type of list
called a \emph{queue}. This is analogous to a queue of people waiting
in line to be served. A person may enter the queue by joining the rear
of the queue. The person who is in the queue the longest amount of
time is served first, followed by the person who has waited the second
longest time, and so on. Formally, a queue $Q$ is a list of
elements. At any time, we only have access to the first element of
$Q$, known as the \emph{front} or \emph{start} of the queue. We insert
a new element into $Q$ by appending the new element to the \emph{rear}
or \emph{end} of the queue. The operation of removing the front of $Q$
is referred to as \emph{dequeue}, while the operation of appending to
the rear of $Q$ is called \emph{enqueue}. That is, a queue implements
a first-in first-out~(FIFO) protocol for adding and moving
elements. As with lists, the \emph{length} of a queue is its total
number of elements.

\begin{figure}[!htbp]
\centering
\subfigure[]{
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  linedecorate/.style={-,thick}]
%% nodes or vertices
\foreach \nodename/\x/\y/\direction/\navigate in {2/4/4/above/north,
  1/0/4/above/north, 3/6/3/above/north, 4/6/1/below/south,
  5/4/0/below/south, 7/-2/2/above/north, 6/0/0/below/south} {
  \node (\nodename) at (\x,\y) [nodedecorate] {};
  \node [\direction] at (\nodename.\navigate) {$\nodename$};
}
%% edges or lines
\path
\foreach \startnode/\endnode in {1/2, 1/5, 2/3, 2/5, 2/7, 3/4, 3/5,
  4/6, 5/6, 6/7} {
  (\startnode) edge[linedecorate] node {} (\endnode)
};
\end{tikzpicture}
}
%%
\qquad
\subfigure[]{
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  linedecorate/.style={-,thick},%
  scale=1.6]
%% nodes or vertices
\foreach \nodename/\x/\y/\direction/\navigate in {4/0/0/below/south,
  3/0/1/left/west, 7/1/1/below/south, 6/2/1/below/south,
  2/0.5/2/left/west, 5/2/2/left/west, 1/1.25/3/above/north} {
  \node (\nodename) at (\x,\y) [nodedecorate] {};
  \node [\direction] at (\nodename.\navigate) {$\nodename$};
}
%% edges or lines
\path
\foreach \startnode/\endnode in {1/2, 1/5, 2/3, 2/7, 3/4, 5/6} {
  (\startnode) edge[linedecorate] node {} (\endnode)
};
\end{tikzpicture}
}
%%
%%
\subfigure[]{
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  arrowdecorate/.style={->,>=stealth,thick}]
%% nodes or vertices
\foreach \nodename/\x/\y/\direction/\navigate in {2/4/4/above/north,
  1/0/4/above/north, 3/6/3/above/north, 4/6/1/below/south,
  5/4/0/below/south, 7/-2/2/above/north, 6/0/0/below/south} {
  \node (\nodename) at (\x,\y) [nodedecorate] {};
  \node [\direction] at (\nodename.\navigate) {$\nodename$};
}
%% edges or lines
\path
\foreach \startnode/\endnode in {2/5, 3/2, 3/5, 4/3, 5/1, 6/7, 7/2} {
  (\startnode) edge[arrowdecorate] node {} (\endnode)
}
\foreach \startnode/\endnode/\benddirection/\angle in {
  1/2/bend left/12, 4/6/bend right/12, 6/5/bend right/12} {
  (\startnode) edge[arrowdecorate,\benddirection=\angle] node {} (\endnode)
};
\end{tikzpicture}
}
%%
\qquad
\subfigure[]{
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  arrowdecorate/.style={->,>=stealth,thick},
  scale=1.6]
%% nodes or vertices
\foreach \nodename/\x/\y/\direction/\navigate in {2/0/1/below/south,
  1/1/0/below/south, 5/1/1/left/west, 7/2/1/below/south,
  6/2/2/left/west, 3/0.5/2/left/west, 4/1.25/3/above/north} {
  \node (\nodename) at (\x,\y) [nodedecorate] {};
  \node [\direction] at (\nodename.\navigate) {$\nodename$};
}
%% edges or lines
\path
\foreach \startnode/\endnode in {4/3, 4/6, 3/2, 3/5, 6/7, 5/1} {
  (\startnode) edge[arrowdecorate] node {} (\endnode)
};
\end{tikzpicture}
}
\caption{Breadth-first search trees for undirected and directed graphs.}
\label{fig:graph_algorithms:breadth_first_search_undirected}
\end{figure}

Note that the BFS
Algorithm~\ref{alg:graph_algorithms:breadth_first_search_template}
works on both undirected and directed graphs. For an undirected graph,
line~\ref{alg:BFS:explore_neighborhood} means that we explore all
the neighbors of vertex $v$, i.e. the set $\adj(v)$ of vertices
adjacent to $v$. In the case of a digraph, we replace
``$w \in \adj(v)$'' on line~\ref{alg:BFS:explore_neighborhood} with
``$w \in \oadj(v)$'' because we only want to explore all vertices that
are out-neighbors of $v$. The algorithm returns two lists $D$ and
$T$. The list $T$ contains a subset of edges in $E(G)$ that make up a
tree rooted at the given start vertex $s$. As trees are connected
graphs without cycles, we may take the vertices comprising the edges
of $T$ to be the vertex set of the tree. It is clear that $T$
represents a tree by means of a list of edges, which allows us to
identify the tree under consideration as the edge list $T$. The list
$D$ has the same number of elements as the order of $G = (V, E)$,
i.e. $\length(D) = |V|$. The $i$-th element $D[i]$ counts the number
of edges in $T$ between the vertices $s$ and $v_i$. In other words,
$D[i]$ is the length of the $s$-$v_i$ path in $T$. It can be shown
that $D[i] = \infty$ if and only if $G$ is disconnected. After one
application of
Algorithm~\ref{alg:graph_algorithms:breadth_first_search_template}, it
may happen that $D[i] = \infty$ for at least one vertex
$v_i \in V$. To traverse those vertices that are unreachable from $s$,
again we apply
Algorithm~\ref{alg:graph_algorithms:breadth_first_search_template} on
$G$ with starting vertex $v_i$. Repeat this algorithm as often as
necessary until all vertices of $G$ are visited. The result may be a
tree that contains all the vertices of $G$ or a collection of trees,
each of which contains a subset of $V(G)$.
Figure~\ref{fig:graph_algorithms:breadth_first_search_undirected}
presents BFS trees resulting from applying
Algorithm~\ref{alg:graph_algorithms:breadth_first_search_template} on
an undirected graph and a digraph.

\begin{theorem}
\label{thm:graph_algorithms:BFS:worst_case_time_complexity}
The worst-case time complexity of
Algorithm~\ref{alg:graph_algorithms:breadth_first_search_template} is
$O(|V| + |E|)$.
\end{theorem}

\begin{proof}
Without loss of generality, we can assume that $G = (V, E)$ is
connected. The initialization steps in
lines~\ref{alg:BFS:initialize_queue_visit_nodes}
to~\ref{alg:BFS:initialize_empty_tree} take $O(|V|)$ time. After
initialization, all but one vertex are labelled
$\infty$. Line~\ref{alg:BFS:marking_vertex_as_visited} ensures that
each vertex is enqueued at most once and hence dequeued at most
once. Each of enqueuing and dequeuing takes constant time. The total
time devoted to queue operations is $O(|V|)$. The adjacency list of a
vertex is scanned after dequeuing that vertex, so each adjacency list
is scanned at most once. Summing the lengths of the adjacency lists,
we have $\Theta(|E|)$ and therefore we require $O(|E|)$ time to scan
the adjacency lists. After the adjacency list of a vertex is scanned,
at most $k$ edges are added to the list $T$, where $k$ is the length
of the adjacency list under consideration. Like queue operations,
appending to a list takes constant time, hence we require $O(|E|)$
time to build the list $T$. Therefore, BFS runs in $O(|V| + |E|)$
time.
\end{proof}

\begin{theorem}
\label{thm:graph_algorithms:BFS:list_D_length_shortest_paths}
For the list $D$ resulting from
Algorithm~\ref{alg:graph_algorithms:breadth_first_search_template},
let $s$ be a starting vertex and let $v$ be a vertex such that
$D[v] \neq \infty$. Then $D[v]$ is the length of any shortest path
from $s$ to $v$.
\end{theorem}

\begin{proof}
It is clear that $D[v] = \infty$ if and only if there are no paths
from $s$ to $v$. Let $v$ be a vertex such that $D[v] \neq \infty$. As
$v$ can be reached from $s$ by a path of length $D[v]$, the length
$d(s,v)$ of any shortest $s$-$v$ path satisfies $d(s,v) \leq
D[v]$. Use induction on $d(s,v)$ to show that equality holds. For the
base case $s = v$, we have $d(s,v) = D[v] = 0$ since the trivial path
has length zero. Assume for induction that if $d(s,v) = k$, then
$d(s,v) = D[v]$.
%% We need to show that if $d(s,u)$ is the length of any
%% shortest $s$-$u$ path, then $d(s,u) = D[u]$.
Let $d(s,u) = k + 1$ with the corresponding shortest $s$-$u$ path
being $(s, v_1, v_2, \dots, v_k, u)$. Then by our induction
hypothesis, $(s, v_1, v_2, \dots, v_k)$ is a shortest path from $s$ to
$v_k$ of length $d(s, v_k) = D[v_k] = k$. In other words, $D[v_k] <
D[u]$ and the while loop spanning
lines~\ref{alg:BFS:while_loop:non_empty_queue}
to~\ref{alg:BFS:while_loop:append_to_tree} processes $v_k$ before
processing $u$. The graph under consideration has the edge $v_k
u$. When examining the adjacency list of $v_k$, BFS reaches $u$~(if
$u$ is not reached earlier) and so $D[u] \leq k + 1$. Hence,
$D[u] = k + 1$ and therefore $d(s,u) = D[u] = k + 1$.
\end{proof}

%% Another version of
%% Algorithm~\ref{alg:graph_algorithms:breadth_first_search} is where you
%% are searching the graph for a vertex (or edge) satisfying a certain
%% property $P$. In that situation, you simply quit at the step where you
%% increment the counter, i.e. line~7 in
%% Algorithm~\ref{alg:graph_algorithms:breadth_first_search}. Other
%% variations are also possible as well.

%% For the example of the graph in
%% Figure~\ref{fig:introduction:types_of_walks}, the list of distances
%% from vertex \verb!a! to any other vertex is
%% %
%% \begin{center}
%% \fontsize{9pt}{9pt}
%% \selectfont
%% \tt
%% \begin{lstlisting}
%% [['a', 0], ['b', 1], ['c', 2], ['d', 3], ['e', 1], ['f', 2], ['g', 2]]
%% \end{lstlisting}
%% \end{center}
%% %
%% To create this list,
%% %
%% \begin{itemize}
%% \item
%% Start at \verb!a! and compute the distance from \verb!a! to itself.

%% \item
%% Move to each neighbor of \verb!a!, namely \verb!b! and \verb!e!, and
%% compute the distance from \verb!a! to each of them.

%% \item
%% Move to each ``unseen'' neighbor of \verb!b!, namely just \verb!c!,
%% and compute the distance from \verb!a! to it.

%% \item
%% Move to each ``unseen'' neighbor of \verb!e!, namely just \verb!f!,
%% and compute the distance from \verb!a! to it.

%% \item
%% Move to each ``unseen'' neighbor of \verb!c!, namely just \verb!d!,
%% and compute the distance from \verb!a! to it.

%% \item
%% Move to each ``unseen'' neighbor of \verb!f!, namely just \verb!g!,
%% and compute the distance from \verb!a! to it.
%% \end{itemize}

%% As an example, here is some Sage code which implements BFS to compute
%% the list distances from a given vertex.
%% %
%% \begin{center}
%% \fontsize{9pt}{9pt}
%% \selectfont
%% \tt
%% \begin{lstlisting}
%% def graph_distance(G, v0):
%%     """
%%     Breadth first search algorithm to find the
%%     distance from a fixed vertex $v_0$ to any
%%     other vertex.

%%     INPUT:
%%         G - a connected graph
%%         v0 - a vertex

%%     OUTPUT:
%%         D - a list of distances to
%%             every other vertex

%%     EXAMPLES:
%%         sage: G = Graph({1: [2, 4], 2: [1, 4], 3: [2, 6],
%%                          4: [1, 3], 5: [4, 2], 6: [3, 1]})
%%         sage: v0 = 1
%%         sage: graph_distance(G,v0)
%%         [[1, 0], [2, 1], [3, 2], [4, 1], [5, 2], [6, 1]]
%%         sage: G = Graph({"a": ["b", "e"], "b": ["c", "e"], \
%%          "c": ["d", "e"], "d": ["f"], "e": ["f"], "f": ["g"], "g":["b"]})
%%         sage: v0 = "a"
%%         sage: graph_distance(G, v0)
%%         [['a', 0], ['b', 1], ['c', 2], ['d', 3], ['e', 1],
%%          ['f', 2], ['g', 2]]
%%         sage: G = Graph({1: [2,3], 2: [1, 3], 3: [2], 4: [5], 5: [6], 6: [5]})
%%         sage: v0 = 1
%%         sage: graph_distance(G, v0) # note G is disconnected
%%         [[1, 0], [2, 1], [3, 1]]
%%     """
%%     V = G.vertices()
%%     Q = [v0]
%%     T = []
%%     D = []
%%     while Q<>[] and T<>V:
%%         for v in Q:
%%             if not(v in T):
%%                 D.append([v,G.distance(v0,v)])
%%             if v in Q:
%%                 Q.remove(v)
%%             T.append(v)
%%             T = list(Set(T))
%%             Q = Q+[x for x in G.neighbors(v) if not(x in T+Q)]
%%             if T == V:
%%                 break
%%     D.sort()
%%     print Q, T
%%     return D
%% \end{lstlisting}
%% \end{center}
%% %
%% \begin{exercise}
%% Using Sage's \verb!shortest_path! method, can you modify the above
%% function to return a list of shortest paths from $v_0$ to any other
%% vertex?
%% \end{exercise}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Depth-first search}

\begin{quote}
\includegraphics[scale=0.5]{image/depth-first-search} \\
\noindent
--- Randall Munroe\index{Munroe, Randall}, xkcd,
\url{http://xkcd.com/761/}
\end{quote}

\noindent
A depth-first search~(DFS) is a graph traversal strategy similar to
breadth-first search. Both BFS and DFS differ in how they explore each
vertex. Whereas BFS explores the neighborhood of a vertex $v$ before
moving on to explore the neighborhoods of the neighbors, DFS explores
as deep as possible a path starting at $v$. One can think of BFS as
exploring the immediate surrounding, while DFS prefers to see what is
on the other side of the hill. In the 19th century,
Lucas~\cite{Lucas1882.1894} and Tarry~\cite{Tarry1895} investigated
DFS as a strategy for traversing mazes. Fundamental properties of DFS
were discovered in the early 1970s by Hopcroft and
Tarjan~\cite{HopcroftTarjan1973,Tarjan1972}.

To get an intuitive appreciation for DFS, suppose we have an
$8 \times 8$ chessboard in front of us. We place a single knight
piece on a fixed square of the board. Our objective is to find a
sequence of knight moves that visits each and every square exactly
once, while obeying the rules of chess that govern the movement of the
knight piece. Such a sequence of moves, if one exists, is called a
\emph{knight's tour}. How do we find such a tour? We could make one
knight move after another, recording each move to ensure that we do
not step on a square that is already visited, until we could not make
any more moves. Acknowledging defeat when encountering a dead end, it
might make sense to \emph{backtrack} a few moves and try again, hoping
we would not get stuck. If we fail again, we try backtracking a few
more moves and traverse yet another path, hoping to make further
progress. Repeat this strategy until a tour is found or until we have
exhausted all possible moves. The above strategy for finding a
knight's tour is an example of depth-first search, sometimes called
\emph{backtracking}.
\index{backtracking}
\index{knight's tour}

\begin{algorithm}[!htpb]
\dontprintsemicolon  % no semicolon at end of pseudocode statements
%% data section
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
%% input/output
\Input{A directed or undirected graph $G = (V, E)$ of order $n > 0$. A
  vertex $s$ from which to start the search. The vertices are numbered
  from $1$ to  $n = |V|$, i.e. $V = \{1, 2, \dots, n\}$.}
\Output{A list $D$ of distances of all vertices from $s$. A tree $T$
  rooted at $s$.}
\BlankLine
$S \leftarrow [s]$ \tcc*[f]{stack of nodes to visit}\;
$D \leftarrow [\infty, \infty, \dots, \infty]$ \tcc*[f]{$n$ copies of $\infty$}\;
$D[s] \leftarrow 0$\;
$T \leftarrow [\,]$\;
\While{$\length(S) > 0$~\nllabel{alg:DFS:while_loop_tests_non_empty_stack}}{
  $v \leftarrow \pop(S)$\;
  \For{\emph{each} $w \in \adj(v)$~\nllabel{alg:DFS:for_loop_visit_neighbors}}{
    \If{$D[w] = \infty$~\nllabel{alg:DFS:if_test_unvisited_neighbors}}{
      $D[w] \leftarrow D[v] + 1$\;
      $\push(S, w)$\;
      $\append(T, vw)$\;
    }
  }
}
\Return $(D, T)$\;
\caption{A general depth-first search template.}
\label{alg:graph_algorithms:depth_first_search_template}
\end{algorithm}

Algorithm~\ref{alg:graph_algorithms:depth_first_search_template}
formalizes the above description of depth-first search. The tree
resulting from applying DFS on a graph is called a
\emph{depth-first search tree}. The general structure of this
algorithm bears close resemblance to
Algorithm~\ref{alg:graph_algorithms:breadth_first_search_template}. A
significant difference is that instead of using a queue to structure
and organize vertices to be visited, DFS uses another special type of
list called a \emph{stack}. To understand how elements of a stack are
organized, we use the analogy of a stack of cards. A new card is added
to the stack by placing it on top of the stack. Any time we want to
remove a card, we are only allowed to remove the top-most card that is
on the top of the stack. A list $L = [a_1, a_2, \dots, a_k]$ of $k$
elements is a stack when we impose the same rules for element
insertion and removal. The top and bottom of the stack are $L[k]$ and
$L[1]$, respectively. The operation of removing the top element of the
stack is referred to as \emph{popping} the element off the
stack. Inserting an element into the stack is called \emph{pushing}
the element onto the stack. In other words, a stack implements a
last-in first-out~(LIFO) protocol for element insertion and removal,
in contrast to the FIFO policy of a queue. We also use the term
\emph{length} to refer to the number of elements in the stack.

The depth-first search
Algorithm~\ref{alg:graph_algorithms:depth_first_search_template} can
be analyzed similar to how we analyzed
Algorithm~\ref{fig:graph_algorithms:breadth_first_search_undirected}. Just
as BFS is applicable to both directed and undirected graphs, we can
also have undirected graphs and digraphs as input to DFS. For the case
of an undirected graph, line~\ref{alg:DFS:for_loop_visit_neighbors} of
Algorithm~\ref{alg:graph_algorithms:depth_first_search_template}
considers all vertices adjacent to the current vertex $v$. In case the
input graph is directed, we replace ``$w \in \adj(v)$'' on
line~\ref{alg:DFS:for_loop_visit_neighbors} with ``$w \in \oadj(v)$''
to signify that we only want to consider the out-neighbors of $v$. If
any neighbors (respectively, out-neighbors) of $v$ are labelled as
$\infty$, we know that we have not explored any paths starting from
any of those vertices. So we label each of those unexplored vertices
with a positive integer and push them onto the stack $S$, where they
will wait for later processing. We also record the paths leading from
$v$ to each of those unvisited neighbors, i.e. the edges $vw$ for each
vertex $w \in \adj(v)$ (respectively, $w \in \oadj(v)$) are appended
to the list $T$. The test on
line~\ref{alg:DFS:if_test_unvisited_neighbors} ensures that we do not
push onto $S$ any vertices on the path that lead to $v$. When we
resume another round of the while loop that starts on
line~\ref{alg:DFS:while_loop_tests_non_empty_stack}, the previous
vertex $v$ have been popped off $S$ and the neighbors (respectively,
out-neighbors) of $v$ have been pushed onto $S$. To explore a path
starting at $v$, we choose any unexplored neighbors of $v$ by popping
an element off $S$ and repeat the for loop starting on
line~\ref{alg:DFS:for_loop_visit_neighbors}. Repeat the DFS algorithm
as often as required in order to traverse all vertices of the input
graph. The output of DFS consists of two lists $D$ and $T$: $T$ is a
tree rooted at the starting vertex $s$; and each $D[i]$ counts the
length of the $s$-$v_i$ path in $T$.
Figure~\ref{fig:graph_algorithms:depth_first_search_undirected}
shows the DFS trees resulting from running
Algorithm~\ref{alg:graph_algorithms:depth_first_search_template} on an
undirected graph and a digraph. The worst-case time complexity of DFS
can be analyzed using an argument similar to that in
Theorem~\ref{thm:graph_algorithms:BFS:worst_case_time_complexity}. Arguing
along the same lines as in the proof of
Theorem~\ref{thm:graph_algorithms:BFS:list_D_length_shortest_paths},
we can also show that the list $D$ returned by DFS contains lengths of
any shortest paths from the starting vertex $s$ to any other vertex in
the tree $T$.

\begin{figure}[!htbp]
\centering
\subfigure[]{
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  linedecorate/.style={-,thick}]
%% nodes or vertices
\foreach \nodename/\x/\y/\direction/\navigate in {2/4/4/above/north,
  1/0/4/above/north, 3/6/3/above/north, 4/6/1/below/south,
  5/4/0/below/south, 7/-2/2/above/north, 6/0/0/below/south} {
  \node (\nodename) at (\x,\y) [nodedecorate] {};
  \node [\direction] at (\nodename.\navigate) {$\nodename$};
}
%% edges or lines
\path
\foreach \startnode/\endnode in {1/2, 1/5, 2/3, 2/5, 2/7, 3/4, 3/5,
  4/6, 5/6, 6/7} {
  (\startnode) edge[linedecorate] node {} (\endnode)
};
\end{tikzpicture}
}
%%
\qquad
\subfigure[]{
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  linedecorate/.style={-,thick},%
  scale=1.6]
%% nodes or vertices
\foreach \nodename/\x/\y/\direction/\navigate in {
  1/1.25/3/above/north, 2/0.5/2/below/south, 5/2/2/right/east,
  3/1.25/1/below/south, 6/2.75/1/right/east, 7/2/0/below/south,
  4/3.5/0/below/south} {
  \node (\nodename) at (\x,\y) [nodedecorate] {};
  \node [\direction] at (\nodename.\navigate) {$\nodename$};
}
%% edges or lines
\path
\foreach \startnode/\endnode in {1/2, 1/5, 3/5, 5/6, 6/7, 6/4} {
  (\startnode) edge[linedecorate] node {} (\endnode)
};
\end{tikzpicture}
}
%%
%%
\subfigure[]{
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  arrowdecorate/.style={->,>=stealth,thick}]
%% nodes or vertices
\foreach \nodename/\x/\y/\direction/\navigate in {2/4/4/above/north,
  1/0/4/above/north, 3/6/3/above/north, 4/6/1/below/south,
  5/4/0/below/south, 7/-2/2/above/north, 6/0/0/below/south} {
  \node (\nodename) at (\x,\y) [nodedecorate] {};
  \node [\direction] at (\nodename.\navigate) {$\nodename$};
}
%% edges or lines
\path
\foreach \startnode/\endnode in {2/5, 3/2, 3/5, 4/3, 5/1, 6/7, 7/2} {
  (\startnode) edge[arrowdecorate] node {} (\endnode)
}
\foreach \startnode/\endnode/\benddirection/\angle in {
  1/2/bend left/12, 4/6/bend right/12, 6/5/bend right/12} {
  (\startnode) edge[arrowdecorate,\benddirection=\angle] node {} (\endnode)
};
\end{tikzpicture}
}
%%
\qquad
\subfigure[]{
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  arrowdecorate/.style={->,>=stealth,thick},
  scale=1.6]
%% nodes or vertices
\foreach \nodename/\x/\y/\direction/\navigate in {
  4/1.25/3/above/north, 3/0.5/2/below/south, 6/2/2/right/east,
  5/1.25/1/left/west, 7/2.75/1/left/west, 1/1.25/0/below/south,
  2/2.75/0/below/south} {
  \node (\nodename) at (\x,\y) [nodedecorate] {};
  \node [\direction] at (\nodename.\navigate) {$\nodename$};
}
%% edges or lines
\path
\foreach \startnode/\endnode in {4/3, 4/6, 6/5, 6/7, 5/1, 7/2} {
  (\startnode) edge[arrowdecorate] node {} (\endnode)
};
\end{tikzpicture}
}
\caption{Depth-first search trees for undirected and directed graphs.}
\label{fig:graph_algorithms:depth_first_search_undirected}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Connectivity of a graph}

Both BFS and DFS can be used to determine if an undirected graph is
connected. Let $G = (V, E)$ be an undirected graph of order $n > 0$
and let $s$ be an arbitrary vertex of $G$. We initialize a counter
$c \leftarrow 1$ to mean that we are starting our exploration at $s$,
hence we have already visited one vertex, i.e. $s$. We apply either
BFS or DFS, treating $G$ and $s$ as input to any of these
algorithms. Each time we visit a vertex that was previously unvisited,
we increment the counter $c$. At the end of the algorithm, we compare
$c$ with $n$. If $c = n$, we know that we have visited all vertices of
$G$ and conclude that $G$ is connected. Otherwise, we conclude that
$G$ is disconnected. This procedure is summarized in
Algorithm~\ref{alg:graph_algorithms:graph_connectivity}.

\begin{algorithm}[!htpb]
\dontprintsemicolon  % no semicolon at end of pseudocode statements
%% data section
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKwData{False}{False}
\SetKwData{True}{True}
%% input/output
\Input{An undirected graph $G = (V, E)$ of order $n > 0$. A vertex $s$
  from which to start the search. The vertices are numbered from $1$
  to  $n = |V|$, i.e. $V = \{1, 2, \dots, n\}$.}
\Output{$\True$ if $G$ is connected; $\False$ otherwise.}
\BlankLine
$Q \leftarrow [s]$~\tcc*[f]{queue of nodes to visit}\;
$D \leftarrow [0, 0, \dots, 0]$~\tcc*[f]{$n$ copies of $0$}\;
$D[s] \leftarrow 1$\;
$c \leftarrow 1$\;
\While{$\length(Q) > 0$}{
  $v \leftarrow \dequeue(Q)$\;
  \For{\emph{each} $w \in \adj(v)$}{
    \If{$D[w] = 0$}{
      $D[w] \leftarrow 1$\;
      $c \leftarrow c + 1$\;
      $\enqueue(Q, w)$\;
    }
  }
}
\If{$c = |V|$~\nllabel{alg:BFS:connectivity_test}}{
  \Return \True\;
} \Else{
  \Return \False\;
}
\caption{Determining whether an undirected graph is connected.}
\label{alg:graph_algorithms:graph_connectivity}
\end{algorithm}

Note that Algorithm~\ref{alg:graph_algorithms:graph_connectivity} uses
the BFS template of
Algorithm~\ref{alg:graph_algorithms:breadth_first_search_template},
with some minor changes. Instead of initializing the list $D$ with
$n = |V|$ copies of $\infty$, we use $n$ copies of $0$. Each time we
have visited a vertex $w$, we make the assignment $D[w] \leftarrow 1$,
instead of incrementing the value $D[v]$ of $w$'s parent vertex and
assign that value to $D[w]$. At the end of the while loop, we have the
equality $c = \sum_{d \in D} d$. The value of this sum could be used
in the test starting from line~\ref{alg:BFS:connectivity_test}.
However, the value of the counter $c$ is incremented immediately after
we have visited an unvisited vertex. An advantage is that we do not
need to perform a separate summation outside of the while loop. To use
the DFS template for determining graph connectivity, we simply replace
the queue implementation in
Algorithm~\ref{alg:graph_algorithms:graph_connectivity} with a stack
implementation.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Weights and distances}

In Chapter~\ref{chap:introduction}, we briefly mentioned some
applications of weighted graphs, but we did not define the concept of
weighted graphs. A graph is said to be
\emph{weighted}\index{weighted graph} when we assign a numeric label
or weight to each of its edges. Depending on the application, we can
let the vertices represent physical locations and interpret the weight
of an edge as the distance separating two adjacent vertices. There
might be a cost involved in travelling from a vertex to one of its
neighbors, in which case the weight assigned to the corresponding edge
can represent such a cost. The concept of
\emph{weighted digraphs}\index{weighted digraph} can be similarly
defined. When no explicit weights are assigned to the edges of an
undirected graph or digraph, it is usually convenient to consider each
edge as having a weight of one.

Based on the concept of weighted graphs, we now define what it means
for a path to be a shortest path. Let $G = (V,E)$ be a (di)graph with
non-negative edge weights $w(e)$ for each edge $e \in E$. The
\emph{length}\index{path!length} or
\emph{distance}\index{path!distance} $d(P)$ of a path $P$ from
$v \in V$ to $w \in V$ is the sum of the edge weights for edges in
$P$. Denote by $d(v,w)$ the smallest value of $d(P)$ for all paths $P$
from $v$ to $w$. When we regard edge weights as physical distances, a
$v$-$w$ path that realizes $d(v,w)$ is sometimes called a
\emph{shortest path}\index{path!shortest} from $v$ to $w$.

The distance function $d$ on a graph with nonnegative edge weights is
known as a \emph{metric function}. Intuitively, the distance between
two physical locations is greater than zero. When these two locations
coincide, i.e. they are one and the same location, the distance
separating them is zero. Regardless of whether we are measuring the
distance from location $a$ to $b$ or from $b$ to $a$, we would obtain
the same distance. Imagine now a third location $c$. The distance from
$a$ to $b$ plus the distance from $b$ to $c$ is greater than or equal
to the distance from $a$ to $c$. The latter principle is known as the
\emph{triangle inequality}. In summary, given three vertices $u,v,w$
in a connected graph $G$, the distance function $d$ on $G$ satisfies
the following property.

\begin{lemma}
Let $G = (V,E)$ be a connected graph with a positive weight function
$w: E \longrightarrow \RR^{+}$. Define a distance function
$d: V \times V \longrightarrow \RR$ given by
\[
d(u,v)
=
\begin{cases}
\infty, & \text{if there are no paths from $u$ to $v$}, \\
\min\{w(W) \;|\; \text{$W$ is a $u$-$v$ walk}\}, & \text{otherwise}.
\end{cases}
\]
Then $d$ satisfies the following properties:
%
\begin{enumerate}
\item Nonnegativity: $d(u,v) \geq 0$ with $d(u,v) = 0$ if and only if
  $u = v$.

\item Symmetry: $d(u,v) = d(v,u)$.

\item Triangle inequality: $d(u,v) + d(v,w) \geq d(u,w)$.
\end{enumerate}
\end{lemma}

The pair $(V, d)$ is called a \emph{metric space}, where the word
``metric'' refers to the distance function $d$. Any graphs we consider
are assumed to have finite sets of vertices. For this reason, $(V,d)$
is also known as a \emph{finite metric space}. The distance matrix
$D = [d(v_i, v_j)]$ of a connected graph is the distance matrix of its
finite metric space. The topic of metric space is covered in further
details in topology texts such as Runde~\cite{Runde2005} and Shirali
and Vasudeva~\cite{ShiraliVasudeva2006}.

Many different algorithms exist for computing a shortest path in a
weighted graph. Some only work if the graph has no negative weight
cycles. Some assume that there is a single start or source
vertex. Some compute the shortest paths from any vertex to any other,
and also detect if the graph has a negative weight cycle. No matter
what algorithm is used for the special case of non-negative weights,
the length of the shortest path can neither equal nor exceed the order
of the graph.

\begin{lemma}
\label{lem:graph_algorithms:shortest_path_length}
Fix a vertex $v$ in a connected graph $G = (V,E)$ of order
$n = |V|$. If there are no negative weight cycles in $G$, then there
exists a shortest path from $v$ to any other vertex $w \in V$ that
uses at most $n - 1$ edges.
\end{lemma}

\begin{proof}
Suppose that $G$ contains no negative weight cycles. Observe that at
most $n - 1$ edges are required to construct a path from $v$ to any
vertex $w$
(Corollary~\ref{cor:introduction:any_path_has_length_at_most_n_minus_1}).
Let $P$ denote such a path:
\[
P: v_0 = v,\, v_1,\, v_2, \dots, v_k = w.
\]
Since $G$ has no negative weight cycles, the weight of $P$ is no less
than the weight of $P'$, where $P'$ is the same as $P$ except that all
cycles have been removed. Thus, we can remove all cycles from $P$ and
obtain a $v$-$w$ path $P'$ of lower weight. Since the final path is
acyclic, it must have no more than $n - 1$ edges.
\end{proof}

\begin{algorithm}[!htpb]
\dontprintsemicolon  % no semicolon at end of pseudocode statements
%% data section
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
%% input/output
\Input{A weighted graph or digraph $G = (V, E)$, where the vertices
  are numbered as $V = \{1, 2, \dots, n\}$. A starting vertex $s$.}
\Output{A list $D$ of distances from $s$ to all other vertices. A list
  $P$ of parent vertices such that $P[v]$ is the parent of $v$.}
\BlankLine
$D \leftarrow [\infty, \infty, \dots, \infty]$~\tcc*[f]{$n$ copies of $\infty$}\;
let $C$ be a list of candidate vertices to visit\;
\While{$\length(C) > 0$}{
  select $v \in C$\;
  $C \leftarrow \remove(C, v)$\;
  \For{\emph{each} $u \in \adj(v)$~\nllabel{alg:generic_shortest_path:neighbors}}{
    \If{$D[u] > D[v] + w(vu)$}{
      $D[u] \leftarrow D[v] + w(vu)$\;
      $P[u] \leftarrow v$\;
      if $u$ is not in $C$, add $u$ to $C$\;
    }
  }
}
\Return $(D,P)$\;
\caption{A template for shortest path algorithms.}
\label{alg:graph_algorithms:generic_shortest_path_algorithm}
\end{algorithm}

Having defined weights and distances, we are now ready to discuss
shortest path algorithms for weighted graphs. The breadth-first search
Algorithm~\ref{alg:graph_algorithms:breadth_first_search_template} can
be applied where each edge has unit weight. Moving on to the general
case of graphs with positive edge weights, algorithms for determining
shortest paths in such graphs can be classified as
\emph{weight-setting} or
\emph{weight-corrrecting}~\cite{GalloPallottino1986}. A weight-setting
method traverses a graph and assigns weights that, once assigned,
remain unchanged for the duration of the algorithm. Weight-setting
algorithms cannot deal with negative weights. On the other hand, a
weight-correcting method is able to change the value of a weight many
times while traversing a graph. In contrast to a weight-setting
algorithm, a weight-correcting algorithm is able to deal with negative
weights, provided that the weight sum of any cycle is
nonnegative. The term \emph{negative cycle} refers to the weight sum $s$
of a cycle such that $s < 0$.

Algorithm~\ref{alg:graph_algorithms:generic_shortest_path_algorithm}
is a general template for many shortest path algorithms. With a tweak
here and there, one could modify it to suit the problem at hand. Note
that $w(vu)$ is the weight of the edge $vu$. If the input graph is
undirected, line~\ref{alg:generic_shortest_path:neighbors} considers
all the neighbors of $v$. For digraphs, we are interested in
out-neighbors of $v$ and accordingly we replace ``$u \in \adj(v)$'' in
line~\ref{alg:generic_shortest_path:neighbors} with
``$u \in \oadj(v)$''. The general flow of
Algorithm~\ref{alg:graph_algorithms:generic_shortest_path_algorithm}
follows the same pattern as depth-first and breadth-first searches.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Dijkstra's algorithm}
\label{sec:graph_algorithms:Dijkstra_algorithm}

Dijkstra's algorithm~\cite{Dijkstra1959}, discovered by E. W.~Dijkstra
in 1959, is a graph search algorithm that solves the single-source
shortest path problem for a graph with nonnegative edge
weights. Imagine that the vertices of a weighted graph represent
cities and edge weights represent distances between pairs of cities
connected by a direct road. Dijkstra's algorithm can be used to find a
shortest route from a fixed city to any other city.

Let $G = (V,E)$ be a (di)graph with nonnegative edge weights. Fix a
start or source vertex $s \in V$. Dijkstra's
Algorithm~\ref{alg:graph_algorithms:dijkstra_general} performs a
number of steps, basically one step for each vertex in $V$. First, we
initialize a list $D$ with $n$ copies of $\infty$ and then assign $0$
to $D[s]$. The purpose of the symbol $\infty$ is to denote the largest
possible value. The list $D$ is to store the distances of all shortest
paths from $s$ to any other vertices in $G$, where we take the
distance of $s$ to itself to be zero. The list $P$ of parent vertices
is initially empty and the queue $Q$ is initialized to all vertices in
$G$. We now consider each vertex in $Q$, removing any vertex after we
have visited it. The while loop starting on
line~\ref{alg:dijkstra_general:while_loop} runs until we have visited
all
vertices. Line~\ref{alg:dijkstra_general:find_vertex_minimal_distance}
chooses which vertex to visit, preferring a vertex $v$ whose distance
value $D[v]$ from $s$ is minimal. After we have determined such a
vertex $v$, we remove it from the queue $Q$ to signify that we have
visited $v$. The for loop starting on
line~\ref{alg:dijkstra_general:for_loop} adjusts the distance values
of each neighbor $u$ of $v$ such that $u$ is also in $Q$. If $G$ is
directed, we only consider out-neighbors of $v$ that are also in
$Q$. The conditional starting on
line~\ref{alg:dijkstra_general:if_relaxation} is where the adjustment
takes place. The expression $D[v] + w(vu)$ sums the distance from $s$
to $v$ and the distance from $v$ to $u$. If this total sum is less
than the distance $D[u]$ from $s$ to $u$, we assign this lesser
distance to $D[u]$ and let $v$ be the parent vertex of $u$. In this
way, we are choosing a neighbor vertex that results in minimal
distance from $s$. Each pass through the while loop decreases the
number of elements in $Q$ by one without adding any elements to
$Q$. Eventually, we would exit the while loop and the algorithm
returns the lists $D$ and $P$.

\begin{algorithm}[!htpb]
\dontprintsemicolon  % no semicolon at end of pseudocode statements
%% data section
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
%% input/output
\Input{An undirected or directed graph $G = (V, E)$ that is weighted
  and has no self-loops. The order of $G$ is $n > 0$. A vertex $s \in V$
  from which to start the search. Vertices are numbered from 1 to $n$,
  i.e. $V = \{1, 2, \dots, n\}$.}
\Output{A list $D$ of distances such that $D[v]$ is the distance of a
  shortest path from $s$ to $v$. A list $P$ of vertex parents such
  that $P[v]$ is the parent of $v$, i.e. $v$ is adjacent from $P[v]$.}
\BlankLine
%% algorithm body
$D \leftarrow [\infty, \infty, \dots, \infty]$~\tcc*[f]{$n$ copies of $\infty$}\;
$D[s] \leftarrow 0$\;
$P \leftarrow [\,]$\;
$Q \leftarrow V$~\tcc*[f]{list of nodes to visit}\;
\While{$\length(Q) > 0$~\nllabel{alg:dijkstra_general:while_loop}}{
  find $v \in Q$ such that $D[v]$ is minimal~\nllabel{alg:dijkstra_general:find_vertex_minimal_distance}\;
  $Q \leftarrow \remove(Q, v)$\;
  \For{\emph{each} $u \in \adj(v) \cap Q$~\nllabel{alg:dijkstra_general:for_loop}}{
    \If{$D[u] > D[v] + w(vu)$~\nllabel{alg:dijkstra_general:if_relaxation}}{
      $D[u] \leftarrow D[v] + w(vu)$\;
      $P[u] \leftarrow v$\;
    }
  }
}
\Return $(D, P)$\;
\caption{A general template for Dijkstra's algorithm.}
\label{alg:graph_algorithms:dijkstra_general}
\end{algorithm}

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  arrowdecorate/.style={->,>=stealth,thick}]
%% nodes or vertices
\foreach \nodename/\x/\y/\direction/\navigate in {
  v_1/0/0/below/south, v_2/2/2.5/above/north, v_3/4/0/below/south,
  v_4/6/2.5/above/north, v_5/8/0/below/south} {
  \node (\nodename) at (\x,\y) [nodedecorate] {};
  \node [\direction] at (\nodename.\navigate) {$\nodename$};
}
%% edges or lines
\path
\foreach \startnode/\endnode/\direction/\weight in {
  v_1/v_2/left/10, v_1/v_3/below/3, v_2/v_4/above/2, v_3/v_4/left/8,
  v_3/v_5/below/2} {
  (\startnode) edge[arrowdecorate] node[\direction]{$\weight$} (\endnode)
}
\foreach \startnode/\endnode/\direction/\weight in {
  v_3/v_2/left/4, v_2/v_3/right/1, v_4/v_5/right/7, v_5/v_4/left/9} {
  (\startnode) edge[arrowdecorate,bend left] node[\direction]{$\weight$} (\endnode)
};
\end{tikzpicture}
\caption{Searching a weighted digraph using Dijkstra's algorithm.}
\label{fig:graph_algorithms:Dijkstra_algorithm_digraph}
\end{figure}
%% sage: M = matrix([[0,10,3,0,0],[0,0,1,2,0],[0,4,0,8,2],[0,0,0,0,7],[0,0,0,9,0]])
%% sage: D = DiGraph(M, format="weighted_adjacency_matrix")
%% sage: D.plot(edge_labels=True, graph_border=True).show()
%%
%% sage: G = DiGraph({1: {2:10, 3:3},
%% ....: 2: {3:1, 4:2},
%% ....: 3: {2:4, 4:8, 5:2},
%% ....: 4: {5:7},
%% ....: 5: {4:9}}, implementation=''c_graph'')
%% sage: G.shortest_paths(1, by_weight=True)
%% {1: [1], 2: [1, 3, 2], 3: [1, 3], 4: [1, 3, 2, 4], 5: [1, 3, 5]}

\begin{table}[!htbp]
\centering
\begin{tabular}{ccccc} \hline
$v_1$               & $v_2$                 & $v_3$                 & $v_4$                 & $v_5$ \\\hline
\underline{$(0,-)$} & $(\infty,-)$          & $(\infty,-)$          & $(\infty,-)$          & $(\infty,-)$ \\
                    & $(10,v_1)$            & \underline{$(3,v_1)$} & $(11,v_3)$            & \underline{$(5,v_3)$} \\
                    & \underline{$(7,v_3)$} &                       & \underline{$(9,v_2)$} & \\\hline
\end{tabular}
\caption{Stepping through Dijkstra's algorithm.}
\label{tab:graph_algorithms:working_through_Dijkstra_algorithm}
\end{table}

\begin{example}
Apply Dijkstra's algorithm to the graph in
Figure~\ref{fig:graph_algorithms:Dijkstra_algorithm_digraph}, with
starting vertex $v_1$.
\end{example}

\begin{proof}[Solution]
Dijkstra's Algorithm~\ref{alg:graph_algorithms:dijkstra_general}
applied to the graph in
Figure~\ref{fig:graph_algorithms:Dijkstra_algorithm_digraph} yields
Table~\ref{tab:graph_algorithms:working_through_Dijkstra_algorithm}. For
any column $v_i$ in the table, each 2-tuple represents the distance
and parent vertex of $v_i$. As we move along the graph, processing
vertices according to Dijkstra's algorithm, the distance and parent
vertex of a column are updated. The underlined 2-tuple represents the
final distance and parent vertex produced by Dijkstra's
algorithm. From
Table~\ref{tab:graph_algorithms:working_through_Dijkstra_algorithm},
we have the following shortest paths and distances:
\[
\begin{array}{ll}
v_1\text{-}v_2: v_1, v_3, v_2      &\quad d(v_1, v_2) = 7 \\[4pt]
v_1\text{-}v_3: v_1, v_3           &\quad d(v_1, v_3) = 3 \\[4pt]
v_1\text{-}v_4: v_1, v_3, v_2, v_4 &\quad d(v_1, v_4) = 9 \\[4pt]
v_1\text{-}v_5: v_1, v_3, v_5      &\quad d(v_1, v_5) = 5
\end{array}
\]
Intermediary vertices for a $u$-$v$ path are obtained by starting from
$v$ and work backward using the parent of $v$, then the parent of the
parent, and so on.
\end{proof}

Dijkstra's algorithm is an example of a \emph{greedy algorithm}.
Whenever it tries to find the next vertex, it chooses only that vertex
that minimizes the total weight so far. Greedy algorithms may not
produce the best possible result. However, as the following theorem
shows, Dijkstra's algorithm does indeed produce shortest paths.

\begin{theorem}
\textbf{Correctness of
  Algorithm~\ref{alg:graph_algorithms:dijkstra_general}.}
Let $G = (V, E)$ be a weighted (di)graph with a nonnegative weight
function $w$. When Dijkstra's algorithm is applied to $G$ with source
vertex $s \in V$, the algorithm terminates with $D[u] = d(s,u)$ for
all $u \in V$. Furthermore, if $D[v] \neq \infty$ and $v \neq s$, then
$s=u_1, u_2, \dots, u_k = v$ is a shortest $s$-$v$ path such that
$u_{i-1} = P[u_i]$ for $i = 2,3,\dots,k$.
\end{theorem}

\begin{proof}
If $G$ is disconnected, then any $v \in V$ that cannot be reached from
$s$ has distance $D[v] = \infty$ upon algorithm termination. Hence, it
suffices to consider the case where $G$ is connected. Let
$V = \{s=v_1, v_2, \dots, v_n\}$ and use induction on $i$ to show that
after visiting $v_i$ we have
%
\begin{equation}
\label{eq:graph_algorithms:Dijkstra:shortest_distance}
D[v]
=
d(s,v)
\qquad
\text{for all $v \in V_i = \{v_1, v_2, \dots, v_i\}$}.
\end{equation}
%
For $i = 1$, equality holds. Assume for induction
that~(\ref{eq:graph_algorithms:Dijkstra:shortest_distance}) holds for
some $1 \leq i \leq n - 1$, so that now our task is to show
that~(\ref{eq:graph_algorithms:Dijkstra:shortest_distance}) holds for
$i + 1$. To verify $D[v_{i+1}] = d(s, v_{i+1})$, note that by our
inductive hypothesis,
\[
D[v_{i+1}]
=
\min \left\{
\left. d(s,v) + w(vu) \;\right|\;
v \in V_i \text{ and } u \in \adj(v) \cap (Q \backslash V_i)
\right\}
\]
and respectively
\[
D[v_{i+1}]
=
\min \left\{
\left. d(s,v) + w(vu) \;\right|\;
v \in V_i \text{ and } u \in \oadj(v) \cap (Q \backslash V_i)
\right\}
\]
if $G$ is directed. Therefore, $D[v_{i+1}] = d(s, v_{i+1})$.

Let $v \in V$ such that $D[v] \neq \infty$ and $v \neq s$. We now
construct an $s$-$v$ path. When
Algorithm~\ref{alg:graph_algorithms:dijkstra_general} terminates, we
have $D[v] = D[v_1] + w(v_1 v)$, where $P[v] = v_1$ and
$d(s,v) = d(s, v_1) + w(v_1 v)$. This means that $v_1$ is the
second-to-last vertex in a shortest $s$-$v$ path. Repeated application
of this process using the parent list $P$, we eventually produce a
shortest $s$-$v$ path $s=v_m, v_{m-1}, \dots, v_1, v$, where
$P[v_i] = v_{i+1}$ for $i = 1, 2, \dots, m - 1$.
\end{proof}

To analyze the worst case time complexity of
Algorithm~\ref{alg:graph_algorithms:dijkstra_general}, note that
initializing $D$ takes $O(n + 1)$ and initializing $Q$ takes $O(n)$,
for a total of $O(n)$ devoted to initialization. Each extraction of a
vertex $v$ with minimal $D[v]$ requires $O(n)$ since we search through
the entire list $Q$ to determine the minimum value, for a total of
$O(n^2)$. Each insertion into $D$ requires constant time and the same
holds for insertion into $P$. Thus, insertion into $D$ and $P$ takes
$O(|E| + |E|) = O(|E|)$, which require at most $O(n)$ time. In the
worst case, Dijkstra's
Algorithm~\ref{alg:graph_algorithms:dijkstra_general} has running time
$O(n^2 + n) = O(n^2)$.

Can we improve the run time of Dijkstra's algorithm? The time
complexity of Dijkstra's algorithm depends on its implementation. With
a simple list implementation as presented in
Algorithm~\ref{alg:graph_algorithms:dijkstra_general}, we have a worst
case time complexity of $O(n^2)$, where $n$ is the order of the graph
under consideration. Let $m$ be the size of the
graph. Table~\ref{tab:graph_algorithms:worst_case_time_complexity_Dijkstra}
presents time complexities of Dijkstra's algorithm for various
implementations. Out of all the four implementations in this table,
the heap implementations are much more efficient than the list
implementation presented in
Algorithm~\ref{alg:graph_algorithms:dijkstra_general}. A heap is a type
of tree, a topic which will be covered in
Chapter~\ref{chap:trees_forests}.

\begin{table}[!htbp]
\centering
\begin{tabular}{ll} \hline
Implementation & Time complexity \\\hline
list           & $O(n^2)$ \\
binary heap    & $O \big( \log(n) \cdot (n + m) \big)$ \\
$k$-ary heap   & $O \big( (kn + m) \frac{\log(n)}{\log(k)} \big)$ \\
Fibonacci heap & $O(n \cdot \log(n) + m)$ \\\hline
\end{tabular}
\caption{Implementation specific worst case time complexity of
  Dijkstra's algorithm.}
\label{tab:graph_algorithms:worst_case_time_complexity_Dijkstra}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Bellman-Ford algorithm}

\begin{quote}
\includegraphics[scale=2.5]{image/pillow-talk-bellman-ford} \\
\noindent
--- Randall Munroe\index{Munroe, Randall}, xkcd,
\url{http://xkcd.com/69/}
\end{quote}

\noindent
A disadvantage of Dijkstra's
Algorithm~\ref{alg:graph_algorithms:dijkstra_general} is that it
cannot handle graphs with negative edge weights. The Bellman-Ford
algorithm computes single-source shortest paths in a weighted graph or
digraph, where some of the edge weights may be negative. This
algorithm is a modification of the one published in 1957 by Richard E.
Bellman~\cite{Bellman1957} and that by Lester Randolph Ford,
Jr.~\cite{Ford1956} in 1956. Shimbel~\cite{Shimbel1955} independently
discovered the same method in~1955, and Moore~\cite{Moore1959}
in~1959. In contrast to the ``greedy'' approach that Dijkstra's
algorithm takes, i.e. searching for the ``cheapest'' path, the
Bellman-Ford algorithm searches over all edges and keeps track of the
shortest one found as it searches.

\begin{algorithm}[!htpb]
\dontprintsemicolon  % no semicolon at end of pseudocode statements
%% data section
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKwData{False}{False}
%% input/output
\Input{An undirected or directed graph $G = (V, E)$ that is weighted
  and has no self-loops. Negative edge weights are allowed. The order
  of $G$ is $n > 0$. A vertex $s \in V$ from which to start the
  search. Vertices are numbered from 1 to $n$, i.e.
  $V = \{1, 2, \dots, n\}$.}
\Output{A list $D$ of distances such that $D[v]$ is the distance of a
  shortest path from $s$ to $v$. A list $P$ of vertex parents such
  that $P[v]$ is the parent of $v$, i.e. $v$ is adjacent from
  $P[v]$. If $G$ has negative-weight cycles, then return
  \False. Otherwise, return $D$ and $P$.}
\BlankLine
%% algorithm body
$D \leftarrow [\infty, \infty, \dots, \infty]$~\tcc*[f]{$n$ copies of $\infty$}~\nllabel{alg:Bellman_Ford:init_infinity}\;
$D[s] \leftarrow 0$\;
$P \leftarrow [\,]$~\nllabel{alg:Bellman_Ford:init_parent_list}\;
\For{$i \leftarrow 1, 2, \dots, n-1$~\nllabel{alg:Bellman_Ford:for_loop:relax}}{
  \For{\emph{each edge} $uv \in E$}{
    \If{$D[v] > D[u] + w(uv)$}{
      $D[v] \leftarrow D[u] + w(uv)$\;
      $P[v] \leftarrow u$\;
    }
  }
  \nllabel{alg:Bellman_Ford:for_loop:end_relax}
}
\For{\emph{each edge} $uv \in E$~\nllabel{alg:Bellman_Ford:for_loop:check_negative_weight_cycles}}{
  \If{$D[v] > D[u] + w(uv)$}{
    \Return \False\;
  }
}
\Return $(D, P)$\;
\caption{The Bellman-Ford algorithm.}
\label{alg:graph_algorithms:Bellman_Ford}
\end{algorithm}

The Bellman-Ford Algorithm~\ref{alg:graph_algorithms:Bellman_Ford}
runs in time $O(mn)$, where $m$ and $n$ are the size and order of an
input graph, respectively. To see this, note that the initialization
on lines~\ref{alg:Bellman_Ford:init_infinity}
to~\ref{alg:Bellman_Ford:init_parent_list} takes $O(n)$. Each of the
$n - 1$ rounds of the for loop starting on
line~\ref{alg:Bellman_Ford:for_loop:relax} takes $O(m)$, for a total
of $O(mn)$ time. Finally, the for loop starting on
line~\ref{alg:Bellman_Ford:for_loop:check_negative_weight_cycles}
takes $O(m)$.

The loop starting on line~\ref{alg:Bellman_Ford:for_loop:relax}
performs at most $n - 1$ updates of the distance $D[v]$ of each head
of an edge. Many graphs have sizes that are less then $n - 1$,
resulting in a number of redundant rounds of updates. To avoid such
redundancy, we could add an extra check in the outer loop spanning
lines~\ref{alg:Bellman_Ford:for_loop:relax}
to~\ref{alg:Bellman_Ford:for_loop:end_relax} to immediately terminate
that outer loop after any round that did not result in an update of
any $D[v]$.
Algorithm~\ref{alg:graph_algorithms:Bellman_Ford:redundant_updates}
presents a modification of the Bellman-Ford
Algorithm~\ref{alg:graph_algorithms:Bellman_Ford} that avoids
redundant rounds of updates.

\begin{algorithm}[!htpb]
\dontprintsemicolon  % no semicolon at end of pseudocode statements
%% data section
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKwData{False}{False}
\SetKwData{True}{True}
\SetKwData{Updated}{updated}
%% input/output
\Input{An undirected or directed graph $G = (V, E)$ that is weighted
  and has no self-loops. Negative edge weights are allowed. The order
  of $G$ is $n > 0$. A vertex $s \in V$ from which to start the
  search. Vertices are numbered from 1 to $n$, i.e.
  $V = \{1, 2, \dots, n\}$.}
\Output{A list $D$ of distances such that $D[v]$ is the distance of a
  shortest path from $s$ to $v$. A list $P$ of vertex parents such
  that $P[v]$ is the parent of $v$, i.e. $v$ is adjacent from
  $P[v]$. If $G$ has negative-weight cycles, then return
  \False. Otherwise, return $D$ and $P$.}
\BlankLine
%% algorithm body
$D \leftarrow [\infty, \infty, \dots, \infty]$~\tcc*[f]{$n$ copies of $\infty$}~\nllabel{alg:Bellman_Ford:init_infinity}\;
$D[s] \leftarrow 0$\;
$P \leftarrow [\,]$\;
\For{$i \leftarrow 1, 2, \dots, n-1$}{
  $\Updated \leftarrow \False$\;
  \For{\emph{each edge} $uv \in E$}{
    \If{$D[v] > D[u] + w(uv)$}{
      $D[v] \leftarrow D[u] + w(uv)$\;
      $P[v] \leftarrow u$\;
      $\Updated \leftarrow \True$\;
    }
  }
  \If{$\Updated = \False$}{
    exit the loop\;
  }
}
\For{\emph{each edge} $uv \in E$}{
  \If{$D[v] > D[u] + w(uv)$}{
    \Return \False\;
  }
}
\Return $(D, P)$\;
\caption{The Bellman-Ford algorithm with checks for redundant updates.}
\label{alg:graph_algorithms:Bellman_Ford:redundant_updates}
\end{algorithm}

%% The implementation below takes in a graph or digraph, and creates two
%% Python dictionaries \verb!dist! and \verb!predecessor!, keyed on the
%% list of vertices, which store the distance and shortest
%% paths. However, if a negative weight cycle exists~(in the case of a
%% digraph), then an error is raised.

%% \begin{center}
%% \fontsize{9pt}{9pt}
%% \selectfont
%% \tt
%% \begin{lstlisting}
%% def bellman_ford(Gamma, s):
%%     """
%%     Computes the shortest distance from s to all other vertices in Gamma.
%%     If Gamma has a negative weight cycle, then return an error.

%%     INPUT:

%%     - Gamma -- a graph.
%%     - s -- the source vertex.

%%     OUTPUT:

%%     - (d,p) -- pair of dictionaries keyed on the list of vertices,
%%       which store the distance and shortest paths.

%%     REFERENCE:

%%     http://en.wikipedia.org/wiki/Bellman-Ford_algorithm
%%     """
%%     P = []
%%     dist = {}
%%     predecessor = {}
%%     V = Gamma.vertices()
%%     E = Gamma.edges()
%%     for v in V:
%%         if v == s:
%%             dist[v] = 0
%%         else:
%%             dist[v] = infinity
%%         predecessor[v] = 0
%%     for i in range(1, len(V)):
%%         for e in E:
%%             u = e[0]
%%             v = e[1]
%%             wt = e[2]
%%             if dist[u] + wt < dist[v]:
%%                 dist[v] = dist[u] + wt
%%                 predecessor[v] = u
%%     # check for negative-weight cycles
%%     for e in E:
%%         u = e[0]
%%         v = e[1]
%%         wt = e[2]
%%         if dist[u] + wt < dist[v]:
%%             raise ValueError("Graph contains a negative-weight cycle")
%%     return dist, predecessor
%% \end{lstlisting}
%% \end{center}

%% Here are some examples.

%% \begin{center}
%% \fontsize{9pt}{9pt}
%% \selectfont
%% \tt
%% \begin{lstlisting}
%% sage: M = matrix([[0,1,4,0], [0,0,1,5], [0,0,0,3], [0,0,0,0]])
%% sage: G = Graph(M, format="weighted_adjacency_matrix")
%% sage: bellman_ford(G, G.vertices()[0])
%%   {0: 0, 1: 1, 2: 2, 3: 5}
%% \end{lstlisting}
%% \end{center}
%% %
%% The plot of this graph is given in
%% Figure~\ref{fig:graph_algorithms:Bellman_Ford_example}.

%% \begin{figure}[!htbp]
%% \centering
%% \begin{tikzpicture}
%% [nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
%%   linedecorate/.style={-,thick}]
%% % nodes or vertices
%% \node (0) at (0,0) [nodedecorate] {};
%% \node [below] at (0.south) {$0$};
%% \node (2) at (4,0) [nodedecorate] {};
%% \node [below] at (2.south) {$2$};
%% \node (1) at (1,2.5) [nodedecorate] {};
%% \node [above] at (1.north) {$1$};
%% \node (3) at (5,2.5) [nodedecorate] {};
%% \node [above] at (3.north) {$3$};
%% % edges or lines
%% \path
%% (0) edge[linedecorate] node[left]{$1$} (1)
%% (0) edge[linedecorate] node[below]{$4$} (2)
%% (1) edge[linedecorate] node[right]{$1$} (2)
%% (1) edge[linedecorate] node[above]{$5$} (3)
%% (2) edge[linedecorate] node[right]{$3$} (3);
%% \end{tikzpicture}
%% \caption{Shortest paths in a weighted graph using the Bellman-Ford
%%   algorithm.}
%% \label{fig:graph_algorithms:Bellman_Ford_example}
%% \end{figure}
%% %sage: M = matrix([[0,1,4,0],[0,0,1,5],[0,0,0,3],[0,0,0,0]])
%% %sage: G = Graph(M, format = "weighted_adjacency_matrix")
%% %sage: G.plot(graph_border=True, edge_labels=True).show()

%% The following example illustrates the case of a negative-weight cycle.

%% \begin{center}
%% \fontsize{9pt}{9pt}
%% \selectfont
%% \tt
%% \begin{lstlisting}
%% sage: M = matrix([[0,1,0,0],[1,0,-4,1],[1,1,0,0],[0,0,1,0]])
%% sage: G = DiGraph(M, format = "weighted_adjacency_matrix")
%% sage: bellman_ford(G, G.vertices()[0])
%% ---------------------------------------------------------------------------
%% ...
%% ValueError: Graph contains a negative-weight cycle
%% \end{lstlisting}
%% \end{center}
%% %
%% The plot of this graph is given in
%% Figure~\ref{fig:graph_algorithms:Bellman_Ford_negative_weights}.

%% \begin{figure}[!htbp]
%% \centering
%% \begin{tikzpicture}
%% [nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
%%   arrowdecorate/.style={->,>=stealth,thick}]
%% % nodes or vertices
%% \node (0) at (5,0) [nodedecorate] {};
%% \node [below] at (0.south) {$0$};
%% \node (1) at (1,0) [nodedecorate] {};
%% \node [below] at (1.south) {$1$};
%% \node (2) at (4,3) [nodedecorate] {};
%% \node [above] at (2.north) {$2$};
%% \node (3) at (0,3) [nodedecorate] {};
%% \node [above] at (3.north) {$3$};
%% % edges or lines
%% \path
%% (0) edge[arrowdecorate,bend left=15] node[below]{$1$} (1)
%% (1) edge[arrowdecorate,bend left=10] node[above]{$1$} (0)
%% (1) edge[arrowdecorate,bend left=15] node[left]{$-4$} (2)
%% (1) edge[arrowdecorate] node[left]{$1$} (3)
%% (2) edge[arrowdecorate] node[right]{$1$} (0)
%% (2) edge[arrowdecorate,bend left=15] node[right]{$1$} (1)
%% (3) edge[arrowdecorate] node[above]{$1$} (2);
%% \end{tikzpicture}
%% \caption{Searching a digraph with negative weight using the
%%   Bellman-Ford algorithm.}
%% \label{fig:graph_algorithms:Bellman_Ford_negative_weights}
%% \end{figure}
%% %sage: M = matrix([[0,1,0,0],[1,0,-4,1],[1,1,0,0],[0,0,1,0]])
%% %sage: G = Graph(M, format = "weighted_adjacency_matrix")
%% %sage: G.plot(graph_border=True, edge_labels=True).show()


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Floyd-Roy-Warshall algorithm}

Let $D$ be a weighted digraph of order $n$ and size $m$. Dijkstra's
Algorithm~\ref{alg:graph_algorithms:dijkstra_general} and the
Bellman-Ford Algorithm~\ref{alg:graph_algorithms:Bellman_Ford} can be
used to determine shortest paths from a single source vertex to all
other vertices of $D$. To determine a shortest path between each pair
of distinct vertices in $D$, we repeatedly apply either of these
algorithms to each vertex of $D$. Such repeated application of
Dijkstra's and the Bellman-Ford algorithms results in algorithms that
run in time $O(n^3)$ and $O(n^2m)$, respectively.

The \emph{Floyd-Roy-Warshall algorithm}~(FRW), or the Floyd-Warshall
algorithm, is an algorithm for finding shortest paths in a weighted,
directed graph. Like the Bellman-Ford algorithm, it allows for
negative edge weights and detects a negative weight cycle if one
exists. Assuming that there are no negative weight cycles, a single
execution of the FRW algorithm will find the shortest paths between
all pairs of vertices. It was discovered independently by Bernard
Roy~\cite{Roy1959} in 1959, Robert Floyd~\cite{Floyd1962} in 1962, and
by Stephen Warshall~\cite{Warshall1962} in 1962.

In some sense, the FRW algorithm is an example of
\emph{dynamic programming}, which allows one to break the computation
into simpler steps using some sort of recursive procedure. The rough
idea is as follows. Temporarily label the vertices of a weighted
digraph $G$ as $V = \{1,2,\dots,n\}$ with $n = |V(G)|$. Let
$W = [w(i,j)]$ be the weight matrix of $G$ where
%
\begin{equation}
\label{eq:graph_algorithms:Floyd_Roy_Warshall_weight_matrix}
w(i,j)
=
\begin{cases}
w(ij), & \text{if $ij \in E(G)$}, \\
0, & \text{if $i = j$}, \\
\infty, & \text{otherwise}.
\end{cases}
\end{equation}
%
Let $P_k(i,j)$ be a shortest path from $i$ to $j$ such that its
intermediate vertices are in $\{1, 2, \dots, k\}$. Let $D_k(i,j)$ be
the weight (or distance) of $P_k(i,j)$. If no shortest $i$-$j$ paths
exist, define $P_k(i,j) = \infty$ and $D_k(i,j) = \infty$ for all
$k \in \{1, 2, \dots, n\}$. If $k = 0$, then $P_0(i,j): i, j$ since no
intermediate vertices are allowed in the path and hence
$D_0(i,j) = w(i,j)$. In other words, if $i$ and $j$ are adjacent, a
shortest $i$-$j$ path is the edge $ij$ itself and the weight of this
path is simply the weight of $ij$. Now consider $P_k(i,j)$ for
$k > 0$. Either $P_k(i,j)$ passes through $k$ or it does not. If $k$
is not on the path $P_k(i,j)$, then the intermediate vertices of
$P_k(i,j)$ are in  $\{1, 2, \dots, k-1\}$, as are the vertices of
$P_{k-1}(i,j)$. In case $P_k(i,j)$ contains the vertex $k$, then
$P_k(i,j)$ traverses $k$ exactly once by the definition of path. The
$i$-$k$ subpath in $P_k(i,j)$ is a shortest $i$-$k$ path whose
intermediate vertices are drawn from $\{1, 2, \dots, k-1\}$, which is
also the set of intermediate vertices for the $k$-$j$ subpath in
$P_k(i,j)$. That is, to obtain $P_k(i,j)$, we take the union of the
paths $P_{k-1}(i,k)$ and $P_{k-1}(k,j)$. We compute the weight
$D_k(i,j)$ of $P_k(i,j)$ using the expression
%
\begin{equation}
\label{eq:graph_algorithms:Floyd_Roy_Warshall:shortest_path_weights}
D_k(i,j)
=
\begin{cases}
w(i,j), & \text{if $k = 0$}, \\
\min\{D_{k-1}(i,j),\, D_{k-1}(i,k) + D_{k-1}(k,j)\}, & \text{if $k > 0$}.
\end{cases}
\end{equation}

The key to the Floyd-Roy-Warshall algorithm lies in exploiting
expression~(\ref{eq:graph_algorithms:Floyd_Roy_Warshall:shortest_path_weights}).
If $n = |V|$, then this is a $O(n^3)$ time algorithm. For
comparison, the Bellman-Ford algorithm has complexity
$O(|V| \cdot |E|)$, which is $O(n^3)$ time for dense graphs. However,
Bellman-Ford only yields the shortest paths emanating from a
\emph{single} vertex. To achieve comparable output, we would need to
iterate Bellman-Ford over \emph{all} vertices, which would be
an  $O(n^4)$ time algorithm for dense graphs. Except possibly for
sparse graphs, Floyd-Roy-Warshall is better than an iterated
implementation of Bellman-Ford. Note that $P_k(i,k) = P_{k-1}(i,k)$
and $P_k(k,i) = P_{k-1}(k,i)$, consequently $D_k(i,k) = D_{k-1}(i,k)$
and $D_k(k,i) = D_{k-1}(k,i)$. This observation allows us to replace
$P_k(i,j)$ with $P(i,j)$ for $k = 1, 2, \dots, n$. The final results of
$P(i,j)$ and $D(i,k)$ are the same as $P_n(i,j)$ and $D_n(i,j)$,
respectively. Algorithm~\ref{alg:graph_algorithms:Floy_Roy_Warshall}
summarizes the above discussion into an algorithmic presentation.

\begin{algorithm}[!htpb]
\dontprintsemicolon  % no semicolon at end of pseudocode statements
%% data section
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
%% input/output
\Input{A weighted digraph $G = (V, E)$ that has no
  self-loops. Negative edge weights are allowed. The order of $G$ is
  $n > 0$. Vertices are numbered from 1 to $n$, i.e.
  $V = \{1, 2, \dots, n\}$. The weight matrix $W = [w(i,j)]$ of $G$ as
  defined in~(\ref{eq:graph_algorithms:Floyd_Roy_Warshall_weight_matrix}).}
\Output{A matrix $P = [a_{ij}]$ of shortest paths in $G$. A matrix
  $D = [a_{ij}]$ of distances where $D[i,j]$ is the weight~(or
  distance) of a shortest $i$-$j$ path in $G$.}
\BlankLine
%% algorithm body
$n \leftarrow |V|$\;
$P[a_{ij}] \leftarrow$ an $n \times n$ zero matrix\;
$D[a_{ij}] \leftarrow W[w(i,j)]$\;
\For{$k \leftarrow 1, 2, \dots, n$}{
  \For{$i \leftarrow 1, 2, \dots, n$}{
    \For{$j \leftarrow 1, 2, \dots, n$}{
      \If{$D[i,j] > D[i,k] + D[k,j]$}{
        $P[i,j] \leftarrow k$\;
        $D[i,j] \leftarrow D[i,k] + D[k,j]$\;
      }
    }
  }
}
\Return $(P,D)$\;
\caption{The Floyd-Roy-Warshall algorithm for all-pairs shortest paths.}
\label{alg:graph_algorithms:Floy_Roy_Warshall}
\end{algorithm}

Like the Bellman-Ford algorithm, the Floyd-Roy-Warshall algorithm can
also detect the presence of negative weight cycles. If $G$ is a
weighted digraph without self-loops,
by~(\ref{eq:graph_algorithms:Floyd_Roy_Warshall_weight_matrix}) we
have $D(i,i) = 0$ for $i = 1, 2, \dots, n$. Any path $p$ starting and
ending at $i$ could only improve upon the initial weight of $0$ if the
weight sum of $p$ is less than zero, i.e. a negative-weight
cycle. Upon termination of
Algorithm~\ref{alg:graph_algorithms:Floy_Roy_Warshall}, if $D(i,i) <
0$, we conclude that there is a path starting and ending at $i$ whose
weight sum is negative.

Here is an implementation in Sage.
%
\begin{lstlisting}
def floyd_roy_warshall(A):
    """
    Shortest paths

    INPUT:

    - A -- weighted adjacency matrix

    OUTPUT:

    - dist -- a matrix of distances of shortest paths.
    - paths -- a matrix of shortest paths.
    """
    G = Graph(A, format="weighted_adjacency_matrix")
    V = G.vertices()
    E = [(e[0],e[1]) for e in G.edges()]
    n = len(V)
    dist = [[0]*n for i in range(n)]
    paths = [[-1]*n for i in range(n)]
    # initialization step
    for i in range(n):
        for j in range(n):
            if (i,j) in E:
                paths[i][j] = j
            if i == j:
                dist[i][j] = 0
            elif A[i][j]<>0:
                dist[i][j] = A[i][j]
            else:
                dist[i][j] = infinity
    # iteratively finding the shortest path
    for j in range(n):
        for i in range(n):
            if i <> j:
                for k in range(n):
                    if k <> j:
                        if dist[i][k]>dist[i][j]+dist[j][k]:
                            paths[i][k] = V[j]
                        dist[i][k] = min(dist[i][k], dist[i][j] +dist[j][k])
    for i in range(n):
        if dist[i][i] < 0:
            raise ValueError, "A negative edge weight cycle exists."
    return dist, matrix(paths)
\end{lstlisting}

Here are some examples.

%
%\begin{center}
%\fontsize{9pt}{9pt}
%\selectfont
%\tt
%\begin{lstlisting}
%
%        sage: A = matrix([[0,1,2,3],[0,0,2,1],[20,10,0,3],[11,12,13,0]]); A
%        sage: floyd_roy_warshall(A)
%        ([[0, 1, 2, 2], [12, 0, 2, 1], [14, 10, 0, 3], [11, 12, 13, 0]],
%          [-1  1  2  1]
%          [ 3 -1  2  3]
%          [ 3 -1 -1  3]
%          [-1 -1 -1 -1])
%
%\end{lstlisting}
%\end{center}
%

%
%\begin{center}
%\fontsize{9pt}{9pt}
%\selectfont
%\tt
%\begin{lstlisting}
%
%        sage: A = matrix([[0,1,2,4],[0,0,2,1],[0,0,0,5],[0,0,0,0]])
%        sage: floyd_roy_warshall(A)
%        ([[0, 1, 2, 2], [+Infinity, 0, 2, 1], [+Infinity, +Infinity, 0, 5],
%          [+Infinity, +Infinity, +Infinity, 0]],
%          [-1  1  2  1]
%          [-1 -1  2  3]
%          [-1 -1 -1  3]
%          [-1 -1 -1 -1])
%
%\end{lstlisting}
%\end{center}
%

\begin{lstlisting}
sage: A = matrix([[0,1,2,3], [0,0,2,1], [-5,0,0,3], [1,0,1,0]]); A
sage: floyd_roy_warshall(A)
Traceback (click to the left of this block for traceback)
...
ValueError: A negative edge weight cycle exists.
\end{lstlisting}

The plot of this weighted digraph with four vertices appears in
Figure~\ref{fig:graph_algorithms:Floyd_Roy_Warshall_demo}.

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  arrowdecorate/.style={->,>=stealth,thick}]
% nodes or vertices
\node (0) at (0,0) [nodedecorate] {};
\node [below] at (0.south) {$0$};
\node (1) at (6,0) [nodedecorate] {};
\node [below] at (1.south) {$1$};
\node (2) at (6,5) [nodedecorate] {};
\node [above] at (2.north) {$2$};
\node (3) at (0,5) [nodedecorate] {};
\node [above] at (3.north) {$3$};
% edges or lines
\path
(0) edge[arrowdecorate] node[below]{$1$} (1)
(0) edge[arrowdecorate,bend left=15] node[below left]{$2$} (2)
(0) edge[arrowdecorate,bend left=15] node[left]{$3$} (3)
(1) edge[arrowdecorate] node[right]{$2$} (2)
(1) edge[arrowdecorate] node[right]{$1$} (3)
(2) edge[arrowdecorate,bend left=15] node[above right]{$5$} (0)
(2) edge[arrowdecorate,bend left=10] node[below]{$3$} (3)
(3) edge[arrowdecorate,bend left=15] node[right]{$1$} (0)
(3) edge[arrowdecorate,bend left=10] node[above]{$1$} (2);
\end{tikzpicture}
\caption{Demonstrating the Floyd-Roy-Warshall algorithm.}
\label{fig:graph_algorithms:Floyd_Roy_Warshall_demo}
\end{figure}
%sage: A = matrix([[0,1,2,3],[0,0,2,1],[-5,0,0,3],[1,0,1,0]])
%sage: D = DiGraph(A, format="weighted_adjacency_matrix")
%sage: D.plot(edge_labels=True, graph_border=True).show()

\begin{lstlisting}
sage: A = matrix([[0,1,2,3], [0,0,2,1], [-1/2,0,0,3], [1,0,1,0]]); A
sage: floyd_roy_warshall(A)
([[0, 1, 2, 2], [3/2, 0, 2, 1], [-1/2, 1/2, 0, 3/2], [1/2, 3/2, 1, 0]],
  [-1  1  2  1]
  [ 2 -1  2  3]
  [-1  0 -1  1]
  [ 2  2 -1 -1])
\end{lstlisting}

The plot of this weighted digraph with four vertices appears in
Figure~\ref{fig:graph_algorithms:another_Floyd_Roy_Warshall_demo}.

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  arrowdecorate/.style={->,>=stealth,thick}]
% nodes or vertices
\node (0) at (0,0) [nodedecorate] {};
\node [below] at (0.south) {$0$};
\node (1) at (0,7) [nodedecorate] {};
\node [above] at (1.north) {$1$};
\node (2) at (4,3.5) [nodedecorate] {};
\node [right] at (2.east) {$2$};
\node (3) at (-4,3.5) [nodedecorate] {};
\node [left] at (3.west) {$3$};
% edges or lines
\path
(0) edge[arrowdecorate] node[left]{$1$} (1)
(0) edge[arrowdecorate,bend left=10] node[above]{$2$} (2)
(0) edge[arrowdecorate,bend left=15] node[below]{$3$} (3)
(1) edge[arrowdecorate] node[above]{$2$} (2)
(1) edge[arrowdecorate] node[above]{$1$} (3)
(2) edge[arrowdecorate,bend left=15] node[below]{$-1/2$} (0)
(2) edge[arrowdecorate,bend left=10] node[below right]{$3$} (3)
(3) edge[arrowdecorate,bend left=10] node[above]{$1$} (0)
(3) edge[arrowdecorate,bend left=10] node[above right]{$1$} (2);
\end{tikzpicture}
\caption{Another demonstration of the Floyd-Roy-Warshall algorithm.}
\label{fig:graph_algorithms:another_Floyd_Roy_Warshall_demo}
\end{figure}
%sage: A = matrix([[0,1,2,3],[0,0,2,1],[-1/2,0,0,3],[1,0,1,0]])
%sage: D = DiGraph(A, format="weighted_adjacency_matrix")
%sage: D.plot(edge_labels=True, graph_border=True).show()


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Transitive closure}

Consider a digraph $G = (V, E)$ of order $n = |V|$. The
\emph{transitive closure}\index{transitive closure} of $G$ is defined
as the digraph $G^* = (V, E^*)$ having the same vertex set as
$G$. However, the edge set $E^*$ of $G^*$ consists of all edges $uv$
such that there is a $u$-$v$ path in $G$ and $uv \notin E$. The
transitive closure $G^*$ answers an important question about $G$: If
$u$ and $v$ are two distinct vertices of $G$, are they connected by a
path with length $\geq 1$?

To compute the transitive closure of $G$, we let each edge of $G$ be
of unit weight and apply the Floyd-Roy-Warshall
Algorithm~\ref{alg:graph_algorithms:Floy_Roy_Warshall} on $G$. By
Corollary~\ref{cor:introduction:any_path_has_length_at_most_n_minus_1},
for any $i$-$j$ path in $G$ we have $D[i,j] < n$, and if there are no
paths from $i$ to $j$ in $G$, we have $D[i,j] = \infty$. This
procedure for computing transitive closure runs in time $O(n^3)$.

Modifying the Floyd-Roy-Warshall algorithm slightly, we obtain an
algorithm for computing transitive closure that, in practice, is more
efficient than Algorithm~\ref{alg:graph_algorithms:Floy_Roy_Warshall}
in terms of time and space. Instead of using the operations $\min$ and
$+$ as is the case in the Floyd-Roy-Warshall algorithm, we replace
these operations with the logical operations $\vee$~(logical \OR) and
$\wedge$~(logical \AND), respectively. For $i,j,k = 1, 2, \dots, n$,
define $T_k(i,j) = 1$ if there is an $i$-$j$ path in $G$ with all
intermediate vertices belonging to $\{1, 2, \dots, k\}$, and
$T_k(i,j) = 0$ otherwise. Thus, the edge $ij$ belongs to the
transitive closure $G^*$ if and only if $T_k(i,j) = 1$. The definition
of $T_k(i,j)$ can be cast in the form of a recursive definition as
follows. For $k = 0$, we have
\[
T_0(i,j)
=
\begin{cases}
0, & \text{if $i \neq j$ and $ij \notin E$}, \\
1, & \text{if $i = j$ or $ij \in E$}
\end{cases}
\]
and for $k > 0$, we have
\[
T_k(i,j)
=
T_{k-1}(i,j) \vee \big( T_{k-1}(i,k) \wedge T_{k-1}(k,j) \big).
\]
We need not use the subscript $k$ at all and instead let $T$ be a
boolean matrix such that $T[i,j] = 1$ if and only if there is an
$i$-$j$ path in $G$, and $T[i,j] = 0$ otherwise. Using the above
notations, the Floyd-Roy-Warshall algorithm is translated to
Algorithm~\ref{alg:graph_algorithms:Floy_Roy_Warshall:transitive_closure}
for obtaining the boolean matrix $T$. We can then use $T$ and the
definition of transitive closure to obtain the edge set $E^*$ in the
transitive closure $G^* = (V, E^*)$ of $G = (V, E)$.

A more efficient transitive closure algorithm can be found in the PhD
thesis of Esko Nuutila~\cite{Nuutila1995}. The transitive closure
algorithm as presented in
Algorithm~\ref{alg:graph_algorithms:Floy_Roy_Warshall:transitive_closure}
is due to Stephen Warshall~\cite{Warshall1962}. It is a special case
of a more general algorithm in automata theory due to Stephen
Kleene~\cite{Kleene1956}, called Kleene's algorithm.

\begin{algorithm}[!htpb]
\dontprintsemicolon  % no semicolon at end of pseudocode statements
%% data section
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
%% input/output
\Input{A digraph $G = (V, E)$ that has no self-loops. Vertices are
  numbered from 1 to $n$, i.e. $V = \{1, 2, \dots, n\}$.}
\Output{The boolean matrix $T$ such that $T[i,j] = 1$ if and only if
  there is an $i$-$j$ path in $G$, and $T[i,j] = 0$ otherwise.}
\BlankLine
%% algorithm body
$n \leftarrow |V|$\;
$T \leftarrow$ adjacency matrix of $G$\;
%% \For{$i \leftarrow 1, 2, \dots, n$}{
%%   \For{$j \leftarrow 1, 2, \dots, n$}{
%%     \eIf{$i = j$ \emph{or} $ij \in E$}{
%%       $T[i,j] \leftarrow 1$\;
%%     }{
%%       $T[i,j] \leftarrow 0$\;
%%     }
%%   }
%% }
\For{$k \leftarrow 1, 2, \dots, n$}{
  \For{$i \leftarrow 1, 2, \dots, n$}{
    \For{$j \leftarrow 1, 2, \dots, n$}{
      $T[i,j] \leftarrow T[i,j] \vee \big( T[i,k] \wedge T[k,j] \big)$\;
    }
  }
}
\Return $T$\;
\caption{Variant of the Floyd-Roy-Warshall algorithm for transitive closure.}
\label{alg:graph_algorithms:Floy_Roy_Warshall:transitive_closure}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Johnson's algorithm}

See section~25.3 of Cormen~et~al.~\cite{CormenEtAl2001} and
Johnson~\cite{Johnson1977}.

Let $G = (V,E)$ be a graph with edge weights but no negative cycles.
\emph{Johnson's algorithm} finds a shortest path between all pairs of
vertices in a ``sparse'' directed graph.
\index{Johnson's algorithm}

\begin{algorithm}[!htpb]
\dontprintsemicolon  % no semicolon at end of pseudocode statements
%% data section
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKwData{False}{False}
%% input/output
\Input{A sparse weighted digraph $G = (V, E)$, where the vertex set is
  $V = \{1, 2, \dots, n\}$.}
\Output{If $G$ has negative-weight cycles, then return
  \False. Otherwise, return an $n \times n$ matrix $D$ of shortest-path
  weights and a list $P$ such that $P[v]$ is a parent list resulting
  from running Dijkstra's algorithm on $G$ with start vertex $v$.}
\BlankLine
%% algorithm body
$s \leftarrow$ vertex not in $V$\;
$V' \leftarrow V \cup \{s\}$\;
$E' \leftarrow E \cup \{sv \;|\; v \in V\}$\;
$G' \leftarrow$ digraph $(V', E')$ with weight $w(sv) = 0$ for all $v \in V$\;
\If{$\BellmanFord(G', w, s) = \False$}{
  \Return \False\;
}
$d \leftarrow$ distance list returned by $\BellmanFord(G', w, s)$\;
\For{\emph{each edge} $uv \in E'$}{
  $\hat{w}(uv) \leftarrow w(uv) + d[u] - d[v]$\;
}
\For{\emph{each vertex} $u \in V$}{
  $(\hat{\delta}, \hat{P}) \leftarrow$ distance and parent lists returned by $\Dijkstra(G, \hat{w}, u)$\;
  $P[u] \leftarrow \hat{P}$\;
  \For{\emph{each vertex} $v \in V$}{
    $D[u,v] \leftarrow \hat{\delta}[v] + d[v] - d[u]$\;
  }
}
\Return $(D, P)$\;
\caption{Johnson's algorithm for sparse graphs.}
\label{alg:graph_algorithms:Johnson_algorithm}
\end{algorithm}

The time complexity, for sparse graphs, is
$O(|V|^2\log |V| + |V| \cdot |E)|=O(n^2\log n)$, where $n = |V|$ is
the number of vertices of the original graph $G$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Problems}

\begin{enumerate}
\item Let $G = (V, E)$ be an undirected graph, let $s \in V$, and $D$
  is the list of distances resulting from running
  Algorithm~\ref{alg:graph_algorithms:breadth_first_search_template}
  with $G$ and $s$ as input. Show that $G$ is connected if and only if
  $D[v]$ is defined for each $v \in V$.

\item Show that the worst-case time complexity of depth-first search
  Algorithm~\ref{alg:graph_algorithms:depth_first_search_template} is
  $O(|V| + |E|)$.

\item Let $D$ be the list of distances returned by
  Algorithm~\ref{alg:graph_algorithms:depth_first_search_template},
  let $s$ be a starting vertex, and let $v$ be a vertex such that
  $D[v] \neq \infty$. Show that $D[v]$ is the length of any shortest
  path from $s$ to $v$.

\item Consider the graph in
  Figure~\ref{fig:graph_algorithms:Dijkstra_algorithm_digraph} as
  undirected. Run this undirected version through Dijkstra's algorithm
  with starting vertex $v_1$.

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}
[nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
  arrowdecorate/.style={->,>=stealth,thick}]
%% nodes or vertices
\foreach \nodename/\x/\y/\direction/\navigate in {v_1/4/0/below/south,
  v_2/0/0/below/south, v_3/0/3/above/north, v_4/4/3/above/north,
  v_5/7/1.5/right/east} {
  \node (\nodename) at (\x,\y) [nodedecorate] {};
  \node [\direction] at (\nodename.\navigate) {$\nodename$};
}
%% edges or lines
\path
\foreach \startnode/\endnode/\direction/\weight in {v_1/v_2/below/1,
  v_1/v_5/below/6, v_2/v_4/right/3, v_3/v_4/above/1, v_4/v_1/right/3} {
  (\startnode) edge[arrowdecorate] node[\direction]{$\weight$} (\endnode)
}
\foreach \startnode/\endnode/\direction/\angle/\weight in {
  v_3/v_2/right/20/2, v_3/v_1/above left/15/1, v_2/v_3/left/20/1,
  v_1/v_3/below right/15/3, v_4/v_5/above/15/2, v_5/v_4/below/15/1} {
  (\startnode) edge[arrowdecorate,bend left=\angle] node[\direction]{$\weight$} (\endnode)
};
\end{tikzpicture}
\caption{Searching a directed house graph using Dijkstra's algorithm.}
\label{fig:graph_algorithms:Dijkstra_directed_house_graph}
\end{figure}
%sage: M = matrix([[0,1,3,0,6],[0,0,1,3,0],[1,2,0,1,0],[3,0,0,0,2],[0,0,0,1,0]])
%sage: D = DiGraph(M, format="weighted_adjacency_matrix")
%sage: D.plot(edge_labels=True, graph_border=True).show()

\item Consider the graph in
  Figure~\ref{fig:graph_algorithms:Dijkstra_directed_house_graph}. Choose
  any vertex as a starting vertex and run Dijkstra's algorithm over
  it. Now consider the undirected version of that digraph and repeat
  the exercise.
\end{enumerate}
