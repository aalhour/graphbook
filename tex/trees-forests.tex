%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% This file is part of the book
%%
%% Algorithmic Graph Theory
%% http://code.google.com/p/graph-theory-algorithms-book/
%%
%% Copyright (C) 2010 David Joyner <wdjoyner@gmail.com>
%% Copyright (C) 2009, 2010 Minh Van Nguyen <nguyenminh2@gmail.com>
%%
%% See the file COPYING for copying conditions.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Trees and Forests}
\label{chap:trees_forests}

\begin{quote}
\includegraphics[scale=3]{image/trees-forests/in-the-trees.png} \\
\noindent
--- Randall Munroe\index{Munroe, Randall}, xkcd,
\url{http://xkcd.com/71/}
\end{quote}

\noindent
In section~\ref{subsec:introduction:walks_trails_paths}, we briefly
touched upon trees and provided examples of how trees could be used to
model hierarchical structures. This chapter provides an in-depth study
of trees, their properties, and various applications.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Definitions and examples}

Recall that a path in a graph $G = (V, E)$ whose start and end vertices
are the same is called a cycle\index{cycle}. We say $G$ is
\emph{acyclic}\index{acyclic}, or a \emph{forest}\index{forest}, if it
has no cycles. In a forest, a vertex of degree one is called an
\emph{endpoint}\index{endpoint} or a \emph{leaf}\index{leaf}. Any
vertex that is not a leaf is called an \emph{internal} vertex. A
connected forest is a \emph{tree}\index{tree}. In other words, a tree
is a graph without cycles and each edge is a bridge\index{bridge}.

A \emph{rooted tree}\index{tree!rooted} $T$ is a tree\index{tree} with
a specified \emph{root}\index{root} vertex $v_0$, i.e. the vertex
$v_0$ has been specially designated as the root of $T$. However, if
$G$ is a rooted tree with root vertex $v_0$ having degree one, then by
convention we do not call $v_0$ an endpoint or a leaf. The
\emph{depth} of a vertex in $T$ is its distance from the root. The
\emph{height} of $T$ is the length of a longest path starting from the
root vertex, i.e. the height is the maximum depth among all vertices
of $T$.

The Unix\index{Unix}, in particular Linux\index{Linux},
filesystem\index{filesystem} hierarchy can be viewed as a tree~(see
Figure~\ref{fig:trees_forests:filesystem_hierarchy}). As shown in
Figure~\ref{fig:trees_forests:filesystem_hierarchy}, the root
vertex\index{vertex!root} is designated with the forward slash, which
is also referred to as the root
directory\index{root directory}. Other examples of trees include the
organism classification\index{tree!classification} tree in
Figure~\ref{fig:trees_forests:classification_tree_organisms}, the
family\index{tree!family} tree in
Figure~\ref{fig:introduction:family_tree}, and the
expression\index{tree!expression} tree in
Figure~\ref{fig:trees_forests:expression_tree_perfect_square}.

A \emph{directed tree}\index{tree!directed} is a digraph which would
be a tree if the directions on the edges were ignored. A rooted tree
can be regarded as a directed tree since we can imagine an edge $uv$
for $u,v \in V$ being directed from $u$ to $v$ if and only if $v$ is
further away from $v_0$ than $u$ is. If $uv$ is an edge in a rooted
tree, then we call $v$ a \emph{child}\index{vertex!child} vertex with
\emph{parent}\index{vertex!parent} $u$. Directed trees are pervasive in
theoretical computer science, as they are useful structures for
describing algorithms and relationships between objects in certain
data sets.

\begin{figure}[!htbp]
\centering
\index{filesystem hierarchy}
\index{Linux}
\input{image/trees-forests/filesystem-hierarchy.tex}
\caption{The Linux filesystem hierarchy.}
\label{fig:trees_forests:filesystem_hierarchy}
\end{figure}

\begin{figure}[!htbp]
\centering
\index{tree!classification}
\input{image/trees-forests/classification-tree-organisms.tex}
\caption{Classification tree of organisms.}
\label{fig:trees_forests:classification_tree_organisms}
\end{figure}

\begin{figure}[!htbp]
\centering
\index{tree!expression}
\input{image/trees-forests/expression-tree-perfect-square.tex}
\caption{Expression tree for the perfect square $a^2 + 2ab + b^2$.}
\label{fig:trees_forests:expression_tree_perfect_square}
\end{figure}

An \emph{ordered tree}\index{tree!ordered} is a rooted tree for which
an ordering is specified for the children of each vertex. An
$n$-\emph{ary tree}\index{tree!$n$-ary} is a rooted tree for which
each vertex that is not a leaf has at most $n$ children. The case
$n = 2$ are called \emph{binary trees}\index{tree!binary}. An $n$-ary
tree is said to be \emph{complete} if each of its internal vertices
has exactly $n$ children and all leaves have the same depth. A
\emph{spanning tree}\index{spanning tree} of a connected, undirected
graph $G$ is a subgraph that is a tree and containing all vertices of
$G$.

\begin{example}
\label{eg:trees_forests:spanning_tree}
{\rm
Consider the $4 \times 4$ grid graph with $16$ vertices and
$24$ edges. Two examples of a spanning tree are given in
Figure~\ref{fig:trees_forests:grid_graph_spanning_trees} by using a
darker line shading for its edges.}
\qed
\end{example}

\begin{figure}[!htbp]
\centering
\input{image/trees-forests/grid-graph-spanning-trees.tex}
\caption{Two spanning trees for the $4 \times 4$ grid graph.}
\label{fig:trees_forests:grid_graph_spanning_trees}
\end{figure}

\begin{example}
For $n = 1, \dots, 6$, how many distinct (nonisomorphic) trees are
there of order $n$? Construct all such trees for each $n$.
\end{example}

\begin{proof}[Solution]
For $n = 1$, there is only one tree of order $1$, i.e. $K_1$. The same
is true for $n = 2$ and $n = 3$, where the required trees are $P_2$
and $P_3$, respectively (see
Figure~\ref{fig:trees_forests:distinct_trees_specified_order_1_2_3}). We
have two trees of order $n = 4$ (see
Figure~\ref{fig:trees_forests:distinct_trees_specified_order_4}),
three of order $n = 5$ (see
Figure~\ref{fig:trees_forests:distinct_trees_specified_order_5}), and
six of order $n = 6$ (see
Figure~\ref{fig:trees_forests:distinct_trees_specified_order_6}).
\end{proof}

\begin{figure}[!htbp]
\centering
\input{image/trees-forests/distinct-trees-specified-order-1-2-3.tex}
\caption{All distinct trees of order $n = 1, 2, 3$.}
\label{fig:trees_forests:distinct_trees_specified_order_1_2_3}
\end{figure}

\begin{figure}[!htbp]
\centering
\input{image/trees-forests/distinct-trees-specified-order-4.tex}
\caption{All distinct trees of order $n = 4$.}
\label{fig:trees_forests:distinct_trees_specified_order_4}
\end{figure}

\begin{figure}[!htbp]
\centering
\input{image/trees-forests/distinct-trees-specified-order-5.tex}
\caption{All distinct trees of order $n = 5$.}
\label{fig:trees_forests:distinct_trees_specified_order_5}
\end{figure}

\begin{figure}[!htbp]
\centering
\input{image/trees-forests/distinct-trees-specified-order-6.tex}
\caption{All distinct trees of order $n = 6$.}
\label{fig:trees_forests:distinct_trees_specified_order_6}
\end{figure}

\begin{example}
\label{eg:trees_forests:branch_cut_binary_tree}
Let $T = (V, E)$ be a tree with vertex set
\[
V
=
\{a, b, c, d, e, f, v, w, x, y, z\}
\]
edge set
\[
E
=
\{va,\, vw,\, wx,\, wy,\, xb,\, xc,\, yd,\, yz,\, ze,\, zf\}
\]
and root vertex $v$. Verify that $T$ is a binary tree. Suppose that
$x$ is the root of the branch we want to remove from $T$. Find all
children of $x$ and cut off the branch rooted at $x$ from $T$. Is the
resulting graph also a binary tree?
\end{example}

\begin{proof}[Solution]
We construct the tree $T$ in Sage as follows:
%%
\begin{lstlisting}
sage: T = DiGraph({
...   "v":["a","w"], "w":["x","y"],
...   "x":["c","b"], "y":["z","d"],
...   "z":["f","e"]})
sage: for v in T.vertex_iterator():
...       print(v),
a c b e d f w v y x z
sage: for e in T.edge_iterator():
...       print("%s%s" % (e[0], e[1])),
wy wx va vw yd yz xc xb ze zf
\end{lstlisting}
%%
Each vertex in a binary tree has at most $2$ children. Use this
definition to test whether or not a graph is a binary tree.
%%
\begin{lstlisting}
sage: T.is_tree()
True
sage: def is_bintree1(G):
...       for v in G.vertex_iterator():
...           if len(G.neighbors_out(v)) > 2:
...               return False
...       return True
sage: is_bintree1(T)
True
\end{lstlisting}
%%
Here's another way to test for binary trees. Let $T$ be an undirected
rooted tree. Each vertex in a binary tree has a maximum degree of
$3$. If the root vertex is the only vertex with degree $2$, then $T$
is a binary tree.
(Problem~\thechapter.\ref{prob:trees_forests:binary_tree_test} asks
you to prove this result.) We can use this test because the root
vertex $v$ of $T$ is the only vertex with two children.
%%
\begin{lstlisting}
sage: def is_bintree2(G):
...       if G.is_tree() and max(G.degree()) == 3 and G.degree().count(2) == 1:
...           return True
...       return False
sage: is_bintree2(T.to_undirected())
True
\end{lstlisting}
%%
As $x$ is the root vertex of the branch we want to cut off from $T$,
we could use breadth- or depth-first search to determine all the
children of $x$. We then delete $x$ and its children from $T$.
%%
\begin{lstlisting}
sage: T2 = copy(T)
sage: # using breadth-first search
sage: V = list(T.breadth_first_search("x")); V
['x', 'c', 'b']
sage: T.delete_vertices(V)
sage: for v in T.vertex_iterator():
...       print(v),
a e d f w v y z
sage: for e in T.edge_iterator():
...       print("%s%s" % (e[0], e[1])),
wy va vw yd yz ze zf
sage: # using depth-first search
sage: V = list(T2.depth_first_search("x")); V
['x', 'b', 'c']
sage: T2.delete_vertices(V)
sage: for v in T2.vertex_iterator():
...       print(v),
a e d f w v y z
sage: for e in T2.edge_iterator():
...       print("%s%s" % (e[0], e[1])),
wy va vw yd yz ze zf
\end{lstlisting}
%%
The resulting graph $T$ is a binary tree because each vertex has at
most two children.
%%
\begin{lstlisting}
sage: T
Digraph on 8 vertices
sage: is_bintree1(T)
True
\end{lstlisting}
%%
Notice that the test defined in the function \verb!is_bintree2! can no
longer be used to test whether or not $T$ is a binary tree, because
$T$ now has two vertices, i.e. $v$ and $w$, each of which has degree $2$.
\end{proof}

Consider again the organism classification tree in
Figure~\ref{fig:trees_forests:classification_tree_organisms}. We can
view the vertex ``organism'' as the root of the tree and having two
children. The first branch of ``organism'' is the subtree rooted at
``plant'' and its second branch is the subtree rooted at
``animal''. We form the complete tree by joining an edge between
``organism'' and ``plant'', and an edge between ``organism'' and
``animal''. The subtree rooted at ``plant'' can be constructed in the
same manner. The first branch of this subtree is the subtree rooted at
``tree'' and the second branch is the subtree rooted at ``flower''. To
construct the subtree rooted at ``plant'', we join an edge between
``plant'' and ``tree'', and an edge between ``plant'' and
``flower''. The other subtrees of the tree in
Figure~\ref{fig:trees_forests:classification_tree_organisms} can be
constructed using the above recursive procedure.

In general, the recursive construction in
Theorem~\ref{thm:trees_forests:recursive_construction_trees} provides
an alternative way to define trees. We say \emph{construction} because
it provides an algorithm to contruct a tree, as opposed to the
nonconstructive definition presented earlier in this section, where we
defined the conditions under which a graph qualifies as a tree without
presenting a procedure to construct a tree. Furthermore, we say
\emph{recursive} since a larger tree can be viewed as being
constructed from smaller trees, i.e. join up existing trees to obtain
a new tree. The recursive construction of trees as presented in
Theorem~\ref{thm:trees_forests:recursive_construction_trees} is
illustrated in
Figure~\ref{fig:trees_forests:recursive_construction_tree}.

\begin{theorem}
\label{thm:trees_forests:recursive_construction_trees}
\textbf{Recursive construction of trees.}
An isolated vertex is a tree. That single vertex is the root of the
tree. Given a collection $T_1, T_2, \dots, T_n$ of $n > 0$ trees,
construct a new tree as follows:
%%
\begin{enumerate}
\item Let $T$ be a tree having exactly the one vertex $v$, which is
  the root of $T$.

\item Let $v_i$ be the root of the tree $T_i$.

\item For $i = 1, 2, \dots, n$, add the edge $v v_i$ to $T$ and add
  $T_i$ to $T$. That is, each $v_i$ is now a child of $v$.
\end{enumerate}
%%
The result is the tree $T$ rooted at $v$ with vertex set
\[
V(T)
=
\{v\} \cup \left(\bigcup_i V(T_i)\right)
\]
and edge set
\[
E(T)
=
\bigcup_i \big(\{v v_i\} \cup E(T_i)\big).
\]
\end{theorem}

\begin{figure}[!htbp]
\centering
\input{image/trees-forests/recursive-definition-tree.tex}
\caption{Recursive construction of a tree.}
\label{fig:trees_forests:recursive_construction_tree}
\end{figure}

The following game is a variant of the Shannon
switching\index{Shannon!switching game} game, due to Edmonds and
Lehman\index{edge-tagging game}. We follow the description
in Oxley's\index{Oxley, James} survey~\cite{Oxley2003}. Recall that a
minimal edge cut of a graph is also called a bond\index{bond} of the
graph. The following two-person game is played on a connected graph
$G = (V,E)$. Two players Alice and Bob alternately tag elements of
$E$. Alice's goal is to tag the edges of a spanning tree, while Bob's
goal is to tag the edges of a bond. If we think of this game in terms
of a communication network, then Bob's goal is to separate the network
into pieces that are no longer connected to each other, while Alice is
aiming to reinforce edges of the network to prevent their
destruction. Each move for Bob consists of destroying one edge, while
each move for Alice involves securing an edge against destruction. The
next result characterizes winning strategies on $G$. The full proof
can be found in Oxley\index{Oxley, James}~\cite{Oxley2003}. See
Rasmussen~\cite{Rasmussen2007}\index{Rasmussen, Rune} for
optimization\index{algorithm!optimization} algorithms for solving
similar games.

\begin{theorem}
The following statements are equivalent for a connected graph
$G = (V, E)$.
%%
\begin{enumerate}
\item Bob plays first and Alice can win against all possible
  strategies of Bob.

\item The graph $G$ has $2$ edge-disjoint spanning trees.

\item For all partitions $P$ of the vertex set $V$, the number of
  edges of $G$ that join vertices in different classes of the
  partition is at least $2(|P| - 1)$.
\end{enumerate}
\end{theorem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Properties of trees}

By Theorem~\ref{thm:inroduction:edge_is_bridge_iff_edge_not_on_cycle},
each edge of a tree is a bridge. Removing any edge of a tree
partitions the tree into two components, each of which is a subtree of
the original tree. The following results provide further basic
characterizations of trees.

\begin{theorem}
\label{thm:trees_forests:each_tree_has_size_n_minus_one}
Any tree $T = (V,E)$ has size $|E| = |V| - 1$.
\end{theorem}

\begin{proof}
This follows by induction on the number of vertices. By definition, a
tree has no cycles. We need to show that any tree $T = (V,E)$ has size
$|E| = |V| - 1$. For the base case $|V| = 1$, there are no
edges. Assume for induction that the result holds for all integers
less than or equal to $k \geq 2$. Let $T = (V,E)$ be a tree having
$k + 1$ vertices. Remove an edge from $T$, but not the vertices it is
incident to. This disconnects $T$ into two components
$T_1 = (V_1, E_1)$ and $T_2 = (V_2, E_2)$, where
$|E| = |E_1| + |E_2| + 1$ and $|V| = |V_1| + |V_2|$ (and possibly one
of the $E_i$ is empty). Each $T_i$ is a tree satisfying the conditions
of the induction hypothesis. Therefore,
%%
\begin{align*}
|E|
&=
|E_1| + |E_2| + 1 \\[4pt]
&=
|V_1| - 1 + |V_2| - 1 + 1 \\[4pt]
&=
|V| - 1.
\end{align*}
%%
as required.
\end{proof}

\begin{corollary}
If $T = (V,E)$ is a graph of order $|V| = n$, then the following are
equivalent:
%%
\begin{enumerate}
\item\label{enu:trees_forests:is_tree} $T$ is a tree.

\item\label{enu:trees_forests:no_cycles_n_minus_one_edges} $T$
  contains no cycles and has $n - 1$ edges.

\item\label{enu:trees_forests:connected_n_minus_one_edges} $T$ is
  connected and has $n - 1$ edges.

\item\label{enu:trees_forests:each_edge_is_cut_set} Every edge of $T$
  is a cut set.
\end{enumerate}
\end{corollary}

\begin{proof}
(\ref{enu:trees_forests:is_tree}) $\implies$
  (\ref{enu:trees_forests:no_cycles_n_minus_one_edges}):
This holds by definition of trees and
Theorem~\ref{thm:trees_forests:each_tree_has_size_n_minus_one}.

(\ref{enu:trees_forests:no_cycles_n_minus_one_edges}) $\implies$
(\ref{enu:trees_forests:connected_n_minus_one_edges}):
If $T = (V,E)$ has $k$ connected components then it is a disjoint
union of trees $T_i = (V_i, E_i)$, $i = 1, 2, \dots, k$, for some
$k$. By part~(\ref{enu:trees_forests:no_cycles_n_minus_one_edges}),
each of these satisfy
\[
|E_i|
=
|V_i| - 1
\]
so
%%
\begin{align*}
|E|
&=
\sum_{i=1}^k |E_i| \\[4pt]
&=
\sum_{i=1}^k |V_i| - k \\[4pt]
&=
|V| - k.
\end{align*}
%%
This contradicts
part~(\ref{enu:trees_forests:no_cycles_n_minus_one_edges}) unless
$k = 1$. Therefore, $T$ is connected.

(\ref{enu:trees_forests:connected_n_minus_one_edges}) $\implies$
(\ref{enu:trees_forests:each_edge_is_cut_set}):
If removing an edge $e \in E$ leaves $T = (V,E)$ connected then
$T' = (V, E')$ is a tree, where $E' = E - e$. However, this means that
$|E'| = |E| - 1 = |V| - 1 - 1 = |V| - 2$, which contradicts
part~(\ref{enu:trees_forests:connected_n_minus_one_edges}). Therefore
$e$ is a cut set.

(\ref{enu:trees_forests:each_edge_is_cut_set}) $\implies$
(\ref{enu:trees_forests:is_tree}):
From part~(\ref{enu:trees_forests:no_cycles_n_minus_one_edges}) we
know that $T$ has no cycles and from
part~(\ref{enu:trees_forests:connected_n_minus_one_edges}) we know
that $T$ is connected. Conclude by the definition of trees that $T$ is
a tree.
\end{proof}

\begin{theorem}
\label{thm:trees_forests:tree_has_one_u_v_path}
Let $T = (V,E)$ be a tree and let $u,v \in V$ be distinct
vertices. Then $T$ has exactly one $u$-$v$ path.
\end{theorem}

\begin{proof}
Suppose for contradiction that
\[
P : v_0 = u,\, v_1,\, v_2, \dots, v_k = v
\]
and
\[
Q : w_0 = u,\, w_1,\, w_2, \dots, w_\ell = v
\]
are two distinct $u$-$v$ paths. Then $P$ and $Q$ has a common vertex
$x$, which is possibly $x = u$. For some $i \geq 0$ and some
$j \geq 0$ we have $v_i = x = w_j$, but $v_{i+1} \neq w_{j+1}$. Let
$y$ be the first vertex after $x$ such that $y$ belongs to both $P$
and $Q$. (It is possible that $y = v$.) We now have two distinct
$x$-$y$ paths that have only $x$ and $y$ in common. Taken together,
these two $x$-$y$ paths result in a cycle, contradicting our
hypothesis that $T$ is a tree. Therefore $T$ has only one $u$-$v$ path.
\end{proof}

\begin{theorem}
If $T = (V,E)$ is a graph then the following are equivalent:
%%
\begin{enumerate}
\item $T$ is a tree.

\item For any new edge $e$, the join $T + e$ has exactly one cycle.
\end{enumerate}
\end{theorem}

\begin{proof}
(1) $\implies$ (2):
Let $e = uv$ be a new edge connecting $u,v \in V$. Suppose that
\[
P : v_0 = w,\, v_1,\, v_2, \dots, v_k = w
\]
and
\[
P' : v'_0 = w,\, v'_1,\, v'_2, \dots, v'_\ell = w
\]
are two cycles in $T + e$. If either $P$ or $P'$ does not contain $e$,
say $P$ does not contain $e$, then $P$ is a cycle in $T$. Let
$u = v_0$ and let $v = v_1$. The edge $(v_0 = w,\, v_1)$ is a $u$-$v$
path and the sequence $v = v_1,\, v_2, \dots, v_k = w = u$ taken in
reverse order is another $u$-$v$ path. This contradicts
Theorem~\ref{thm:trees_forests:tree_has_one_u_v_path}.

We may now suppose that $P$ and $P'$ both contain $e$. Then $P$
contains a subpath $P_0 = P - e$ (which is not closed) that is the
same as $P$ except it lacks the edge from $u$ to $v$. Likewise, $P'$
contains a subpath $P'_0=P'-e$ (which is not closed) that is the same
as $P'$ except it lacks the edge from $u$ to $v$. By
Theorem~\ref{thm:trees_forests:tree_has_one_u_v_path}, these $u$-$v$
paths $P_0$ and $P_0'$ must be the same. This forces $P$ and $P'$ to
be the same, which proves part~(2).

(2) $\implies$ (1):
Part~(2) implies that $T$ is acyclic. (Otherwise, it is trivial
to make two cycles by adding an extra edge.) We must show $T$ is
connected. Suppose $T$ is disconnected. Let $u$ be a vertex in one
component, $T_1$ say, of $T$ and $v$ a vertex in another component,
$T_2$ say, of $T$. Adding the edge $e = uv$ does not create a cycle
(if it did then $T_1$ and $T_2$ would not be disjoint), which
contradicts part~(2).
\end{proof}

Taking together the results in this section, we have the following
characterizations of trees.

\begin{theorem}
\label{thm:trees_forests:basic_characterizations_of_trees}
\textbf{Basic characterizations of trees.}
If $T = (V, E)$ is a graph with $n$ vertices, then the following
statements are equivalent:
%%
\begin{enumerate}
\item $T$ is a tree.

\item $T$ contains no cycles and has $n - 1$ edges.

\item $T$ is connected and has $n - 1$ edges.

\item Every edge of $T$ is a cut set.

\item For any pair of distinct vertices $u,v \in V$, there is exactly
  one $u$-$v$ path.

\item For any new edge $e$, the join $T + e$ has exactly one cycle.
\end{enumerate}
\end{theorem}

Let $G = (V_1, E_1)$ be a graph and $T = (V_2, E_2)$ a subgraph of $G$
that is a tree\index{tree}. As in part~(6) of
Theorem~\ref{thm:trees_forests:basic_characterizations_of_trees}, we
see that adding just one edge in $E_1 - E_2$ to $T$ will create a
unique cycle in $G$. Such a cycle is called a
\emph{fundamental cycle}\index{fundamental cycle} of $G$. The set of
such fundamental cycles of $G$ depends on $T$.

The following result essentially says that if a tree has at least one
edge, then the tree has at least two vertices each of which has degree
one. In other words, each tree of order $\geq 2$ has at least two
pendants\index{pendant}.

\begin{theorem}
Every nontrivial tree has at least two leaves.
\end{theorem}

\begin{proof}
Let $T$ be a nontrivial tree of order $m$ and size $n$. Consider the
degree sequence $d_1, d_2, \dots, d_m$ of $T$ where
$d_1 \leq d_2 \leq \cdots \leq d_m$. As $T$ is nontrivial and
connected, then $m \geq 2$ and $d_i \geq 1$ for $i = 1, 2, \dots, m$.
If $T$ has less than two leaves, then $d_1 \geq 1$ and $d_i \geq 2$
for $2 \leq i \leq m$, hence
%%
\begin{equation}
\label{eqn:trees_forests:less_than_two_leaves_lower_bound_degree_sum}
\sum_{i=1}^m d_i
\geq
1 + 2(m-1)
=
2m - 1.
\end{equation}
%%
But by Theorems~\ref{thm:introduction:degree_sum}
and~\ref{thm:trees_forests:each_tree_has_size_n_minus_one}, we have
\[
\sum_{i=1}^m d_i
=
2n
=
2(m - 1)
=
2m - 2
\]
which contradicts
inequality~\eqref{eqn:trees_forests:less_than_two_leaves_lower_bound_degree_sum}.
Conclude that $T$ has at least two leaves.
\end{proof}

\begin{theorem}
If $T$ is a tree of order $m$ and $G$ is a graph with minimum degree
$\delta(G) \geq m - 1$, then $T$ is isomorphic to a subgraph of $G$.
\end{theorem}

\begin{proof}
Use an inductive argument on the number of vertices. The result holds
for $m = 1$ because $K_1$ is a subgraph of every nontrivial graph. The
result also holds for $m = 2$ since $K_2$ is a subgraph of any graph
with at least one edge.

Let $m \geq 3$, let $T_1$ be a tree of order $m - 1$, and let $H$ be a
graph with $\delta(H) \geq m - 2$. Assume for induction that $T_1$ is
isomorphic to a subgraph of $H$. We need to show that if $T$ is a tree
of order $m$ and $G$ is a graph with $\delta(G) \geq m - 1$, then $T$
is isomorphic to a subgraph of $G$. Towards that end, consider a leaf
$v$ of $T$ and let $u$ be a vertex of $T$ such that $u$ is adjacent to
$v$. Then $T - v$ is a tree of order $m - 1$ and
$\delta(G) \geq m - 1 > m - 2$. Apply the inductive hypothesis to see
that $T - v$ is isomorphic to a subgraph $T'$ of $G$. Let $u'$ be the
vertex of $T'$ that corresponds to the vertex $u$ of $T$ under an
isomorphism. Since $\deg(u') \geq m - 1$ and $T'$ has $m - 2$ vertices
distinct from $u'$, it follows that $u'$ is adjacent to some
$w \in V(G)$ such that $w \notin V(T')$. Therefore $T$ is isomorphic
to the graph obtained by adding the edge $u'w$ to $T'$.
\end{proof}

\begin{example}
\label{eg:trees_forests:Euler_phi_function_tree}
Consider a positive integer $n$. The Euler phi function $\varphi(n)$
counts the number of integers $a$, with $1 \leq a \leq n$, such that
$\gcd(a,n) = 1$. The Euler phi sequence of $n$ is obtained by
repeatedly iterating $\varphi(n)$ with initial iteration value
$n$. Continue on iterating and stop when the output of
$\varphi(\alpha_k)$ is $1$, for some positive integer $\alpha_k$. The
number of terms generated by the iteration, including the initial
iteration value $n$ and the final value of $1$, is the length of
$\varphi(n)$.
%%
\begin{enumerate}[(a)]
\item Let $s_0=n, s_1, s_2, \dots, s_k=1$ be the Euler phi sequence of
  $n$ and produce a digraph $G$ of this sequence as follows. The
  vertex set of $G$ is $V = \{s_0=n, s_1, s_2, \dots, s_k=1\}$ and the
  edge set of $G$ is $E = \{s_i s_{i+1} \mid 0 \leq i < k\}$. Produce
  the digraphs of the Euler phi sequences of $15$, $22$, $33$, $35$,
  $69$, and $72$. Construct the union of all such digraphs and
  describe the resulting graph structure.

\item For each $n = 1, 2, \dots, 1000$, compute the length of
  $\varphi(n)$ and plot the pairs $(n, \varphi(n))$ on one set of axes.
\end{enumerate}
\end{example}

\begin{proof}[Solution]
The Euler phi sequence of $15$ is
\[
15,\quad
\varphi(15) = 8,\quad
\varphi(8) = 4,\quad
\varphi(4) = 2,\quad
\varphi(2) = 1.
\]
The Euler phi sequences of $22$, $33$, $35$, $69$, and $72$ can be
similarly computed to obtain their respective digraph
representations. The union of all such digraphs is a directed tree
rooted at $1$, as shown in
Figure~\ref{fig:trees_forests:union_digraphs_Euler_phi_sequences}.
Figure~\ref{fig:trees_forests:plot_length_Euler_phi} shows a
scatterplot of $n$ versus the length of $\varphi(n)$.
\end{proof}

\begin{figure}[!htbp]
\centering
%%
%%
\subfigure[]{
\label{fig:trees_forests:union_digraphs_Euler_phi_sequences}
\input{image/trees-forests/union-digraphs-Euler-phi-sequences.tex}
}
\quad
%%
%%
\subfigure[]{
\label{fig:trees_forests:plot_length_Euler_phi}
\input{image/trees-forests/phi-length.tex}
}
\caption{Union of digraphs of Euler phi sequences and scatterplot.}
\label{fig:trees_forests:union_digraphs_scatterplot}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Minimum spanning trees}

Suppose we want to design an electronic circuit connecting several
components. If these components represent the vertices of a graph and
a wire connecting two components represents an edge of the graph, then
for economical reasons we will want to connect the components together
using the least amount of wire. The problem essentially amounts to
finding a minimum spanning tree in the graph containing these vertices.

But what is a spanning tree? We can characterize a spanning tree in
several ways, each leading to an algorithm for constructing a spanning
tree. Let $G$ be a connected graph and let $T$ be a subgraph of
$G$. If $T$ is a tree that contains all the vertices of $G$, then $T$
is called a \emph{spanning tree} of $G$. We can think of $T$ as a tree
that is also an edge-deletion subgraph of $G$. That is, we start with
a connected graph $G$ and delete an edge from $G$ such that the
resulting edge-deletion subgraph $T_1$ is still connected. If $T_1$ is
a tree, then we have obtained a spanning tree of $G$. Otherwise, we
delete an edge from $T_1$ to obtain an edge-deletion subgraph $T_2$
that is still connected. If $T_2$ is a tree, then we are
done. Otherwise, we repeat the above procedure until we obtain an
edge-deletion subgraph $T_k$ of $G$ such that $T_k$ is connected,
$T_k$ is a tree, and it contains all vertices of $G$. Each edge
removal does not decrease the number of vertices and must also leave
the resulting edge-deletion subgraph connected. Thus eventually the
above procedure results in a spanning tree of $G$. Our discussion is
summarized in
Algorithm~\ref{alg:trees_forests:randomized_spanning_tree_construction}.

\begin{algorithm}[!htbp]
\input{algorithm/trees-forests/randomized-spanning-tree.tex}
\caption{Randomized spanning tree construction.}
\label{alg:trees_forests:randomized_spanning_tree_construction}
\end{algorithm}

Another characterization of a spanning tree $T$ of a connected graph
$G$ is that $T$ is a maximal set of edges of $G$ that contains no
cycle. Kruskal's algorithm~(see
section~\ref{subsec:trees_forests:Kruskal_algorithm}) exploits this
condition to construct a minimum spanning tree~(MST)\index{MST}. A
\emph{minimum spanning tree}\index{tree!minimum spanning} is a
spanning tree of a weighted graph having lowest total weight among all
possible spanning trees of the graph. A third characterization of a
spanning tree is that it is a minimal set of edges that connect all
vertices, a characterization that results in yet another algorithm
called Prim's algorithm~(see
section~\ref{subsec:trees_forests:Prim_algorithm}) for constructing
minimum spanning trees. The task of determining a minimum spanning
tree in a connected weighted graph is called the
\emph{minimum spanning tree problem}. As early as 1926, Otakar
Bor\r{u}vka stated~\cite{Boruvka1926a,Boruvka1926b} this problem and
offered a solution now known as Bor\r{u}vka's algorithm~(see
section~\ref{subsec:trees_forests:Boruvka_algorithm}). See Graham and
Hell~\cite{GrahamHell1985} for a history of the minimum spanning tree
problem.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Kruskal's algorithm}
\label{subsec:trees_forests:Kruskal_algorithm}

In 1956, Joseph B. Kruskal published~\cite{Kruskal1956} a procedure
for constructing a minimum spanning tree of a connected weighted
graph $G = (V,E)$. Now known as Kruskal's algorithm, with a suitable
implementation the procedure runs in $O\big(|E| \cdot \log |E|\big)$
time. Variants of Kruskal's algorithm include the algorithm by
Prim~\cite{Prim1957} and that by Loberman and
Weinberger~\cite{LobermanWeinberger1957}.

Kruskal's algorithm belongs to the class of greedy algorithms. As will
be explained below, when constructing a minimum spanning tree
Kruskal's algorithm considers only the edge having minimum weight
among all available edges. Given a weighted nontrivial graph
$G = (V,E)$ that is connected, let $w: E \to \R$ be the weight
function of $G$. The first stage is creating a ``skeleton'' of the
tree $T$ that is initially set to be a graph with no edges, i.e.
$T = (V, \emptyset)$. The next stage involves sorting the edges of $G$
by weights in nondecreasing order. In other words, we label the edges
of $G$ as follows:
\[
E
=
\{e_1, e_2, \dots, e_n\}
\]
where $n = |E|$ and $w(e_1) \leq w(e_2) \leq \cdots \leq w(e_n)$.
Now consider each edge $e_i$ for $i = 1,2,\dots,n$. We add $e_i$ to
the edge set of $T$ provided that $e_i$ does not result in $T$ having
a cycle. The only way adding $e_i = u_i v_i$ to $T$ would create a
cycle is if both $u_i$ and $v_i$ were endpoints of edges~(not
necessarily distinct) in the same connected component of $T$. As long
as the acyclic condition holds with the addition of a new edge to $T$,
we add that new edge. Following the acyclic test, we also test that
the (updated) graph $T$ is a tree of $G$. As $G$ is a graph of order
$|V|$, apply
Theorem~\ref{thm:trees_forests:basic_characterizations_of_trees} to
see that if $T$ has size $|V| - 1$, then it is a spanning tree of
$G$. Algorithm~\ref{alg:trees_forests:Kruskal_algorithm} provides
pseudocode of our discussion of Kruskal's algorithm. When the
algorithm halts, it returns a minimum spanning tree of $G$. The
correctness of Algorithm~\ref{alg:trees_forests:Kruskal_algorithm} is
proven in Theorem~\ref{thm:trees_forests:correctness_Kruskal_algorithm}.

\begin{algorithm}[!htbp]
\input{algorithm/trees-forests/Kruskal-algorithm.tex}
\caption{Kruskal's algorithm.}
\label{alg:trees_forests:Kruskal_algorithm}
\end{algorithm}

\begin{theorem}
\label{thm:trees_forests:correctness_Kruskal_algorithm}
\textbf{Correctness of Algorithm~\ref{alg:trees_forests:Kruskal_algorithm}.}
If $G$ is a nontrivial connected weighted graph, then
Algorithm~\ref{alg:trees_forests:Kruskal_algorithm} outputs a minimum
spanning tree of $G$.
\end{theorem}

\begin{proof}
Let $G$ be a nontrivial connected graph of order $m$ and having weight
function $w$. Let $T$ be a subgraph of $G$ produced by Kruskal's
algorithm~\ref{alg:trees_forests:Kruskal_algorithm}. By construction,
$T$ is a spanning tree of $G$ with
\[
E(T)
=
\{e_1, e_2, \dots, e_{m-1}\}
\]
where $w(e_1) \leq w(e_2) \leq \cdots \leq w(e_{m-1})$ so that the
total weight of $T$ is
\[
w(T)
=
\sum_{i=1}^{m-1} w(e_i).
\]
Suppose for contradiction that $T$ is not a minimum spanning tree of
$G$. Among all the minimum spanning trees of $G$, let $H$ be a minimum
spanning tree of $G$ such that $H$ has the most number of edges in
common with $T$. As $T$ and $H$ are distinct subgraphs of $G$, then
$T$ has at least an edge not belonging to $H$. Let $e_i \in E(T)$ be
the first edge not in $H$. Construct the graph $G_0 = H + e_i$
obtained by adding the edge $e_i$ to $H$. Note that $G_0$ has exactly
one cycle $C$. Since $T$ is acyclic, there exists an edge
$e_0 \in E(C)$ such that $e_0$ is not in $T$. Construct the graph
$T_0 = G_0 - e_0$ obtained by deleting the edge $e_0$ from $G_0$. Then
$T_0$ is a spanning tree of $G$ with
\[
w(T_0)
=
w(H) + w(e_i) - w(e_0)
\]
and $w(H) \leq w(T_0)$ and hence $w(e_0) \leq w(e_i)$. By Kruskal's
algorithm~\ref{alg:trees_forests:Kruskal_algorithm}, $e_i$ is an edge
of minimum weight such that
$\{e_1, e_2, \dots, e_{i-1}\} \cup \{e_i\}$ is acyclic. Furthermore,
the subgraph $\{e_1, e_2, \dots, e_{i-1}, e_0\}$ of $H$ is
acyclic. Thus we have $w(e_i) = w(e_0)$ and $w(T_0) = w(H)$ and so $T$
is a minimum spanning tree of $G$. By construction, $T_0$ has more
edges in common with $T$ than $H$ has with $T$, in contradiction of
our hypothesis.
\end{proof}

\begin{lstlisting}
def kruskal(G):
    """
    Implements Kruskal's algorithm to compute a MST of a graph.

    INPUT:
        G - a connected edge-weighted graph or digraph
               whose vertices are assumed to be 0, 1, ...., n-1.
    OUTPUT:
        T - a minimum weight spanning tree.

    If G is not explicitly edge-weighted then the algorithm
    assumes all edge weights are 1. The tree T returned is
    a weighted graph, even if G is not.

    EXAMPLES:
        sage: A = matrix([[0,1,2,3],[0,0,2,1],[0,0,0,3],[0,0,0,0]])
        sage: G = DiGraph(A, format = "adjacency_matrix", weighted = True)
        sage: TE = kruskal(G); TE.edges()
        [(0, 1, 1), (0, 2, 2), (1, 3, 1)]
        sage: G.edges()
        [(0, 1, 1), (0, 2, 2), (0, 3, 3), (1, 2, 2), (1, 3, 1), (2, 3, 3)]
        sage: G = graphs.PetersenGraph()
        sage: TE = kruskal(G); TE.edges()
        [(0, 1, 1), (0, 4, 1), (0, 5, 1), (1, 2, 1), (1, 6, 1), (2, 3, 1),
         (2, 7, 1), (3, 8, 1), (4, 9, 1)]

    TODO:
        Add ''verbose'' option to make steps more transparent.
       (Useful for teachers and students.)
    """
    T_vertices = G.vertices() # a list of the form range(n)
    T_edges = []
    E = G.edges() # a list of triples
    # start ugly hack
    Er = [list(x) for x in E]
    E0 = []
    for x in Er:
        x.reverse()
        E0.append(x)
    E0.sort()
    E = []
    for x in E0:
        x.reverse()
        E.append(tuple(x))
    # end ugly hack to get E is sorted by weight
    for x in E:  # find edges of T
        TV = flatten(T_edges)
        u = x[0]
        v = x[1]
        if not(u in TV and v in TV):
            T_edges.append([u,v])
    # find adj mat of T
    if G.weighted():
        AG = G.weighted_adjacency_matrix()
    else:
        AG = G.adjacency_matrix()
    GV = G.vertices()
    n = len(GV)
    AT = []
    for i in GV:
        rw = [0]*n
        for j in GV:
            if [i,j] in T_edges:
                rw[j] = AG[i][j]
        AT.append(rw)
    AT = matrix(AT)
    return Graph(AT, format = "adjacency_matrix", weighted = True)
\end{lstlisting}
\index{Kruskal's algorithm}

Here is an example. We start with the grid graph. This is implemented
in Sage such that the vertices are given by the coordinates of the
grid the graph lies on, as opposed to $0, 1, \dots, n-1$. Since the
above implementation of Kruskal's algorithm assumes that the vertices
are $V = \{0, 1, \dots, n-1\}$, we first redefine the graph suitable
for running Kruskal's algorithm on it.
%%
\begin{lstlisting}
sage: G = graphs.GridGraph([4,4])
sage: A = G.adjacency_matrix()
sage: G = Graph(A, format="adjacency_matrix", weighted=True)
sage: T = kruskal(G); T.edges()
[(0, 1, 1), (0, 4, 1), (1, 2, 1), (1, 5, 1), (2, 3, 1), (2, 6, 1), (3,7, 1),
 (4, 8, 1), (5, 9, 1), (6, 10, 1), (7, 11, 1), (8, 12, 1), (9, 13, 1),
 (10, 14, 1), (11, 15, 1)]
\end{lstlisting}
%%
An illustration of this graph is given in
Figure~\ref{fig:trees-forests:Kruskal_example}.

\begin{figure}[!htbp]
\centering
\input{image/trees-forests/Kruskal-example.tex}
\caption{Kruskal's algorithm for the $4\times 4$ grid graph.}
\label{fig:trees-forests:Kruskal_example}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Prim's algorithm}
\label{subsec:trees_forests:Prim_algorithm}

Like Kruskal's algorithm, Prim's algorithm uses a greedy approach to
computing a minimum spanning tree of a connected weighted graph
$G = (V,E)$, where $n = |V|$ and $m = |E|$. The algorithm was
developed in 1930 by Czech mathematician V.~Jarn\'ik~\cite{Jarnik1930}
and later independently by R.~C.~Prim~\cite{Prim1957} and
E.~W.~Dijkstra~\cite{Dijkstra1959}. However, Prim was the first to
present an implementation that runs in time $O(n^2)$. Using $2$-heaps,
the runtime can be reduced~\cite{KershenbaumVanSlyke1972} to
$O(m \cdot \log n)$. With a Fibonacci heap
implementation~\cite{FredmanTarjan1984,FredmanTarjan1987}, the runtime
can be reduced even further to $O(m + n \cdot \log n)$.

Pseudocode of Prim's algorithm is given in
Algorithm~\ref{alg:trees_forests:prim}. For each $v \in V$, $\cost[v]$
denotes the minimum weight among all edges connecting $v$ to a vertex
in the tree $T$, and $\parent[v]$ denotes the parent of $v$ in
$T$. During the algorithm's execution, vertices $v$ that are not in
$T$ are organized in the minimum-priority queue $Q$, prioritized
according to $\cost[v]$. Lines~\ref{alg:prim:for_start:init}
to~\ref{alg:prim:for_end:init} set each $\cost[v]$ to a number that
is larger than any weight in the graph $G$, usually written $\infty$.
The parent of each vertex is set to \texttt{NULL} because we have not
yet started constructing the MST $T$. In
lines~\ref{alg:prim:arbitrary_root}
to~\ref{alg:prim:init_min_priority_queue}, we choose an arbitrary
vertex $r$ from $V$ and mark that vertex as the root of $T$. The
minimum-priority queue is set to be all vertices from $V$. We set
$\cost[r]$ to zero, making $r$ the only vertex so far with a cost that
is $< \infty$. During the first execution of the while loop from
lines~\ref{alg:prim:while_start:build_tree}
to~\ref{alg:prim:while_end:build_tree}, $r$ is the first vertex to be
extracted from $Q$ and processed. Line~\ref{alg:prim:extract_min}
extracts a vertex $u$ from $Q$ based on the key $\cost$, thus moving
$u$ to the vertex set of $T$. Line~\ref{alg:prim:neighbors_u}
considers all vertices adjacent to $u$. In an undirected graph, these
are the neighbors of $u$; in a digraph, we replace $\adj(u)$ with the
out-neighbors $\oadj(u)$. The while loop updates the $\cost$ and
$\parent$ fields of each vertex $v$ adjacent to $u$ that is not in
$T$. If $\parent[v] \neq \texttt{NULL}$, then $\cost[v] < \infty$ and
$\cost[v]$ is the weight of an edge connecting $v$ to some vertex
already in $T$. Lines~\ref{alg:prim:MST_edge_set}
to~\ref{alg:prim:return_MST} construct the edge set of the minimum
spanning tree and return this edge set. The proof of correctness of
Algorithm~\ref{alg:trees_forests:prim} is similar to the proof of
Theorem~\ref{thm:trees_forests:correctness_Kruskal_algorithm}.
Figure~\ref{fig:tree_forests:Prim_algorithm_digraph} shows the minimum
spanning tree rooted at vertex $1$ as a result of running Prim's
algorithm over a digraph;
Figure~\ref{fig:tree_forests:Prim_algorithm_undirected_graph} shows
the corresponding tree rooted at vertex $5$ of an undirected graph.

\begin{algorithm}[!htbp]
\input{algorithm/trees-forests/Prim-algorithm.tex}
\caption{Prim's algorithm.}
\label{alg:trees_forests:prim}
\index{Prim's algorithm}
\end{algorithm}

\begin{lstlisting}
def prim(G):
    """
    Implements Prim's algorithm to compute a MST of a graph.

    INPUT:
        G - a connected graph.
    OUTPUT:
        T - a minimum weight spanning tree.

    REFERENCES:
        http://en.wikipedia.org/wiki/Prim's_algorithm
    """
    T_vertices = [0] # assumes G.vertices = range(n)
    T_edges = []
    E = G.edges() # a list of triples
    V = G.vertices()
    # start ugly hack to sort E
    Er = [list(x) for x in E]
    E0 = []
    for x in Er:
        x.reverse()
        E0.append(x)
    E0.sort()
    E = []
    for x in E0:
        x.reverse()
        E.append(tuple(x))
    # end ugly hack to get E is sorted by weight
    for x in E:
        u = x[0]
        v = x[1]
        if u in T_vertices and not(v in T_vertices):
            T_edges.append([u,v])
            T_vertices.append(v)
    # found T_vertices, T_edges
    # find adj mat of T
    if G.weighted():
        AG = G.weighted_adjacency_matrix()
    else:
        AG = G.adjacency_matrix()
    GV = G.vertices()
    n = len(GV)
    AT = []
    for i in GV:
        rw = [0]*n
        for j in GV:
            if [i,j] in T_edges:
                rw[j] = AG[i][j]
        AT.append(rw)
    AT = matrix(AT)
    return Graph(AT, format = "adjacency_matrix", weighted = True)
\end{lstlisting}

\begin{lstlisting}
sage: A = matrix([[0,1,2,3], [3,0,2,1], [2,1,0,3], [1,1,1,0]])
sage: G = DiGraph(A, format="adjacency_matrix", weighted=True)
sage: E = G.edges(); E
[(0, 1, 1), (0, 2, 2), (0, 3, 3), (1, 0, 3), (1, 2, 2), (1, 3, 1), (2, 0, 2),
(2, 1, 1), (2, 3, 3), (3, 0, 1), (3, 1, 1), (3, 2, 1)]
sage: prim(G)
Multi-graph on 4 vertices
sage: prim(G).edges()
[(0, 1, 1), (0, 2, 2), (1, 3, 1)]
\end{lstlisting}

\begin{figure}[!htbp]
\centering
\input{image/trees-forests/Prim-algorithm-digraph.tex}
\caption{Running Prim's algorithm over a digraph.}
\label{fig:tree_forests:Prim_algorithm_digraph}
\end{figure}

\begin{lstlisting}
sage: A = matrix([[0,7,0,5,0,0,0], [0,0,8,9,7,0,0], [0,0,0,0,5,0,0], \
...  [0,0,0,0,15,6,0], [0,0,0,0,0,8,9], [0,0,0,0,0,0,11], [0,0,0,0,0,0,0]])
sage: G = Graph(A, format="adjacency_matrix", weighted=True)
sage: E = G.edges(); E
[(0, 1, 7), (0, 3, 5), (1, 2, 8), (1, 3, 9), (1, 4, 7), (2, 4, 5),
(3, 4, 15), (3, 5, 6), (4, 5, 8), (4, 6, 9), (5, 6, 11)]
sage: prim(G).edges()
[(0, 1, 7), (0, 3, 5), (1, 2, 8), (1, 4, 7), (3, 5, 6), (4, 6, 9)]
\end{lstlisting}

\begin{figure}[!htbp]
\centering
\input{image/trees-forests/Prim-algorithm-undirected-graph.tex}
\caption{Running Prim's algorithm over an undirected graph.}
\label{fig:tree_forests:Prim_algorithm_undirected_graph}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Bor\r{u}vka's algorithm}
\label{subsec:trees_forests:Boruvka_algorithm}

Bor\r{u}vka's
algorithm~\cite{Boruvka1926a,Boruvka1926b}\index{Bor\r{u}vka's algorithm}
is an algorithm for finding a minimum spanning tree in a weighted
connected graph $G = (V,E)$ for which all edge weights are
distinct. It was first published in~1926 by Otakar Bor\r{u}vka but
subsequently rediscovered by many others, including
Choquet~\cite{Choquet1938} and Florek~et~al.~\cite{FlorekEtAl1951}. If
$G$ has order $n = |V|$ and size $m = |E|$, it can be shown that
Bor\r{u}vka's algorithm runs in time $O(m \cdot \log n)$.

\begin{algorithm}[!htbp]
\input{algorithm/trees-forests/Boruvka.tex}
\caption{Bor\r{u}vka's algorithm.}
\label{alg:trees_forests:Boruvka}
\end{algorithm}

Algorithm~\ref{alg:trees_forests:Boruvka} provides pseudocode of
Bor\r{u}vka's algorithm. Given a weighted connected graph $G = (V,E)$
all of whose edge weights are distinct, the initialization steps in
lines~\ref{alg:Boruvka:get_graph_order}
and~\ref{alg:Boruvka:spanning_forest} construct a spanning forest $T$
of $G$, i.e. the subgraph of $G$ containing all of the latter's
vertices and no edges. The initial forest has $n$ components, each
being the trivial graph $K_1$. The while loop from
lines~\ref{alg:Boruvka:while_loop_start}
to~\ref{alg:Boruvka:while_loop_end} constructs a spanning tree of $G$
via a recursive procedure similar to
Theorem~\ref{thm:trees_forests:recursive_construction_trees}. For each
component $T'$ of $T$, we consider all the out-going edges of $T'$ and
choose an edge $e'$ that has minimum weight among all such edges. This
edge is then added to the edge set of $T$. In this way, two distinct
components, each of which is a tree, are joined together by a
bridge. At the end of the while loop, our final graph is a minimum
spanning tree of $G$. Note that the forest-merging steps in the for
loop from lines~\ref{alg:Boruvka:for_loop}
to~\ref{alg:Boruvka:while_loop_end} are amenable to parallelization,
hence the alternative name to Bor\r{u}vka's algorithm: the parallel
forest-merging method.

\begin{figure}[!htbp]
\centering
\input{image/trees-forests/Boruvkas-algorithm.tex}
\caption{Recursive construction of MST via Bor\r{u}vka's algorithm.}
\label{fig:trees_forests:Boruvka_algorithm}
\end{figure}

\begin{example}
\rm
Figure~\ref{fig:trees_forests:Boruvka_algorithm} illustrates the
gradual construction of a minimum spanning tree for the undirected
graph given in
Figure~\ref{fig:Boruvkas_algorithm:original_undirected_graph}. In this
case, we require two iterations of the while loop in Bor\r{u}vka's
algorithm in order to obtain the final minimum spanning tree in
Figure~\ref{fig:Boruvkas_algorithm:final_minimum_spanning_tree}.\qed
\end{example}

\begin{lstlisting}
def which_index(x,L):
    """
    L is a list of sublists (or tuple of sets or list
    of tuples, etc).

    Returns the index of the first sublist which x belongs
    to, or None if x is not in flatten(L).

    The 0-th element in
    Lx = [L.index(S) for S in L if x in S]
    almost works, but if the list is empty then Lx[0]
    throws an exception.

    EXAMPLES:
        sage: L = [[1,2,3],[4,5],[6,7,8]]
        sage: which_index(3,L)
        0
        sage: which_index(4,L)
        1
        sage: which_index(7,L)
        2
        sage: which_index(9,L)
        sage: which_index(9,L) == None
        True
    """
    for S in L:
        if x in S:
            return L.index(S)
    return None

def boruvka(G):
    """
    Implements Boruvka's algorithm to compute a MST of a graph.

    INPUT:
        G - a connected edge-weighted graph with distinct weights.
    OUTPUT:
        T - a minimum weight spanning tree.

    REFERENCES:
        http://en.wikipedia.org/wiki/Boruvka's_algorithm
    """
    T_vertices = [] # assumes G.vertices = range(n)
    T_edges = []
    T = Graph()
    E = G.edges() # a list of triples
    V = G.vertices()
    # start ugly hack to sort E
    Er = [list(x) for x in E]
    E0 = []
    for x in Er:
        x.reverse()
        E0.append(x)
    E0.sort()
    E = []
    for x in E0:
        x.reverse()
        E.append(tuple(x))
    # end ugly hack to get E is sorted by weight
    for e in E:
        # create about |V|/2 edges of T "cheaply"
        TV = T.vertices()
        if not(e[0] in TV) or not(e[1] in TV):
           T.add_edge(e)
    for e in E:
        # connect the "cheapest" components to get T
        C = T.connected_components_subgraphs()
        VC = [S.vertices() for S in C]
        if not(e in T.edges()) and (which_index(e[0],VC) != which_index(e[1],VC)):
           if T.is_connected():
                break
            T.add_edge(e)
    return T
\end{lstlisting}

Some examples using Sage:
%
\begin{lstlisting}
sage: A = matrix([[0,1,2,3], [4,0,5,6], [7,8,0,9], [10,11,12,0]])
sage: G = DiGraph(A, format="adjacency_matrix", weighted=True)
sage: boruvka(G)
Multi-graph on 4 vertices
sage: boruvka(G).edges()
[(0, 1, 1), (0, 2, 2), (0, 3, 3)]
sage: A = matrix([[0,2,0,5,0,0,0], [0,0,8,9,7,0,0], [0,0,0,0,1,0,0],\
...   [0,0,0,0,15,6,0], [0,0,0,0,0,3,4], [0,0,0,0,0,0,11], [0,0,0,0,0,0,0]])
sage: G = Graph(A, format="adjacency_matrix", weighted=True)
sage: E = G.edges(); E
[(0, 1, 2), (0, 3, 5), (1, 2, 8), (1, 3, 9), (1, 4, 7),
(2, 4, 1), (3, 4, 15), (3, 5, 6), (4, 5, 3), (4,6, 4), (5, 6, 11)]
sage: boruvka(G)
Multi-graph on 7 vertices
sage: boruvka(G).edges()
[(0, 1, 2), (0, 3, 5), (2, 4, 1), (3, 5, 6), (4, 5, 3), (4, 6, 4)]
sage: A = matrix([[0,1,2,5], [0,0,3,6], [0,0,0,4], [0,0,0,0]])
sage: G = Graph(A, format="adjacency_matrix", weighted=True)
sage: boruvka(G).edges()
[(0, 1, 1), (0, 2, 2), (2, 3, 4)]
sage: A = matrix([[0,1,5,0,4], [0,0,0,0,3], [0,0,0,2,0], [0,0,0,0,0], [0,0,0,0,0]])
sage: G = Graph(A, format="adjacency_matrix", weighted=True)
sage: boruvka(G).edges()
[(0, 1, 1), (0, 2, 5), (1, 4, 3), (2, 3, 2)]
\end{lstlisting}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Binary trees}

A \emph{binary tree} is a rooted tree with at most two children per
parent. Each child is designated as either a \emph{left-child} or a
\emph{right-child}. Thus binary trees are also $2$-ary trees. Some
examples of binary trees are illustrated in
Figure~\ref{fig:trees_forests:examples_binary_trees}. Given a vertex
$v$ in a binary tree $T$ of height $h$, the \emph{left subtree} of $v$
is comprised of the subtree that spans the left-child of $v$ and all
of this child's descendants. The notion of a \emph{right-subtree} of a
binary tree is similarly defined. Each of the left- and right-subtrees
of $v$ is itself a binary tree with height $\leq h - 1$. If $v$ is the
root vertex, then each of its left- and right-subtrees has height
$\leq h - 1$, and at least one of these subtrees has height equal to
$h - 1$.

\begin{figure}[!htbp]
\centering
\input{image/trees-forests/example-binary-trees.tex}
\caption{Examples of binary trees.}
\label{fig:trees_forests:examples_binary_trees}
\end{figure}

\begin{theorem}
\label{thm:trees_forests:complete_binary_tree_exact_order}
If $T$ is a complete binary tree of height $h$, then $T$ has
$2^{h+1} - 1$ vertices.
\end{theorem}

\begin{proof}
Argue by induction on $h$. The assertion of the theorem is trivially
true in the base case $h = 0$. Let $k \geq 0$ and assume for induction
that any complete binary tree of height $k$ has order $2^{k+1} - 1$.
Suppose $T$ is a complete binary tree of height $k + 1$ and denote the
left- and right-subtrees of $T$ by $T_1$ and $T_2$, respectively. Each
$T_i$ (for $i = 1,2$) is a complete binary tree of height $k$ and by
our induction hypothesis $T_i$ has $2^{k+1} - 1$ vertices. Thus $T$
has order
\[
1 + (2^{k+1} - 1) + (2^{k+1} - 1)
=
2^{k+2} - 1
\]
as required.
\end{proof}

Theorem~\ref{thm:trees_forests:complete_binary_tree_exact_order}
provides a useful upper bound on the order of a binary tree of a given
height. This upper bound is stated in the following corollary.

\begin{corollary}
A binary tree of height $h$ has at most $2^{h+1} - 1$ vertices.
\end{corollary}

We now count the number of possible binary trees on $n$
vertices. Let $b_n$ be the number of binary trees of order $n$. For
$n = 0$, we set $b_0 = 1$. The trivial graph is the only binary tree
with one vertex, hence $b_1 = 1$. Suppose $n > 1$ and let $T$ be a
binary tree on $n$ vertices. Then the left subtree of $T$ has order
$0 \leq i \leq n - 1$ and the right subtree has $n - 1 - i$
vertices. As there are $b_i$ possible left subtrees and $b_{n-1-i}$
possible right subtrees, $T$ has a total of $b_i b_{n-1-i}$ different
combinations of left and right subtrees. Summing from $i = 0$ to
$i = n - 1$ and we have
%%
\begin{equation}
\label{eqn:trees_forests:Catalan_recursion}
b_n
=
\sum_{i=0}^{n-1} b_i b_{n-1-i}.
\end{equation}
%%
Expression~\eqref{eqn:trees_forests:Catalan_recursion} is known as the
\emph{Catalan recursion} and the number $b_n$ is the $n$-th Catalan
number, which we know from
problem~\ref{chap:introduction}.\ref{prob:introduction:Euler_polygon_division}
can be expressed in the closed form
%%
\begin{equation}
\label{eqn:trees_forests:nth_Catalan_number_closed_form}
b_n
=
\frac{1}{n+1} \binom{2n}{n}.
\end{equation}
%%
Figures~\ref{fig:trees_forests:binary_trees_2_vertices}
to~\ref{fig:trees_forests:binary_trees_4_vertices} enumerate all the
different binary trees on $2$, $3$, and $4$ vertices, respectively.

\begin{figure}[!htbp]
\centering
\input{image/trees-forests/binary-trees-2-vertices.tex}
\caption{The $b_2 = 2$ binary trees on $2$ vertices.}
\label{fig:trees_forests:binary_trees_2_vertices}
\end{figure}

\begin{figure}[!htbp]
\centering
\input{image/trees-forests/binary-trees-3-vertices.tex}
\caption{The $b_3 = 5$ binary trees on $3$ vertices.}
\label{fig:trees_forests:binary_trees_3_vertices}
\end{figure}

\begin{figure}[!htbp]
\centering
\input{image/trees-forests/binary-trees-4-vertices.tex}
\caption{The $b_4 = 14$ binary trees on $4$ vertices.}
\label{fig:trees_forests:binary_trees_4_vertices}
\end{figure}

The first few values
of~\eqref{eqn:trees_forests:nth_Catalan_number_closed_form} are
\[
b_0 = 1,\quad
b_1 = 1,\quad
b_2 = 2,\quad
b_3 = 5,\quad
b_4 = 14
\]
which are rather small and of manageable size if we want to explicitly
enumerate all different binary trees with the above orders. However,
from $n = 4$ onwards the value of $b_n$ increases very fast. Instead
of enumerating all the $b_n$ different binary trees of a specified
order $n$, a related problem is generating a random binary tree of
order $n$. That is, we consider the set $B$ as a sample space of $b_n$
different binary trees on $n$ vertices, and choose a random element
from $B$. Such a random element can be generated using
Algorithm~\ref{alg:trees_forests:random_binary_tree}. The list
\texttt{parent} holds all vertices with less than two children, each
vertex can be considered as a candidate parent to which we can add a
child. An element of \texttt{parent} is a two-tuple $(v, k)$ where the
vertex $v$ currently has $k$ children.

\begin{algorithm}[!htbp]
\input{algorithm/trees-forests/random-binary-tree.tex}
\caption{Random binary tree.}
\label{alg:trees_forests:random_binary_tree}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Binary codes}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{What is a code?}

A \emph{code}\index{code} is a rule for converting data in one format,
or well-defined tangible representation, into sequences of symbols in
another format. The finite set of symbols used is called the
\emph{alphabet}\index{alphabet}. We shall identify a code as a finite
set of symbols which are the image of the alphabet under this
conversion rule. The elements of this set are referred to as
\emph{codewords}\index{codeword}. For example, using the ASCII code,
the letters in the English alphabet get converted into numbers in the
set $\{0, 1, \dots, 255\}$. If these numbers are written in binary,
then each codeword of a letter has length $8$, i.e. eight bits. In
this way, we can reformat or encode a ``string'' into a sequence of
binary symbols, i.e. $0$'s and $1$'s. \emph{Encoding}\index{encode} is
the conversion process one way. \emph{Decoding}\index{decode} is the
reverse process, converting these sequences of code-symbols back into
information in the original format.

Codes are used for:
%%
\begin{itemize}
\item \emph{Economy}. Sometimes this is called \emph{entropy encoding}
  since there is an entropy function which describes how much
  information a channel (with a given error rate) can carry and such
  codes are designed to maximize entropy as best as possible. In this
  case, in addition to simply being given an alphabet $\cA$, one might
  be given a \emph{weighted alphabet}, i.e. an alphabet for which each
  symbol $a \in \cA$ is associated with a nonnegative number
  $w_a \geq 0$~(in practice, the probability that the symbol $a$
  occurs in a typical word).

\item \emph{Reliability}. Such codes are called
  \emph{error-correcting codes}, since such codes are designed to
  communicate information over a noisy channel in such a way that the
  errors in transmission are likely to be correctable.

\item \emph{Security}. Such codes are called \emph{cryptosystems}. In
  this case, the inverse of the coding function $c: \cA \to B^*$ is
  designed to be computationally infeasible. In other words, the
  coding function $c$ is designed to be a \emph{trapdoor function}.
\end{itemize}
%%
Other codes are merely simpler ways to communicate
information~(e.g.~flag semaphores, color codes, genetic codes, braille
codes, musical scores, chess notation, football diagrams, and so on)
and have little or no mathematical structure. We shall not study them.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Basic definitions}

If every word in the code has the same length, the code is called a
\emph{block code}\index{code!block}. If a code is not a block code,
then it is called a \emph{variable-length}\index{code!variable-length}
code. A \emph{prefix-free}\index{code!prefix-free} code is a
code~(typically one of variable-length) with the property that there
is no valid codeword in the code that is a prefix or start of any
other codeword.\footnote{
  In other words, a codeword $s = s_1 \cdots s_m$ is a \emph{prefix} of
  a codeword $t = t_1 \cdots t_n$ if and only if $m \leq n$ and
  $s_1 = t_1, \dots, s_m = t_m$. Codes that are prefix-free are easier
  to decode than codes that are not prefix-free.
}
This is the \emph{prefix-free condition}.

One example of a prefix-free code is the ASCII code. Another example
is
\[
00,\, 01,\, 100.
\]
On the other hand, a non-example is the code
\[
00,\, 01,\, 010,\, 100
\]
since the second codeword is a prefix of the third one. Another
non-example is Morse\index{code!Morse} code recalled in
Table~\ref{tab:trees_forests:Morse_code}, where we use $0$ for
``$\cdot$'' (``dit'') and $1$ for ``$-$'' (``dah''). For example,
consider the Morse code for \texttt{a} and the Morse code for
\texttt{w}. These codewords violate the prefix-free condition.

\begin{table}[!htbp]
\centering
\input{data/trees-forests/Morse-code.tex}
\caption{Morse code}
\label{tab:trees_forests:Morse_code}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Gray codes}

We begin with some history.\footnote{
  This history comes from an unpublished section~7.2.1.1
  (``Generating all $n$-tuples'') in volume~4 of Donald Knuth's
  \emph{The Art of Computer Programming}.
}
Frank Gray~(1887--1969) wrote about the so-called Gray codes in a~1951
paper published in the Bell System Technical Journal and then in~1953
patented a device~(used for television sets) based on his
paper. However, the idea of a binary Gray code appeared earlier. In
fact, it appeared in an earlier patent~(one by Stibitz in~1943). It
was also used in the French engineer E.~Baudot's telegraph machine
of~1878 and in a French booklet by L.~Gros on the solution published
in~1872 to the ``Chinese ring puzzle''.

The term ``Gray code'' is ambiguous. It is actually a large family of
sequences of $n$-tuples. Let $\Z_m = \{0, 1, \dots, m-1\}$. More
precisely, an
\emph{$m$-ary Gray code of length $n$}\index{Gray code!$m$-ary}~(called
a \emph{binary Gray code}\index{Gray code!binary} when $m = 2$) is a
sequence of all possible~(i.e. $N = m^n$) $n$-tuples
\[
g_1, g_2, \dots, g_N
\]
where
%%
\begin{itemize}
\item each $g_i \in \Z_m^n$,

\item $g_i$ and $g_{i+1}$ differ by $1$ in exactly one coordinate.
\end{itemize}
%%
In other words, an $m$-ary Gray code of length $n$ is a particular way
to order the set of all $m^n$ $n$-tuples whose coordinates are taken
from $\Z_m$. From the transmission/communication perspective, this
sequence has two advantages:
%%
\begin{itemize}
\item It is easy and fast to produce the sequence, since successive
  entries differ in only one coordinate.

\item An error is relatively easy to detect, since we can compare an
  $n$-tuple with the previous one. If they differ in more than one
  coordinate, we conclude that an error was made.
\end{itemize}

\begin{example}
\rm
Here is a $3$-ary Gray code of length $2$:
\[
[0,0],\; [1,0],\; [2,0],\; [2,1],\; [1,1],\; [0,1],\; [0,2],\;
[1,2],\; [2,2]
\]
and the sequence
\[
[0,0,0],\; [1,0,0],\; [1,1,0],\; [0,1,0],\; [0,1,1],\;
[1,1,1],\; [1,0,1],\; [0,0,1]
\]
is a binary Gray code of length $3$. \qed
\end{example}

Gray codes have applications to engineering, recreational
mathematics~(solving the Tower of Hanoi puzzle, ``The Brain'' puzzle,
the ``Chinese ring puzzle'', etc.), and to mathematics~(e.g. aspects
of combinatorics, computational group theory, and the computational
aspects of linear codes).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Binary Gray codes}

Consider the so-called $n$-hypercube\index{graph!hypercube} graph
$Q_n$, whose first few instances are illustrated in
Figure~\ref{fig:introduction:hypercube_graphs}. This can be envisioned
as the graph whose vertices are the vertices of a cube in $n$-space
\[
\{(x_1, \dots, x_n) \mid 0 \leq x_i \leq 1\}
\]
and whose edges are those line segments in $\R^n$ connecting two
\emph{neighboring} vertices, i.e. two vertices that differ in exactly
one coordinate. A binary Gray code of length $n$ can be regarded as a
path on the hypercube graph $Q_n$ that visits each vertex of the cube
exactly once. In other words, a binary Gray code of length $n$ may be
identified with a Hamiltonian path on the graph $Q_n$. For example,
Figure~\ref{fig:trees_forests:gray_code_cube} illustrates a
Hamiltonian path on $Q_3$.

\begin{figure}[!htbp]
\centering
\input{image/trees-forests/gray-code-cube.tex}
\caption{Viewing $\Gamma_3$ as a Hamiltonian path on $Q_3$.}
\label{fig:trees_forests:gray_code_cube}
\end{figure}

How do we efficiently compute a Gray code? Perhaps the simplest way to
state the idea of quickly constructing the
\emph{reflected binary Gray code} $\Gamma_n$ of length $n$ is as
follows:
%%
\begin{align*}
\Gamma_0 &= [\,], \\[4pt]
\Gamma_{n} &= \big[[0, \Gamma_{n-1}],\, [1, \Gamma_{n-1}^{\rm rev}]\big]
\end{align*}
%%
where $\Gamma_m^{\rm rev}$ means the Gray code in reverse order. For
instance, we have
%%
\begin{align*}
\Gamma_0 &= [\,], \\[4pt]
\Gamma_1 &= \big[[0],\, [1]\big], \\[4pt]
\Gamma_2 &= \big[[0,0],\, [0,1],\, [1,1],\, [1,0]\big]
\end{align*}
%%
and so on. This is a nice procedure for creating the entire list at
once, which gets very long very fast. An implementation of the
reflected Gray code using Python is given below.

\begin{lstlisting}
def graycode(length,modulus):
    """
    Returns the n-tuple reflected Gray code mod m.


    EXAMPLES:
        sage: graycode(2,4)

        [[0, 0],
         [1, 0],
         [2, 0],
         [3, 0],
         [3, 1],
         [2, 1],
         [1, 1],
         [0, 1],
         [0, 2],
         [1, 2],
         [2, 2],
         [3, 2],
         [3, 3],
         [2, 3],
         [1, 3],
         [0, 3]]
    """
    n,m = length,modulus
    F = range(m)
    if n == 1:
        return [[i] for i in F]
    L = graycode(n-1, m)
    M = []
    for j in F:
        M = M+[ll+[j] for ll in L]
    k = len(M)
    Mr = [0]*m
    for i in range(m-1):
        i1 = i*int(k/m)       # this requires Python 3.0 or Sage
        i2 = (i+1)*int(k/m)
        Mr[i] = M[i1:i2]
    Mr[m-1] = M[(m-1)*int(k/m):]
    for i in range(m):
        if is_odd(i):
            Mr[i].reverse()
    M0 = []
    for i in range(m):
        M0 = M0+Mr[i]
    return M0
\end{lstlisting}

\vskip .2in
Consider the reflected binary code of length $8$,
i.e. $\Gamma_8$. This has $2^8 = 256$ codewords. Sage can easily
create the list plot of the coordinates $(x,y)$, where $x$ is an
integer $j \in \Z_{256}$ that indexes the codewords in $\Gamma_8$ and
the corresponding $y$ is the $j$-th codeword in $\Gamma_8$ converted
to decimal. This will give us some idea of how the Gray code ``looks''
in some sense. The plot is given in Figure~\ref{fig:trees_forests:Gamma_8}.

\begin{figure}[!htbp]
\centering
\input{image/trees-forests/gamma8.tex}
\caption{Scatterplot of $\Gamma_8$.}
\label{fig:trees_forests:Gamma_8}
\end{figure}

What if we only want to compute the $i$-th Gray codeword in the Gray
code of length $n$? Can it be computed quickly without computing the
entire list? At least in the case of the reflected binary Gray code,
there is a very simple way to do this. The $k$-th element in the
above-described reflected binary Gray code of length $n$ is obtained
by simply adding the binary representation of $k$ to the binary
representation of the integer part of $k / 2$. An example using Sage
is given below.

\begin{lstlisting}
def int2binary(m, n):
    '''
    returns GF(2) vector of length n obtained
    from the binary repr of m, padded by 0's
    (on the left) to length n.

    EXAMPLES:
        sage: for j in range(8):
        ....:     print int2binary(j,3)+int2binary(int(j/2),3)
        ....:
        (0, 0, 0)
        (0, 0, 1)
        (0, 1, 1)
        (0, 1, 0)
        (1, 1, 0)
        (1, 1, 1)
        (1, 0, 1)
        (1, 0, 0)
    '''
    s = bin(m)
    k = len(s)
    F = GF(2)
    b = [F(0)]*n
    for i in range(2,k):
        b[n-k+i] = F(int(s[i]))
    return vector(b)

def graycodeword(m, n):
    '''
    returns the kth codeword in the reflected binary Gray code
    of length n.

    EXAMPLES:
        sage: graycodeword(3,3)
        (0, 1, 0)
    '''
    return map(int, int2binary(m,n)+int2binary(int(m/2),n))
\end{lstlisting}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Huffman codes}
\label{sec:trees_forests:Huffman_codes}

An \emph{alphabet}\index{alphabet} $\cA$ is a finite set whose
elements are referred to as \emph{symbols}. A
\emph{word}\index{word}~(or \emph{string}\index{string} or
\emph{message}\index{message}) over $\cA$ is a finite sequence of
symbols in $\cA$ and the \emph{length} of the word is the number of
symbols it contains. A word is usually written by concatenating
symbols together, e.g. $a_1 a_2 \cdots a_k$ ($a_i \in \cA$) is a word
of length $k$.

A commonly occurring alphabet in practice is the \emph{binary alphabet}
$\B = \{0, 1\}$. A word over the binary alphabet is a finite sequence
of $0$'s and $1$'s. If $\cA$ is an alphabet, let $\cA^*$ denote the
set of all words in $\cA$. The length of a word is denoted by vertical
bars. That is, if $w = a_1 \cdots a_k$ is a word over $\cA$, then
define $|w|: \cA^* \to \Z$ by
\[
|w|
=
|a_1 \cdots a_k|
=
k.
\]
Let $\cA$ and $\cB$ be two alphabets. A \emph{code}\index{code} for
$\cA$ using $\cB$ is an injection $c: \cA \to \cB^*$. By abuse of
notation, we often denote the code simply by the set
\[
C
=
c(A)
=
\{c(a) \mid a \in \cA\}.
\]
The elements of $C$ are called \emph{codewords}\index{codeword}. If
$\cB$ is the binary alphabet, then $C$ is called a
\emph{binary code}\index{code!binary}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Tree representation}

Any binary code can be represented by a tree, as
Example~\ref{eg:tree_forests:binary_code_tree_representation} shows.

\begin{example}
\label{eg:tree_forests:binary_code_tree_representation}
Let $\B_\ell$ be the binary code of length $\leq \ell$. Represent
codewords of $\B_\ell$ using trees.
\end{example}

\begin{proof}[Solution]
Here is how to represent the code $\B_\ell$ consisting of all binary
strings of length $\leq \ell$. Start with the
root\index{node!root} node $\varepsilon$\index{$\varepsilon$}
being the empty string. The two children of this node, $v_0$ and
$v_1$, correspond to the two strings of length $1$. Label $v_0$ with a
``$0$'' and $v_1$ with a ``$1$''. The two children of $v_0$,
i.e. $v_{00}$ and $v_{01}$, correspond to the strings of length $2$
which start with a $0$. Similarly, the  two children of $v_1$,
i.e. $v_{10}$ and $v_{11}$, correspond to the strings of length $2$
that each starts with a $1$. Continue creating child nodes until we
reach length $\ell$, at which point we stop. There are a total of
$2^{\ell + 1} - 1$ nodes in this tree and $2^\ell$ of them are
leaves\index{leaf}~(vertices of a tree with degree $1$, i.e. childless
nodes). Note that the parent of any node is a prefix to that
node. Label each node $v_s$ with the string ``$s$'', where $s$ is a
binary sequence of length $\leq \ell$. See
Figure~\ref{fig:trees_forests:tree_representation_B_2} for an example
when $\ell = 2$.
\end{proof}

\begin{figure}[!htbp]
\centering
\input{image/trees-forests/tree-representation-B-2.tex}
\caption{Tree representation of the binary code $\B_2$.}
\label{fig:trees_forests:tree_representation_B_2}
\end{figure}

In general, if $C$ is a code contained in $\B_\ell$, then to create
the tree\index{code!tree} for $C$, start with the tree for
$\B_\ell$. First, remove all nodes associated to a binary string for
which it and all of its descendants are not in $C$. Next, remove all
labels which do not correspond to codewords in $C$. The resulting
labeled graph is the tree associated to the binary code $C$.

For visualizing the construction of Huffman codes later, it is
important to see that we can \emph{reverse} this construction to
start from such a binary tree and recover a binary code from it. The
codewords are determined by the following rules:
%%
\begin{itemize}
\item The root node gets the empty codeword.

\item Each left-ward branch gets a $0$ appended to the end of its
  parent. Each right-ward branch gets a $1$ appended to the end.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Uniquely decodable codes}

If $c: \cA \to \cB^*$ is a code, then we can extend $c$ to $\cA^*$ by
concatenation:
\[
c(a_1 a_2 \cdots a_k)
=
c(a_1) c(a_2) \cdots c(a_k).
\]
If the extension $c: \cA^* \to \cT^*$ is also an injection, then $c$
is called \emph{uniquely decodable}\index{code!uniquely decodable}.

\begin{example}
Is the Morse code\index{code!Morse} in
Table~\ref{tab:trees_forests:Morse_code} uniquely decodable? Why or
why not?
\end{example}

\begin{proof}[Solution]
Note that these Morse codewords all have lengths less than or equal to
$4$. Other commonly occurring symbols used~(the digits $0$ through
$9$, punctuation symbols, and some others) are also encodable in Morse
code, but they use longer codewords.

Let $\cA$ denote the English alphabet, $\B = \{0, 1\}$ the binary
alphabet, and $c: \cA \to \B^*$ the Morse code. Since
$c(ET) = 01 = c(A)$, it is clear that the Morse code is \emph{not}
uniquely decodable.
\end{proof}

In fact, prefix-free implies uniquely decodable.

\begin{theorem}
If a code $c: \cA \to \cB^*$ is prefix-free, then it is uniquely
decodable.
\end{theorem}

\begin{proof}
We use induction on the length of a message. We want to show that if
$x_1 \cdots x_k$ and $y_1 \cdots y_\ell$ are messages with
$c(x_1) \cdots c(x_k) = c(y_1) \cdots c(y_\ell)$, then
$x_1 \cdots x_k = y_1 \cdots y_\ell$. This in turn implies $k = \ell$
and $x_i = y_i$ for all $i$.

The case of length $1$ follows from the fact that $c: \cA \to \cB^*$
is injective~(by the definition of code).

Suppose that the statement of the theorem holds for all codes of
length $< m$. We must show that the length $m$ case is true. Suppose
$c(x_1) \cdots c(x_k) = c(y_1) \cdots c(y_\ell)$, where
$m = \max(k, \ell)$. These strings are equal, so the substring
$c(x_1)$ of the left-hand side and the substring $c(y_1)$ of the
right-hand side are either equal or one is contained in the other. If,
say, $c(x_1)$ is properly contained in $c(y_1)$, then $c$ is not
prefix-free. Likewise if $c(y_1)$ is properly contained in
$c(x_1)$. Therefore, $c(x_1) = c(y_1)$, which implies $x_1 = y_1$. Now
remove this codeword from both sides, so
$c(x_2) \cdots c(x_k) = c(y_2) \cdots c(y_\ell)$. By the induction
hypothesis, $x_2 \cdots x_k = y_2 \cdots y_\ell$. These facts together
imply $k = \ell$ and $x_i = y_i$ for all $i$.
\end{proof}

Consider now a weighted alphabet $(\cA, p)$, where $p: \cA \to [0,1]$
satisfies $\sum_{a \in \cA} p(a) = 1$, and a code $c: \cA \to \cB^*$.
In other words, $p$ is a probability distribution on $\cA$. Think of
$p(a)$ as the probability that the symbol $a$ arises in a typical
message. The \emph{average word length} $L(c)$ is\footnote{
  In probability terminology, this is the expected value $\E(X)$ of
  the random variable $X$, which assigns to a randomly selected symbol
  in $\cA$ the length of the associated codeword in $c$.
}
\[
L(c)
=
\sum_{a \in \cA} p(a) \cdot |c(a)|
\]
where $|\cdot|$ is the length\index{codeword!length} of a codeword.
Given a weighted alphabet $(\cA, p)$ as above, a code
$c: \cA \to \cB^*$ is called \emph{optimal}\index{code!optimal} if
there is no such code with a smaller average word length. Optimal
codes satisfy the following amazing property. For a proof, which is
very easy and highly recommended for anyone who is curious to
see more, refer to section~3.6 of Biggs~\cite{Biggs2009}.

\begin{lemma}
\label{lem:trees_forests:binary_optimal_prefix_free_code}
Suppose $c: \cA \to \B^*$ is a binary optimal prefix-free code and let
$\ell = \max_{a \in \cA} \big(|c(a)|\big)$ denote the maximum length
of a codeword. The following statements hold.
%%
\begin{enumerate}
\item If $|c(a')| > |c(a)|$, then $p(a') \leq p(a)$.

\item The subset of codewords of length $\ell$, i.e.
\[
C_\ell
=
\{c \in c(\cA) \mid \ell = |c(a)|\}
\]
contains two codewords of the form $b0$ and $b1$ for some $b \in \B^*$.
\end{enumerate}
\end{lemma}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Huffman coding}

The Huffman code construction is based on the second property in
Lemma~\ref{lem:trees_forests:binary_optimal_prefix_free_code}. Using
this property, in 1952 David Huffman~\cite{Huffman1952} presented an
optimal prefix-free binary code, which has since been named Huffman
code.

Here is the recursive/inductive construction of a Huffman code. We
shall regard the binary Huffman code as a tree, as described
above. Suppose that the weighted alphabet $(\cA, p)$ has $n$
symbols. We assume inductively that there is an optimal prefix-free
binary code for any weighted alphabet $(\cA', p')$ having $<n$ symbols.

\begin{description}
\item[Huffman's rule 1] Let $a,a' \in \cA$ be symbols with the
  smallest weights. Construct a new weighted alphabet with $a,a'$
  replaced by the single symbol $a^* = aa'$ and having weight
  $p(a^*) = p(a) + p(a')$. All other symbols and weights remain
  unchanged.

\item[Huffman's rule 2] For the code $(\cA', p')$ above, if $a^*$ is
  encoded as the binary string $s$, then the encoded binary string for
  $a$ is $s0$ and the encoded binary string for $a'$ is $s1$.
\end{description}

The above two rules tell us how to inductively build the tree
representation for the Huffman code of $(\cA, p)$ up from its
leaves~(associated to the low weight symbols).
%%
\begin{itemize}
\item Find two different symbols of lowest weight, $a$ and $a'$. If
  two such symbols do not exist, stop. Replace the weighted alphabet
  with the new weighted alphabet as in Huffman's rule~1.

\item Add two nodes~(labeled with $a$ and $a'$, respectively) to the
  tree, with parent $a^*$ (see Huffman's rule~1).

\item If there are no remaining symbols in $\cA$, label the parent
  $a^*$ with the empty set and stop. Otherwise, go to the first step.
\end{itemize}

These ideas are captured in
Algorithm~\ref{alg:trees_forests:binary_tree_Huffman_codes}, which
outlines steps to construct a binary tree corresponding to the Huffman
code of an alphabet.
Line~\ref{alg:Huffman_tree:initialize_priority_queue} initializes a
minimum priority queue $Q$ with the symbols in the alphabet $A$.
Line~\ref{alg:Huffman_tree:empty_binary_tree} creates an empty binary
tree that will be used to represent the Huffman code corresponding to
$A$. The for loop from lines~\ref{alg:Huffman_tree:for_loop:start}
to~\ref{alg:Huffman_tree:insert_into_queue} repeatedly extracts from
$Q$ two elements $a$ and $b$ of minimum weights. We then create a new
vertex $z$ for the tree $T$ and also let $a$ and $b$ be vertices of
$T$. The weight $W[z]$ of $z$ is the sum of the weights of $a$ and
$b$. We let $z$ be the parent of $a$ and $b$, and insert the new edges
$za$ and $zb$ into $T$. The newly created vertex $z$ is now inserted
into $Q$ with priority $W[z]$. After $n - 1$ rounds of the for loop,
the priority queue has only one element in it, namely the root $r$ of
the binary tree $T$. We extract $r$ from
$Q$~(line~\ref{alg:Huffman_tree:extract_tree_root}) and return it
together with $T$~(line~\ref{alg:Huffman_tree:return_tree_and_root}).

\begin{algorithm}[!htbp]
\input{algorithm/trees-forests/binary-tree-Huffman-codes.tex}
\caption{Binary tree representation of Huffman codes.}
\label{alg:trees_forests:binary_tree_Huffman_codes}
\end{algorithm}

The running time analysis of
Algorithm~\ref{alg:trees_forests:binary_tree_Huffman_codes} depends on
the implementation of the priority queue $Q$. Suppose $Q$ is a simple
unsorted list. The initialization on
line~\ref{alg:Huffman_tree:initialize_priority_queue} requires $O(n)$
time. The for loop from line~\ref{alg:Huffman_tree:for_loop:start}
to~\ref{alg:Huffman_tree:insert_into_queue} is executed exactly
$n - 1$ times. Searching $Q$ to determine the element of minimum
weight requires time at most $O(n)$. Determining two elements of
minimum weights requires time $O(2n)$. The for loop requires time
$O(2n^2)$, which is also the time requirement for the algorithm. An
efficient implementation of the priority queue $Q$, e.g. as a binary
minimum heap, can lower the running time of
Algorithm~\ref{alg:trees_forests:binary_tree_Huffman_codes} down to
$O(n \log_2(n))$.

Algorithm~\ref{alg:trees_forests:binary_tree_Huffman_codes} represents
the Huffman code of an alphabet as a binary tree $T$ rooted at $r$. To
determine the actual encoding of each symbol in the alphabet, we feed
$T$ and $r$ to
Algorithm~\ref{alg:trees_forests:Huffman_encoding_alphabet} to obtain
the encoding of each symbol. Starting from the root $r$ whose
designated label is the empty string $\varepsilon$, the algorithm
traverses the vertices of $T$ in a breadth-first search fashion. If
$v$ is an internal vertex with label \verb!e!, the label of its left
child is the concatenation \verb!e0! and for the right child of $v$ we
assign the label \verb!e1!. If $v$ happens to be a leaf vertex, we
take its label to be its Huffman encoding. Any Huffman encoding
assigned to a symbol of an alphabet is not unique. Either of the two
children of an internal vertex can be designated as the left
(resp. right) vertex. The running time of
Algorithm~\ref{alg:trees_forests:Huffman_encoding_alphabet} is
$O(|V|)$, where $V$ is the vertex set of $T$.

\begin{algorithm}[!htbp]
\input{algorithm/trees-forests/Huffman-encoding-alphabet.tex}
\caption{Huffman encoding of an alphabet.}
\label{alg:trees_forests:Huffman_encoding_alphabet}
\end{algorithm}

\begin{example}
Consider the alphabet $\cA = \{a, b, c, d, e, f\}$ with corresponding
weights $w(a) = 19$, $w(b) = 2$, $w(c) = 40$, $w(d) = 25$,
$w(e) = 31$, and $w(f) = 3$. Construct a binary tree representation of
the Huffman code of $\cA$ and determine the encoding of each symbol of
$\cA$.
\end{example}

\begin{proof}[Solution]
Use Algorithm~\ref{alg:trees_forests:binary_tree_Huffman_codes} to
construct a binary tree representation of the weighted alphabet
$\cA$. The resulting binary tree $T$ is shown in
Figure~\ref{fig:trees_forests:eg:binary_tree_Huffman_encodings:binary_tree},
where $a_i: w_i$ is an abbreviation for
``vertex $a_i$ has weight $w_i$''. The binary tree is rooted at
$k$. To encode each alphabetic symbol, input $T$ and $k$ into
Algorithm~\ref{alg:trees_forests:Huffman_encoding_alphabet} to get the
encodings shown in
Figure~\ref{fig:trees_forests:eg:binary_tree_Huffman_encodings:Huffman_encodings}.
\end{proof}

\begin{figure}[!htbp]
\centering
\input{image/trees-forests/binary-tree-Huffman-encodings.tex}
\caption{Binary tree representation of an alphabet and its Huffman encodings.}
\label{fig:trees_forests:eg:binary_tree_Huffman_encodings}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Tree traversals}

In computer science, \emph{tree traversal}\index{tree traversal}
refers to the process of examining each vertex in a tree data
structure. Starting at the root of an ordered tree $T$, we can
traverse the vertices of $T$ in one of various ways.

A \emph{level-order} traversal of an ordered tree $T$ examines the
vertices in increasing order of depth, with vertices of equal depth
being examined according to their prescribed order. One way to think
about level-order traversal is to consider vertices of $T$ having the
same depth as being ordered from left to right in decreasing order of
importance. If $[v_1, v_2, \dots, v_n]$ lists the vertices from left
to right at depth $k$, a decreasing order of importance can be
realized by assigning each vertex a numeric label using a labelling
function $L: V(T) \to \R$ such that
$L(v_1) < L(v_2) < \cdots < L(v_n)$. In this way, a vertex with a
lower numeric label is examined prior to a vertex with a higher
numeric label. A level-order traversal of $T$, whose vertices of equal
depth are prioritized according to $L$, is an examination of the
vertices of $T$ from top to bottom, left to right. As an example, the
level-order traversal of the tree in
Figure~\ref{fig:trees_forests:level_order_traversal} is
\[
42,\, 4,\, 5,\, 2,\, 3,\, 5,\, 7,\, 10,\, 11,\, 12,\, 13,\, 14.
\]
Our discussion is formalized in
Algorithm~\ref{alg:trees_forests:level_order_traversal}, whose general
structure mimics that of breadth-first search. For this reason,
level-order traversal is also known as \emph{breadth-first traversal}.

\begin{figure}[!htbp]
\centering
\input{image/trees-forests/level-order-traversal.tex}
\caption{Level-order traversal of a tree.}
\label{fig:trees_forests:level_order_traversal}
\end{figure}

\begin{algorithm}[!htbp]
\input{algorithm/trees-forests/level-order-traversal.tex}
\caption{Level-order traversal.}
\label{alg:trees_forests:level_order_traversal}
\end{algorithm}

\begin{itemize}
\item breadth-first, or level-order, traversal

\item depth-first, or pre-order, traversal

\item post-order traversal

\item symmetric, or in-order, traversal
\end{itemize}

\noindent
{\it Depth-first traversal}:
\index{tree traversal!depth-first}
\index{tree traversal!pre-order}
\begin{itemize}
\item
Visit the root vertex.

\item
Traverse the left subtree recursively.

\item
Traverse the right subtree recursively.

\end{itemize}


\noindent
{\it Breadth-first traversal}:
\index{tree traversal!breadth-first}
\index{tree traversal!level-order}
\begin{itemize}
\item
Initialize $i=0$ and set $N$ equal to the maximum depth
of the tree (i.e., the maximum distance from the root vertex
to any other vertex in the tree).
\index{tree!depth}

\item
Visit the vertices of depth $i$.

\item
Increment $i=i+1$. If $i>N$ then stop. Otherwise, go to
the previous step.

\end{itemize}


\noindent
{\it post-order traversal}:
\index{tree traversal!post-order}
\begin{itemize}
\item
Traverse the left subtree recursively.

\item
Visit the root vertex.

\item
Traverse the right subtree recursively.

\end{itemize}


\noindent
{\it symmetric traversal}:
\index{tree traversal!symmetric}
\index{tree traversal!in-order}
\begin{itemize}
\item
Traverse the left subtree recursively.

\item
Visit the root vertex.

\item
Traverse the right subtree recursively.

\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Binary search trees}

See section~3.6 of Gross and Yellen~\cite{GrossYellen1999}, and
chapter~12 of Cormen~et~al.~\cite{CormenEtAl2001}. See also
\url{http://en.wikipedia.org/wiki/Binary_search_tree}.


\begin{itemize}
\item records and keys

\item searching a binary search tree (BST)

\item inserting into a BST

\item deleting from a BST

\item traversing a BST

\item sorting using BST
\end{itemize}

A {\it binary search tree} (BST) is a rooted binary tree
$T=(V,E)$ having weighted vertices ${\rm wt}:V\to \R$ satisfying:
\index{binary search tree}
\index{BST}

\begin{itemize}
\item
 The left subtree of a vertex $v$ contains only vertices whose label
(or ``key'') is less than the label of $v$.
\item
The right subtree of a vertex $v$ contains only vertices whose label
  is greater than the label of $v$.
\item
Both the left and right subtrees must also be binary search trees.
\end{itemize}

From the above properties it naturally follows that:
{\it Each vertex has a distinct label.}


\subsubsection{Traversal}

The vertices of a BST $T$ can be visited retrieved in-order of the
weights of the vertices (i.e.,
using a symmetric search type) by recursively  traversing the left subtree of the
root vertex, then accessing the root vertex itself, then recursively traversing the
right subtree of the root node.

\subsubsection{Searching}

We are given a BST (i.e., a binary rooted tree with weighted vertices
having distinct weights satisfying the above criteria) $T$ and a
label $\ell$. For this search, we are looking for a vertex in $T$
whose label is $\ell$, if one exists.

We begin by examining the root vertex, $v_0$. If $\ell={\rm wt}(v_0)$,
the search is successful. If the $\ell<{\rm wt}(v_0)$,
search the left subtree. Similarly, if $\ell>{\rm wt}(v_0)$,
search the right subtree. This process is repeated until a vertex
$v\in V$ is found for which $\ell={\rm wt}(v)$,
or the indicated subtree is empty.


\subsubsection{Insertion}

We are given a BST (i.e., a binary rooted tree with weighted vertices
having distinct weights satisfying the above criteria) $T$ and a
label $\ell$. We assume $\ell$ is between the
lowest weight of $T$ and the highest weight.
For this procedure, we are looking for a ``parent''
vertex in $T$ which can ``adopt'' a new vertex $v$ having weight $\ell$
and for which this augmented tree $T\cup v$ satisfies
the criteria above.

Insertion proceeds as a search does. However, in this case, you are
searching for vertices $v_1,v_2\in V$ for which
${\rm wt}(v_1)<\ell < {\rm wt}(v_2)$. Once found, these
vertices will tell you where to insert $v$.

\subsubsection{Deletion}

As above, we are given a BST $T$ and a
label $\ell$. We assume $\ell$ is between the
lowest weight of $T$ and the highest weight.
For this procedure, we are looking for a vertex $v$ of
$T$ which has weight $\ell$. We want to remove $v$ from
$T$ (and therefore also the weight $\ell$ from the list of weights),
thereby creating a ``smaller'' tree $T- v$ satisfying
the criteria above.

Deletion proceeds as a search does. However, in this case, you are
searching for vertex $v\in V$ for which
${\rm wt}(v)=\ell$. Once found, we remove $v$ from $V$
and any edge $(u,v)\in E$ is replaced by $(u,w_1)$
and $(u,w_2)$, where $w_1.w_2\in V$ were the children of $v$
in $T$.

\subsubsection{Sorting}

A binary search tree can be used to implement a simple but efficient
sorting algorithm. Suppose we wish to sort a list of numbers
$L = [\ell_1, \ell_2,\dots, \ell_n]$. First, let $V=\{1,2,\dots,n\}$
be the vertices of a tree and weight vertex $i$ with $\ell_i$,
for $1\leq i\leq n$. In this case, we can traverse this tree
in order of its weights, thereby building a BST recursively.
This BST represents the sorting of the list $L$.
Generally, the information represented by each vertex is a
record (or list or dictionary), rather than a single data element. However,
for sequencing purposes, vertices are compared according to their
labels rather than any part of their associated records.

\subsubsection{Traversal}

The vertices of a BST $T$ can be visited retrieved in-order of the
weights of the vertices (i.e.,
using a symmetric search type) by recursively  traversing the left subtree of the
root vertex, then accessing the root vertex itself, then recursively traversing the
right subtree of the root node.

\subsubsection{Searching}

We are given a BST (i.e., a binary rooted tree with weighted vertices
having distinct weights satisfying the above criteria) $T$ and a
label $\ell$. For this search, we are looking for a vertex in $T$
whose label is $\ell$, if one exists.

We begin by examining the root vertex, $v_0$. If $\ell={\rm wt}(v_0)$,
the search is successful. If the $\ell<{\rm wt}(v_0)$,
search the left subtree. Similarly, if $\ell>{\rm wt}(v_0)$,
search the right subtree. This process is repeated until a vertex
$v\in V$ is found for which $\ell={\rm wt}(v)$,
or the indicated subtree is empty.


\subsubsection{Insertion}

We are given a BST (i.e., a binary rooted tree with weighted vertices
having distinct weights satisfying the above criteria) $T$ and a
label $\ell$. We assume $\ell$ is between the
lowest weight of $T$ and the highest weight.
For this procedure, we are looking for a ``parent''
vertex in $T$ which can ``adopt'' a new vertex $v$ having weight $\ell$
and for which this augmented tree $T\cup v$ satisfies
the criteria above.

Insertion proceeds as a search does. However, in this case, you are
searching for vertices $v_1,v_2\in V$ for which
${\rm wt}(v_1)<\ell < {\rm wt}(v_2)$. Once found, these
vertices will tell you where to insert $v$.

\subsubsection{Deletion}

As above, we are given a BST $T$ and a
label $\ell$. We assume $\ell$ is between the
lowest weight of $T$ and the highest weight.
For this procedure, we are looking for a vertex $v$ of
$T$ which has weight $\ell$. We want to remove $v$ from
$T$ (and therefore also the weight $\ell$ from the list of weights),
thereby creating a ``smaller'' tree $T- v$ satisfying
the criteria above.

Deletion proceeds as a search does. However, in this case, you are
searching for vertex $v\in V$ for which
${\rm wt}(v)=\ell$. Once found, we remove $v$ from $V$
and any edge $(u,v)\in E$ is replaced by $(u,w_1)$
and $(u,w_2)$, where $w_1.w_2\in V$ were the children of $v$
in $T$.

\subsubsection{Sorting}

A binary search tree can be used to implement a simple but efficient
sorting algorithm. Suppose we wish to sort a list of numbers
$L = [\ell_1, \ell_2,\dots, \ell_n]$. First, let $V=\{1,2,\dots,n\}$
be the vertices of a tree and weight vertex $i$ with $\ell_i$,
for $1\leq i\leq n$. In this case, we can traverse this tree
in order of its weights, thereby building a BST recursively.
This BST represents the sorting of the list $L$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Problems}

\begin{problem}
\item Construct all nonisomorphic trees of order $7$.

\item Describe and present pseudocode of an algorithm that constructs
  all the nonisomorphic spanning trees of a given connected graph.

\item\label{prob:trees_forests:binary_tree_test} The solution of
  Example~\ref{eg:trees_forests:branch_cut_binary_tree} relied on the
  following result: Let $T = (V,E)$ be a tree rooted at $v_0$ and
  suppose $v_0$ has exactly two children. If
  $\max_{v \in V} \deg(v) = 3$ and $v_0$ is the only vertex with
  degree $2$, then $T$ is a binary tree. Prove this statement. Give
  examples of graphs that are binary trees but do not satisfy the
  conditions of the result. Under which conditions would the above
  test return an incorrect answer?

\item What is the worst-case runtime of
  Algorithm~\ref{alg:trees_forests:randomized_spanning_tree_construction}?

\item Figure~\ref{fig:trees_forests:grid_graph_spanning_trees} shows
  two nonisomorphic spanning trees of the $4 \times 4$ grid graph.
  %%
  \begin{enumerate}[(a)]
  \item For each $n = 1, 2, \dots, 7$, construct all nonisomorphic
    spanning trees of the $n \times n$ grid graph.

  \item Explain and provide pseudocode of an algorithm for
    constructing all spanning trees of the $n \times n$ grid graph,
    where $n > 0$.

  \item In general, if $n$ is a positive integer, how many
    nonisomorphic spanning trees are there in the $n \times n$ grid
    graph?
  \end{enumerate}

\item Theorem~\ref{thm:trees_forests:recursive_construction_trees}
  shows how to recursively construct a new tree from a given
  collection of trees, hence it can be considered as a recursive
  definition of trees. To prove theorems based upon recursive
  definitions, we use a proof technique called
  \emph{structural induction}. Let $S(C)$   be a statement about the
  collection of structures $C$, each of which is defined by a
  recursive definition. In the base case, prove $S(C)$ for the basis
  structure(s) $C$. For the inductive case, let $X$ be a structure
  formed using the recursive definition from the structures
  $Y_1, Y_2, \dots, Y_k$. Assume for induction that the statements
  $S(Y_1),\, S(Y_2), \dots, S(Y_k)$ hold and use the inductive
  hypotheses $S(Y_i)$ to prove $S(X)$. Hence conclude that $S(X)$ is
  true for all $X$. Apply structural induction to show that any graph
  constructed using
  Theorem~\ref{thm:trees_forests:recursive_construction_trees} is
  indeed a tree.

\item In Kruskal's
  Algorithm~\ref{alg:trees_forests:Kruskal_algorithm},
  line~\ref{alg:Kruskal:edge_not_in_T_acyclic} requires that the
  addition of a new edge to $T$ does not result in $T$ having a
  cycle. A tree by definition has no cycles. Suppose
  line~\ref{alg:Kruskal:edge_not_in_T_acyclic} is changed to:
  \[
  \textbf{if } e_i \notin E(T)
  \text{ and }
  T \cup \{e_i\} \text{ is a tree } \textbf{then}
  \]
  With this change, explain why
  Algorithm~\ref{alg:trees_forests:Kruskal_algorithm} would return a
  minimum spanning tree or why the algorithm would fail to do so.

\item Figure~\ref{fig:trees_forests:weighted_Chvatal_graph} shows a
  weighted version of the Chv\'atal graph, which has $12$ vertices and
  $24$ edges. Use this graph as input to Kruskal's, Prim's, and
  Bor\r{u}vka's algorithms and compare the resulting minimum spanning
  trees.

\begin{figure}[!htbp]
\centering
\input{image/trees-forests/weighted-Chvatal-graph.tex}
\caption{Weighted Chv\'atal graph.}
\label{fig:trees_forests:weighted_Chvatal_graph}
\end{figure}

\item Algorithm~\ref{alg:trees_forests:randomized_spanning_tree_construction}
  presents a randomized procedure to construct a spanning tree of a
  given connected graph via repeated edge deletion. Describe and
  present pseudocode of a randomized algorithm to \emph{grow} a
  spanning tree via edge addition.

\item Enumerate all the different binary trees on $5$ vertices.

\item Algorithm~\ref{alg:trees_forests:random_binary_tree} generates a
  random binary tree on $n > 0$ vertices. Modify this algorithm so
  that it generates a random $k$-ary tree of order $n > 0$, where
  $k \geq 3$.

\item Show by giving an example that the Morse code is not
  prefix-free.

\item Consider the alphabet $\cA = \{a,b,c\}$ with corresponding
  probabilities~(or weights) $p(a) = 0.5$, $p(b) = 0.3$, and
  $p(c) = 0.2$. Generate two different Huffman codes for $\cA$ and
  illustrate the tree representations of those codes.

\item Find the Huffman code for the letters of the English alphabet
  weighted by the frequency of common American usage.\footnote{
    You can find this on the Internet or in the literature. Part of
    this exercise is finding this frequency distribution yourself.
  }

\item Let $G = (V_1, E_2)$ be a graph and $T = (V_2, E_2)$ a spanning
  tree of $G$. Show that there is a one-to-one correspondence between
  fundamental cycles in $G$ and edges not in $T$.

\item Let $G = (V,E)$ be the $3 \times 3$ grid graph and
  $T_1 = (V_1, E_1)$, $T_2 = (V_2, E_2)$ be spanning trees of $G$ in
  Example~\ref{eg:trees_forests:spanning_tree}. Find a fundamental
  cycle in $G$ for $T_1$ that is not a fundamental cycle in $G$ for
  $T_2$.

\item Usually there exist many spanning trees of a graph. Can you
  classify those graphs for which there is only one spanning tree? In
  other words, find necessary and sufficient conditions for a graph
  $G$ such that if $T$ is a spanning tree then $T$ is unique.

\item Convert the above function \texttt{graycodeword} into a pure
  Python function.

\item Example~\ref{eg:trees_forests:Euler_phi_function_tree} verifies
  that for any positive integer $n > 1$, repeated iteration of the
  Euler phi function $\varphi(n)$ eventually produces $1$. Show that
  this is the case or provide an explanation why it is in general
  false.

\item The Collatz conjecture~\cite{Lagarias1985} asserts that for any
  integer $n > 0$, repeated iteration of the function
  \[
  T(n)
  =
  \begin{cases}
  \frac{3n + 1}{2}, & \text{if $n$ is odd}, \\
  \frac{n}{2}, & \text{if $n$ is even}
  \end{cases}
  \]
  eventually produces the value $1$. For example, repeated iteration
  of $T(n)$ starting from $n = 22$ results in the sequence
  %%
  \begin{equation}
  \label{eqn:trees_forests:Collatz_sequence_22}
  22,\, 11,\, 17,\, 26,\, 13,\, 20,\, 10,\, 5,\, 8,\, 4,\, 2,\, 1.
  \end{equation}
  %%
  One way to think about the Collatz conjecture is to consider the
  digraph $G$ produced by considering $(a_i,\, T(a_i))$ as a directed
  edge of $G$. Then the Collatz conjecture can be rephrased to say
  that there is some integer $k > 0$ such that
  $(a_k,\, T(a_k)) = (2, 1)$ is a directed edge of $G$. The graph
  obtained in this manner is called the \emph{Collatz graph} of
  $T(n)$. Given a collection of positive integers
  $\alpha_1, \alpha_2, \dots, \alpha_k$, let $G_{\alpha_i}$ be the
  Collatz graph of the function $T(\alpha_i)$ with initial iteration
  value $\alpha_i$. Then the union of the $G_{\alpha_i}$ is the
  directed tree
  \[
  \bigcup_i G_{\alpha_i}
  \]
  rooted at $1$, called the \emph{Collatz tree} of
  $(\alpha_1, \alpha_2, \dots, \alpha_k)$.
  Figure~\ref{fig:trees_forests:Collatz_graph_union} shows such a tree
  for the collection of initial iteration values $1024$, $336$, $340$,
  $320$, $106$, $104$, and $96$. See
  Lagarias~\cite{Lagarias2009a,Lagarias2009b} for a comprehensive
  survey of the Collatz conjecture.
  %%
  \begin{enumerate}[(a)]
  \item The \emph{Collatz sequence} of a positive integer $n > 1$ is
    the integer sequence produced by repeated iteration of $T(n)$ with
    initial iteration value $n$. For example, the Collatz sequence
    of $n = 22$ is the
    sequence~\eqref{eqn:trees_forests:Collatz_sequence_22}. Write a
    Sage function to produce the Collatz sequence of an integer $n > 1$.

  \item The \emph{Collatz length} of $n > 1$ is the number of terms in
    the Collatz sequence of $n$, inclusive of the starting iteration
    value and the final integer $1$. For instance, the Collatz length
    of $22$ is $12$, that of $106$ is $11$, and that of $51$ is
    $18$. Write a Sage function to compute the Collatz length of a
    positive integer $n > 1$. If $n > 1$ is a vertex in a Collatz
    tree, verify that the Collatz length of $n$ is the distance
    $d(n,1)$.

  \item Describe the Collatz graph produced by the function $T(n)$
    with initial iteration value $n = 1$.

  \item Fix a positive integer $n > 1$ and let $L_i$ be the Collatz
    length of the integer $1 \leq i \leq n$. Plot the pairs $(i, L_i)$
    on one set of axes.
  \end{enumerate}

\begin{figure}[!htbp]
\centering
\input{image/trees-forests/collatz-graph-union.tex}
\caption{The union of Collatz graphs is a tree.}
\label{fig:trees_forests:Collatz_graph_union}
\end{figure}

\item The following result was first published in
  Wiener~\cite{Wiener1947}. Let $T = (V,E)$ be a tree of order $n > 0$.
  For each edge $e \in E$, let $n_1(e)$ and $n_2(e) = n - n_1(e)$ be
  the orders of the two components of the edge-deletion subgraph
  $T - e$. Show that the Wiener number of $T$ is
  \[
  W(T)
  =
  \sum_{e \in E} n_1(e) \cdot n_2(e).
  \]

\item The following result~\cite{MoharEtAl1993} was independently
  discovered in the late~1980s by Merris and McKay, and is known as
  the Merris-McKay theorem. Let $T$ be a tree of order $n$ and let
  $\mathcal{L}$ be its Laplacian matrix having eigenvalues
  $\lambda_1, \lambda_2, \dots, \lambda_n$. Show that the Wiener
  number of $T$ is
  \[
  W(T)
  =
  n \sum_{i=1}^{n-1} \frac{1}{\lambda_i}.
  \]

\item For each of the algorithms below: (i)~justify whether or not
  it can be applied to multigraphs or multidigraphs; (ii)~if not,
  modify the algorithm so that it is applicable to multigraphs or
  multidigraphs.
  %%
  \begin{enumerate}[(a)]
  \item Randomized spanning tree construction
    Algorithm~\ref{alg:trees_forests:randomized_spanning_tree_construction}.

  \item Kruskal's Algorithm~\ref{alg:trees_forests:Kruskal_algorithm}.

  \item Prim's Algorithm~\ref{alg:trees_forests:prim}.

  \item Bor\r{u}vka's Algorithm~\ref{alg:trees_forests:Boruvka}.
  \end{enumerate}
\end{problem}
